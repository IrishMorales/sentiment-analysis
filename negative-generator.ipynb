{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd12e830",
   "metadata": {},
   "source": [
    "# Negative Tone Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d930",
   "metadata": {},
   "source": [
    "This notebook contains code for an MLP neural network that generates negatively toned data based on the dataset. Feature vectors are then made from the negatively toned data and generated data then saved into .csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f0cda",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eccd3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import regexp_tokenize, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f00057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c2279",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c222980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with pale blue berries. in these peaceful shad...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it flows so long as falls the rain</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and that is why, the lonesome day</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when i peruse the conquered fame of heroes, an...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of inward strife for truth and liberty.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>to his ears there came a murmur of far seas be...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>the one good man in the world who knows me, --</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>faint voices lifted shrill with pain</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>an', fust you knowed on, back come charles the...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>in the wild glens rough shepherds will deplore</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>892 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Sentiment\n",
       "0    with pale blue berries. in these peaceful shad...        1.0\n",
       "1                   it flows so long as falls the rain        0.0\n",
       "2                    and that is why, the lonesome day       -1.0\n",
       "3    when i peruse the conquered fame of heroes, an...        2.0\n",
       "4              of inward strife for truth and liberty.        2.0\n",
       "..                                                 ...        ...\n",
       "887  to his ears there came a murmur of far seas be...        0.0\n",
       "888     the one good man in the world who knows me, --        1.0\n",
       "889               faint voices lifted shrill with pain       -1.0\n",
       "890  an', fust you knowed on, back come charles the...        0.0\n",
       "891     in the wild glens rough shepherds will deplore       -1.0\n",
       "\n",
       "[892 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('poem_sentiment.csv', header=None, index_col=0, names=['Text', 'Sentiment'])\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a556975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and that is why, the lonesome day</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and so on. then a worthless gaud or two</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sounded o'er earth and sea its blast of war</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>want and woe, which torture us</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>an echo returned on the cold gray morn</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>in town, an' not the leanest runt</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>by death's frequented ways</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>rejection of his humanness</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>faint voices lifted shrill with pain</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>in the wild glens rough shepherds will deplore</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Text  Sentiment\n",
       "2                 and that is why, the lonesome day       -1.0\n",
       "8           and so on. then a worthless gaud or two       -1.0\n",
       "17      sounded o'er earth and sea its blast of war       -1.0\n",
       "37                   want and woe, which torture us       -1.0\n",
       "39           an echo returned on the cold gray morn       -1.0\n",
       "..                                              ...        ...\n",
       "874               in town, an' not the leanest runt       -1.0\n",
       "883                      by death's frequented ways       -1.0\n",
       "885                      rejection of his humanness       -1.0\n",
       "889            faint voices lifted shrill with pain       -1.0\n",
       "891  in the wild glens rough shepherds will deplore       -1.0\n",
       "\n",
       "[155 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative = df_raw[df_raw['Sentiment'] < 0]\n",
    "negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ca11d",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45cd3956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                 and that is why, the lonesome day\\n           and so on. then a worthless gaud or two\\n       sounded o'er earth and sea its blast of war\\n                    want and woe, which torture us\\n            an echo returned on the cold gray morn\\n       while i, ... i built up follies like a wall\\n          ah, what a pang of aching sharp surprise\\n                 and the old swallow-haunted barns\\n     the which she bearing home it burned her nest\\n    the crown of sorrow on their heads, their loss\\n               i lay and watched the lonely gloom;\\n          a sceptremonstrous, winged, intolerable.\\n while the rude winds blow off each shadowy crown.\\n         but o, nevermore can we prison him tight.\\n                 may meditate a whole youth's loss\\n        when thee, the eyes of that harsh long ago\\n        the foes inclosing, and his friend pursued\\nand bow to dread inquisitor and worship lords o...\\n        miles off, three dangerous miles, is home;\\n      else, sufferd, it will se\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = negative['Text'].to_string(index=False)\n",
    "raw_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46bd8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                 and that is why, the lonesome day\\n           and so on. then a worthless gaud or two\\n       sounded o'er earth and sea its blast of war\\n                    want and woe, which torture us\\n            an echo returned on the cold gray morn\\n       while i, ... i built up follies like a wall\\n          ah, what a pang of aching sharp surprise\\n                 and the old swallow-haunted barns\\n     the which she bearing home it burned her nest\\n    the crown of sorrow on their heads, their loss\\n               i lay and watched the lonely gloom;\\n          a sceptremonstrous, winged, intolerable.\\n while the rude winds blow off each shadowy crown.\\n         but o, nevermore can we prison him tight.\\n                 may meditate a whole youth's loss\\n        when thee, the eyes of that harsh long ago\\n        the foes inclosing, and his friend pursued\\nand bow to dread inquisitor and worship lords o...\\n        miles off, three dangerous miles, is home;\\n      else, sufferd, it will se\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all non-ASCII characters\n",
    "processed_text = re.sub(r'[^\\x00-\\x7f]', r'', raw_text).lower()\n",
    "processed_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd9097",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987aadca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word tokens: 1113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['and', 'that', 'is', 'why', 'the', 'lonesome', 'day', 'and', 'so', 'on']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get word tokens from text\n",
    "word_tokens = regexp_tokenize(processed_text, pattern=r'[^\\S\\r]+|[\\.,:;!?()--_\"]', gaps=True)\n",
    "word_tokens.append('\\n')\n",
    "print(f\"Number of word tokens: {len(word_tokens)}\")\n",
    "word_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eff0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization done to make uncommon words more likely to be recognized by \n",
    "# Word2Vec model later when converting to feature vectors\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word_tokens = [lemmatizer.lemmatize(token) for token in word_tokens] # Lemmatize nouns\n",
    "word_tokens = [lemmatizer.lemmatize(token, 'v') for token in word_tokens] # Lemmatize verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ff426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique word tokens: 576\n"
     ]
    }
   ],
   "source": [
    "# Get unique word tokens from word tokens\n",
    "unique_words = sorted(list(set(word_tokens)))\n",
    "print(f\"Number of unique word tokens: {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86037479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " \"'\",\n",
       " \"'twas\",\n",
       " 'a',\n",
       " 'accomplish',\n",
       " 'ache',\n",
       " 'add',\n",
       " 'adulterate',\n",
       " 'afar',\n",
       " 'after']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vocabulary of word tokens\n",
    "word_vocabulary = unique_words\n",
    "word_vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cf74d",
   "metadata": {},
   "source": [
    "### Create word-index mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d81c158c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: \"'\",\n",
       " 2: \"'twas\",\n",
       " 3: 'a',\n",
       " 4: 'accomplish',\n",
       " 5: 'ache',\n",
       " 6: 'add',\n",
       " 7: 'adulterate',\n",
       " 8: 'afar',\n",
       " 9: 'after',\n",
       " 10: 'age',\n",
       " 11: 'ago',\n",
       " 12: 'ah',\n",
       " 13: 'air',\n",
       " 14: 'all',\n",
       " 15: 'altar',\n",
       " 16: 'always',\n",
       " 17: 'among',\n",
       " 18: 'an',\n",
       " 19: \"an'\",\n",
       " 20: 'and',\n",
       " 21: 'angel',\n",
       " 22: 'answer',\n",
       " 23: 'anxious',\n",
       " 24: 'around',\n",
       " 25: 'arrow',\n",
       " 26: 'ash',\n",
       " 27: 'ask',\n",
       " 28: 'at',\n",
       " 29: 'augur',\n",
       " 30: 'avenge',\n",
       " 31: 'away',\n",
       " 32: 'b',\n",
       " 33: 'bad',\n",
       " 34: 'barn',\n",
       " 35: 'barrenly',\n",
       " 36: 'bat',\n",
       " 37: 'be',\n",
       " 38: 'beam',\n",
       " 39: 'bear',\n",
       " 40: 'beat',\n",
       " 41: 'because',\n",
       " 42: 'become',\n",
       " 43: 'bed',\n",
       " 44: 'beguile',\n",
       " 45: 'behold',\n",
       " 46: 'bind',\n",
       " 47: 'bitter',\n",
       " 48: 'black',\n",
       " 49: 'blankness',\n",
       " 50: 'blast',\n",
       " 51: 'bleed',\n",
       " 52: 'blend',\n",
       " 53: 'blight',\n",
       " 54: 'blind',\n",
       " 55: 'blindness',\n",
       " 56: 'blood',\n",
       " 57: 'blow',\n",
       " 58: 'body',\n",
       " 59: 'bow',\n",
       " 60: 'brand',\n",
       " 61: 'break',\n",
       " 62: 'breast',\n",
       " 63: 'briareus',\n",
       " 64: 'build',\n",
       " 65: 'burn',\n",
       " 66: 'but',\n",
       " 67: 'by',\n",
       " 68: 'calumny',\n",
       " 69: 'can',\n",
       " 70: \"captive's\",\n",
       " 71: 'change',\n",
       " 72: 'child',\n",
       " 73: 'claim',\n",
       " 74: 'clatter',\n",
       " 75: 'cloud',\n",
       " 76: 'cloudy',\n",
       " 77: 'co',\n",
       " 78: 'cobweb',\n",
       " 79: 'cold',\n",
       " 80: 'come',\n",
       " 81: 'corps',\n",
       " 82: 'cou',\n",
       " 83: 'cough',\n",
       " 84: 'could',\n",
       " 85: 'course',\n",
       " 86: 'cripple',\n",
       " 87: 'cross',\n",
       " 88: 'crown',\n",
       " 89: 'cruell',\n",
       " 90: 'crush',\n",
       " 91: 'cry',\n",
       " 92: 'curse',\n",
       " 93: 'daily',\n",
       " 94: 'damn',\n",
       " 95: 'danger',\n",
       " 96: 'dangerous',\n",
       " 97: 'dare',\n",
       " 98: 'dark',\n",
       " 99: 'darkness',\n",
       " 100: 'dawn',\n",
       " 101: 'day',\n",
       " 102: 'dead',\n",
       " 103: 'deadly',\n",
       " 104: 'deaf',\n",
       " 105: 'death',\n",
       " 106: \"death's\",\n",
       " 107: 'delusive',\n",
       " 108: 'deplore',\n",
       " 109: 'despair',\n",
       " 110: 'didst',\n",
       " 111: 'dip',\n",
       " 112: 'discontent',\n",
       " 113: 'distress',\n",
       " 114: 'disunion',\n",
       " 115: 'do',\n",
       " 116: 'down',\n",
       " 117: 'dread',\n",
       " 118: 'dreadful',\n",
       " 119: 'drear',\n",
       " 120: \"drill's\",\n",
       " 121: 'droop',\n",
       " 122: 'dull',\n",
       " 123: 'dumb',\n",
       " 124: 'dwell',\n",
       " 125: \"e'en\",\n",
       " 126: 'each',\n",
       " 127: 'earth',\n",
       " 128: 'echo',\n",
       " 129: 'else',\n",
       " 130: 'end',\n",
       " 131: 'enemy',\n",
       " 132: 'enough',\n",
       " 133: 'envy',\n",
       " 134: 'etch',\n",
       " 135: 'expire',\n",
       " 136: 'eye',\n",
       " 137: 'face',\n",
       " 138: 'faint',\n",
       " 139: 'fall',\n",
       " 140: 'false',\n",
       " 141: 'fatal',\n",
       " 142: 'fear',\n",
       " 143: 'featureless',\n",
       " 144: \"feel'st\",\n",
       " 145: 'fell',\n",
       " 146: 'fer',\n",
       " 147: 'fetter',\n",
       " 148: 'fiery',\n",
       " 149: 'fight',\n",
       " 150: 'fill',\n",
       " 151: 'fire',\n",
       " 152: 'flame',\n",
       " 153: 'flicker',\n",
       " 154: 'flight',\n",
       " 155: 'flood',\n",
       " 156: 'flower',\n",
       " 157: 'foe',\n",
       " 158: 'foeman',\n",
       " 159: 'foggy',\n",
       " 160: 'folly',\n",
       " 161: 'food',\n",
       " 162: 'fool',\n",
       " 163: 'for',\n",
       " 164: 'forcd',\n",
       " 165: 'force',\n",
       " 166: 'foredoom',\n",
       " 167: 'forelaid',\n",
       " 168: 'forever',\n",
       " 169: 'forget',\n",
       " 170: 'forlorn',\n",
       " 171: 'fountain',\n",
       " 172: 'fraud',\n",
       " 173: 'fray',\n",
       " 174: 'frequent',\n",
       " 175: 'friend',\n",
       " 176: 'from',\n",
       " 177: 'frown',\n",
       " 178: 'fury',\n",
       " 179: 'future',\n",
       " 180: 'gallic',\n",
       " 181: 'gaud',\n",
       " 182: 'generation',\n",
       " 183: 'get',\n",
       " 184: 'girl',\n",
       " 185: 'give',\n",
       " 186: 'glance',\n",
       " 187: 'glen',\n",
       " 188: 'gloom',\n",
       " 189: 'goad',\n",
       " 190: 'god',\n",
       " 191: \"god's\",\n",
       " 192: 'gray',\n",
       " 193: 'great',\n",
       " 194: 'groom',\n",
       " 195: 'guard',\n",
       " 196: 'guile',\n",
       " 197: 'gun',\n",
       " 198: 'ha',\n",
       " 199: 'half',\n",
       " 200: 'hand',\n",
       " 201: 'hang',\n",
       " 202: 'harsh',\n",
       " 203: 'hast',\n",
       " 204: 'hate',\n",
       " 205: 'haunt',\n",
       " 206: 'have',\n",
       " 207: 'he',\n",
       " 208: 'head',\n",
       " 209: 'heap',\n",
       " 210: 'heart',\n",
       " 211: 'hearthstone',\n",
       " 212: 'heavy',\n",
       " 213: 'hee',\n",
       " 214: 'hell',\n",
       " 215: 'her',\n",
       " 216: 'here',\n",
       " 217: 'hi',\n",
       " 218: 'high',\n",
       " 219: 'him',\n",
       " 220: 'his',\n",
       " 221: 'hither',\n",
       " 222: 'home',\n",
       " 223: 'homesick',\n",
       " 224: 'hope',\n",
       " 225: 'house',\n",
       " 226: 'how',\n",
       " 227: 'howl',\n",
       " 228: 'human',\n",
       " 229: 'humanness',\n",
       " 230: 'i',\n",
       " 231: \"i've\",\n",
       " 232: 'if',\n",
       " 233: 'ignorant',\n",
       " 234: 'ill',\n",
       " 235: 'in',\n",
       " 236: 'inand',\n",
       " 237: 'inclose',\n",
       " 238: 'inexorable',\n",
       " 239: 'infernal',\n",
       " 240: 'inquisitor',\n",
       " 241: 'into',\n",
       " 242: 'intolerable',\n",
       " 243: 'it',\n",
       " 244: 'jane',\n",
       " 245: 'kneel',\n",
       " 246: 'know',\n",
       " 247: 'labyrinth',\n",
       " 248: 'land',\n",
       " 249: 'lap',\n",
       " 250: 'lave',\n",
       " 251: 'law',\n",
       " 252: 'lay',\n",
       " 253: 'le',\n",
       " 254: 'leaf',\n",
       " 255: 'leanest',\n",
       " 256: 'leave',\n",
       " 257: 'let',\n",
       " 258: 'lie',\n",
       " 259: 'life',\n",
       " 260: 'lift',\n",
       " 261: 'like',\n",
       " 262: 'line',\n",
       " 263: 'live',\n",
       " 264: 'liveliest',\n",
       " 265: 'load',\n",
       " 266: 'lone',\n",
       " 267: 'lonely',\n",
       " 268: 'lonesome',\n",
       " 269: 'long',\n",
       " 270: 'look',\n",
       " 271: 'lord',\n",
       " 272: 'lose',\n",
       " 273: 'loss',\n",
       " 274: 'lowly',\n",
       " 275: 'lucrece',\n",
       " 276: 'mad',\n",
       " 277: 'madness',\n",
       " 278: 'maidenhead',\n",
       " 279: 'make',\n",
       " 280: 'mankind',\n",
       " 281: 'may',\n",
       " 282: 'me',\n",
       " 283: 'meditate',\n",
       " 284: 'melancholy',\n",
       " 285: 'melt',\n",
       " 286: 'men',\n",
       " 287: 'might',\n",
       " 288: 'mile',\n",
       " 289: 'mine',\n",
       " 290: 'mission',\n",
       " 291: 'moan',\n",
       " 292: 'mole',\n",
       " 293: 'moloch',\n",
       " 294: 'moment',\n",
       " 295: 'monkey',\n",
       " 296: 'monster',\n",
       " 297: 'monstrous',\n",
       " 298: 'mood',\n",
       " 299: 'moon',\n",
       " 300: 'more',\n",
       " 301: 'morn',\n",
       " 302: 'most',\n",
       " 303: 'mourn',\n",
       " 304: 'mournfully',\n",
       " 305: \"mov'd\",\n",
       " 306: 'multiply',\n",
       " 307: 'murmur',\n",
       " 308: 'must',\n",
       " 309: 'my',\n",
       " 310: \"n't\",\n",
       " 311: 'nail',\n",
       " 312: 'name',\n",
       " 313: \"ne'er\",\n",
       " 314: 'nearest',\n",
       " 315: 'nerve',\n",
       " 316: 'nest',\n",
       " 317: 'never',\n",
       " 318: 'nevermore',\n",
       " 319: 'nice',\n",
       " 320: 'night',\n",
       " 321: 'no',\n",
       " 322: 'none',\n",
       " 323: 'not',\n",
       " 324: 'nothing',\n",
       " 325: 'now',\n",
       " 326: 'nymph',\n",
       " 327: 'o',\n",
       " 328: \"o'er\",\n",
       " 329: 'obliterate',\n",
       " 330: \"oblivion's\",\n",
       " 331: 'of',\n",
       " 332: 'off',\n",
       " 333: 'offspring',\n",
       " 334: 'old',\n",
       " 335: 'on',\n",
       " 336: 'once',\n",
       " 337: 'one',\n",
       " 338: 'open',\n",
       " 339: 'or',\n",
       " 340: 'our',\n",
       " 341: 'out',\n",
       " 342: 'pain',\n",
       " 343: 'pallid',\n",
       " 344: 'pang',\n",
       " 345: 'pass',\n",
       " 346: 'pathetic',\n",
       " 347: 'people',\n",
       " 348: 'perish',\n",
       " 349: 'phrase',\n",
       " 350: 'pillar',\n",
       " 351: 'pity',\n",
       " 352: 'poor',\n",
       " 353: 'portend',\n",
       " 354: 'pride',\n",
       " 355: 'priest',\n",
       " 356: 'prison',\n",
       " 357: 'promise',\n",
       " 358: 'pu',\n",
       " 359: 'pursue',\n",
       " 360: 'quicken',\n",
       " 361: 'quit',\n",
       " 362: 'quiver',\n",
       " 363: 'rage',\n",
       " 364: 'range',\n",
       " 365: 'rave',\n",
       " 366: 'receave',\n",
       " 367: 'regin',\n",
       " 368: 'rejection',\n",
       " 369: 'rest',\n",
       " 370: 'return',\n",
       " 371: 'ridiculous',\n",
       " 372: 'right',\n",
       " 373: 'ripe',\n",
       " 374: 'rise',\n",
       " 375: 'rocky',\n",
       " 376: 'roll',\n",
       " 377: 'root',\n",
       " 378: 'rough',\n",
       " 379: 'rude',\n",
       " 380: 'ruin',\n",
       " 381: 'run',\n",
       " 382: 'runt',\n",
       " 383: 'sad',\n",
       " 384: 'satrap',\n",
       " 385: 'savage',\n",
       " 386: 'save',\n",
       " 387: 'say',\n",
       " 388: 'scare',\n",
       " 389: 'scarlet',\n",
       " 390: 'scatter',\n",
       " 391: 'sceptremonstrous',\n",
       " 392: 'scorn',\n",
       " 393: 'sea',\n",
       " 394: 'seditious',\n",
       " 395: 'see',\n",
       " 396: 'seek',\n",
       " 397: 'seem',\n",
       " 398: 'selfishness',\n",
       " 399: 'send',\n",
       " 400: 'serious',\n",
       " 401: 'set',\n",
       " 402: \"sha'\",\n",
       " 403: 'shadow',\n",
       " 404: 'shadowy',\n",
       " 405: 'shall',\n",
       " 406: 'sharp',\n",
       " 407: 'she',\n",
       " 408: 'shepherd',\n",
       " 409: 'shiver',\n",
       " 410: 'shore',\n",
       " 411: 'shout',\n",
       " 412: 'shrill',\n",
       " 413: 'shun',\n",
       " 414: 'silk',\n",
       " 415: 'silly',\n",
       " 416: 'sit',\n",
       " 417: 'sitteth',\n",
       " 418: 'slaughter',\n",
       " 419: 'slave',\n",
       " 420: 'slavery',\n",
       " 421: 'slay',\n",
       " 422: 'sleep',\n",
       " 423: 'slumber',\n",
       " 424: 'smile',\n",
       " 425: 'smother',\n",
       " 426: 'smoulder',\n",
       " 427: 'so',\n",
       " 428: 'soft',\n",
       " 429: 'solemn',\n",
       " 430: 'some',\n",
       " 431: 'sophist',\n",
       " 432: 'sorrow',\n",
       " 433: \"sorrow's\",\n",
       " 434: 'sorrowful',\n",
       " 435: 'sound',\n",
       " 436: 'spake',\n",
       " 437: 'spoil',\n",
       " 438: 'spy',\n",
       " 439: 'star',\n",
       " 440: 'steal',\n",
       " 441: 'steerd',\n",
       " 442: 'stiff',\n",
       " 443: 'stifle',\n",
       " 444: 'still',\n",
       " 445: 'stock',\n",
       " 446: 'stone',\n",
       " 447: 'stormy',\n",
       " 448: 'strange',\n",
       " 449: 'strangle',\n",
       " 450: 'street',\n",
       " 451: 'string',\n",
       " 452: 'strive',\n",
       " 453: 'struggle',\n",
       " 454: 'such',\n",
       " 455: 'sufferd',\n",
       " 456: 'suicide',\n",
       " 457: 'summon',\n",
       " 458: 'sun',\n",
       " 459: 'surprise',\n",
       " 460: 'swallow',\n",
       " 461: 'sweep',\n",
       " 462: 'sword',\n",
       " 463: 'take',\n",
       " 464: 'task',\n",
       " 465: 'teach',\n",
       " 466: 'tear',\n",
       " 467: 'tempest',\n",
       " 468: 'th',\n",
       " 469: 'than',\n",
       " 470: 'that',\n",
       " 471: 'the',\n",
       " 472: 'thee',\n",
       " 473: 'their',\n",
       " 474: 'them',\n",
       " 475: 'then',\n",
       " 476: 'there',\n",
       " 477: 'these',\n",
       " 478: 'they',\n",
       " 479: 'thine',\n",
       " 480: 'thing',\n",
       " 481: 'think',\n",
       " 482: 'this',\n",
       " 483: 'those',\n",
       " 484: 'thou',\n",
       " 485: 'though',\n",
       " 486: 'thousand',\n",
       " 487: 'threaten',\n",
       " 488: 'three',\n",
       " 489: 'thro',\n",
       " 490: 'throb',\n",
       " 491: 'through',\n",
       " 492: 'throughout',\n",
       " 493: 'throw',\n",
       " 494: 'thunder',\n",
       " 495: 'thus',\n",
       " 496: 'thy',\n",
       " 497: 'tide',\n",
       " 498: 'tight',\n",
       " 499: 'till',\n",
       " 500: 'time',\n",
       " 501: \"time's\",\n",
       " 502: 'tinkle',\n",
       " 503: 'tire',\n",
       " 504: 'to',\n",
       " 505: 'torment',\n",
       " 506: 'torture',\n",
       " 507: 'town',\n",
       " 508: 'trail',\n",
       " 509: 'treatin',\n",
       " 510: 'trouble',\n",
       " 511: 'tumble',\n",
       " 512: 'twas',\n",
       " 513: 'twilight',\n",
       " 514: 'two',\n",
       " 515: 'u',\n",
       " 516: 'unloved',\n",
       " 517: 'until',\n",
       " 518: 'up',\n",
       " 519: 'utter',\n",
       " 520: 'vain',\n",
       " 521: 'vapour',\n",
       " 522: 'vehement',\n",
       " 523: 'visual',\n",
       " 524: 'voice',\n",
       " 525: 'wa',\n",
       " 526: 'wake',\n",
       " 527: 'wall',\n",
       " 528: 'wander',\n",
       " 529: 'want',\n",
       " 530: 'war',\n",
       " 531: 'warlike',\n",
       " 532: 'warn',\n",
       " 533: 'waste',\n",
       " 534: 'watch',\n",
       " 535: 'water',\n",
       " 536: 'waver',\n",
       " 537: 'way',\n",
       " 538: 'we',\n",
       " 539: 'weak',\n",
       " 540: 'weight',\n",
       " 541: 'weird',\n",
       " 542: 'what',\n",
       " 543: 'wheeze',\n",
       " 544: 'when',\n",
       " 545: 'where',\n",
       " 546: 'which',\n",
       " 547: 'while',\n",
       " 548: 'who',\n",
       " 549: 'whole',\n",
       " 550: 'why',\n",
       " 551: 'wild',\n",
       " 552: 'will',\n",
       " 553: 'wilt',\n",
       " 554: 'wind',\n",
       " 555: 'wing',\n",
       " 556: 'winter',\n",
       " 557: 'with',\n",
       " 558: 'wither',\n",
       " 559: 'within',\n",
       " 560: 'woe',\n",
       " 561: 'woman',\n",
       " 562: 'word',\n",
       " 563: 'world',\n",
       " 564: 'worship',\n",
       " 565: 'worthless',\n",
       " 566: 'would',\n",
       " 567: 'wrack',\n",
       " 568: 'wreck',\n",
       " 569: 'wrinkle',\n",
       " 570: 'writ',\n",
       " 571: 'wrong',\n",
       " 572: \"yo'\",\n",
       " 573: 'you',\n",
       " 574: 'your',\n",
       " 575: \"youth's\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create index-word mappings \n",
    "indices_words = dict((index, word) for index, word in enumerate(unique_words))\n",
    "indices_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e485f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " \"'\": 1,\n",
       " \"'twas\": 2,\n",
       " 'a': 3,\n",
       " 'accomplish': 4,\n",
       " 'ache': 5,\n",
       " 'add': 6,\n",
       " 'adulterate': 7,\n",
       " 'afar': 8,\n",
       " 'after': 9,\n",
       " 'age': 10,\n",
       " 'ago': 11,\n",
       " 'ah': 12,\n",
       " 'air': 13,\n",
       " 'all': 14,\n",
       " 'altar': 15,\n",
       " 'always': 16,\n",
       " 'among': 17,\n",
       " 'an': 18,\n",
       " \"an'\": 19,\n",
       " 'and': 20,\n",
       " 'angel': 21,\n",
       " 'answer': 22,\n",
       " 'anxious': 23,\n",
       " 'around': 24,\n",
       " 'arrow': 25,\n",
       " 'ash': 26,\n",
       " 'ask': 27,\n",
       " 'at': 28,\n",
       " 'augur': 29,\n",
       " 'avenge': 30,\n",
       " 'away': 31,\n",
       " 'b': 32,\n",
       " 'bad': 33,\n",
       " 'barn': 34,\n",
       " 'barrenly': 35,\n",
       " 'bat': 36,\n",
       " 'be': 37,\n",
       " 'beam': 38,\n",
       " 'bear': 39,\n",
       " 'beat': 40,\n",
       " 'because': 41,\n",
       " 'become': 42,\n",
       " 'bed': 43,\n",
       " 'beguile': 44,\n",
       " 'behold': 45,\n",
       " 'bind': 46,\n",
       " 'bitter': 47,\n",
       " 'black': 48,\n",
       " 'blankness': 49,\n",
       " 'blast': 50,\n",
       " 'bleed': 51,\n",
       " 'blend': 52,\n",
       " 'blight': 53,\n",
       " 'blind': 54,\n",
       " 'blindness': 55,\n",
       " 'blood': 56,\n",
       " 'blow': 57,\n",
       " 'body': 58,\n",
       " 'bow': 59,\n",
       " 'brand': 60,\n",
       " 'break': 61,\n",
       " 'breast': 62,\n",
       " 'briareus': 63,\n",
       " 'build': 64,\n",
       " 'burn': 65,\n",
       " 'but': 66,\n",
       " 'by': 67,\n",
       " 'calumny': 68,\n",
       " 'can': 69,\n",
       " \"captive's\": 70,\n",
       " 'change': 71,\n",
       " 'child': 72,\n",
       " 'claim': 73,\n",
       " 'clatter': 74,\n",
       " 'cloud': 75,\n",
       " 'cloudy': 76,\n",
       " 'co': 77,\n",
       " 'cobweb': 78,\n",
       " 'cold': 79,\n",
       " 'come': 80,\n",
       " 'corps': 81,\n",
       " 'cou': 82,\n",
       " 'cough': 83,\n",
       " 'could': 84,\n",
       " 'course': 85,\n",
       " 'cripple': 86,\n",
       " 'cross': 87,\n",
       " 'crown': 88,\n",
       " 'cruell': 89,\n",
       " 'crush': 90,\n",
       " 'cry': 91,\n",
       " 'curse': 92,\n",
       " 'daily': 93,\n",
       " 'damn': 94,\n",
       " 'danger': 95,\n",
       " 'dangerous': 96,\n",
       " 'dare': 97,\n",
       " 'dark': 98,\n",
       " 'darkness': 99,\n",
       " 'dawn': 100,\n",
       " 'day': 101,\n",
       " 'dead': 102,\n",
       " 'deadly': 103,\n",
       " 'deaf': 104,\n",
       " 'death': 105,\n",
       " \"death's\": 106,\n",
       " 'delusive': 107,\n",
       " 'deplore': 108,\n",
       " 'despair': 109,\n",
       " 'didst': 110,\n",
       " 'dip': 111,\n",
       " 'discontent': 112,\n",
       " 'distress': 113,\n",
       " 'disunion': 114,\n",
       " 'do': 115,\n",
       " 'down': 116,\n",
       " 'dread': 117,\n",
       " 'dreadful': 118,\n",
       " 'drear': 119,\n",
       " \"drill's\": 120,\n",
       " 'droop': 121,\n",
       " 'dull': 122,\n",
       " 'dumb': 123,\n",
       " 'dwell': 124,\n",
       " \"e'en\": 125,\n",
       " 'each': 126,\n",
       " 'earth': 127,\n",
       " 'echo': 128,\n",
       " 'else': 129,\n",
       " 'end': 130,\n",
       " 'enemy': 131,\n",
       " 'enough': 132,\n",
       " 'envy': 133,\n",
       " 'etch': 134,\n",
       " 'expire': 135,\n",
       " 'eye': 136,\n",
       " 'face': 137,\n",
       " 'faint': 138,\n",
       " 'fall': 139,\n",
       " 'false': 140,\n",
       " 'fatal': 141,\n",
       " 'fear': 142,\n",
       " 'featureless': 143,\n",
       " \"feel'st\": 144,\n",
       " 'fell': 145,\n",
       " 'fer': 146,\n",
       " 'fetter': 147,\n",
       " 'fiery': 148,\n",
       " 'fight': 149,\n",
       " 'fill': 150,\n",
       " 'fire': 151,\n",
       " 'flame': 152,\n",
       " 'flicker': 153,\n",
       " 'flight': 154,\n",
       " 'flood': 155,\n",
       " 'flower': 156,\n",
       " 'foe': 157,\n",
       " 'foeman': 158,\n",
       " 'foggy': 159,\n",
       " 'folly': 160,\n",
       " 'food': 161,\n",
       " 'fool': 162,\n",
       " 'for': 163,\n",
       " 'forcd': 164,\n",
       " 'force': 165,\n",
       " 'foredoom': 166,\n",
       " 'forelaid': 167,\n",
       " 'forever': 168,\n",
       " 'forget': 169,\n",
       " 'forlorn': 170,\n",
       " 'fountain': 171,\n",
       " 'fraud': 172,\n",
       " 'fray': 173,\n",
       " 'frequent': 174,\n",
       " 'friend': 175,\n",
       " 'from': 176,\n",
       " 'frown': 177,\n",
       " 'fury': 178,\n",
       " 'future': 179,\n",
       " 'gallic': 180,\n",
       " 'gaud': 181,\n",
       " 'generation': 182,\n",
       " 'get': 183,\n",
       " 'girl': 184,\n",
       " 'give': 185,\n",
       " 'glance': 186,\n",
       " 'glen': 187,\n",
       " 'gloom': 188,\n",
       " 'goad': 189,\n",
       " 'god': 190,\n",
       " \"god's\": 191,\n",
       " 'gray': 192,\n",
       " 'great': 193,\n",
       " 'groom': 194,\n",
       " 'guard': 195,\n",
       " 'guile': 196,\n",
       " 'gun': 197,\n",
       " 'ha': 198,\n",
       " 'half': 199,\n",
       " 'hand': 200,\n",
       " 'hang': 201,\n",
       " 'harsh': 202,\n",
       " 'hast': 203,\n",
       " 'hate': 204,\n",
       " 'haunt': 205,\n",
       " 'have': 206,\n",
       " 'he': 207,\n",
       " 'head': 208,\n",
       " 'heap': 209,\n",
       " 'heart': 210,\n",
       " 'hearthstone': 211,\n",
       " 'heavy': 212,\n",
       " 'hee': 213,\n",
       " 'hell': 214,\n",
       " 'her': 215,\n",
       " 'here': 216,\n",
       " 'hi': 217,\n",
       " 'high': 218,\n",
       " 'him': 219,\n",
       " 'his': 220,\n",
       " 'hither': 221,\n",
       " 'home': 222,\n",
       " 'homesick': 223,\n",
       " 'hope': 224,\n",
       " 'house': 225,\n",
       " 'how': 226,\n",
       " 'howl': 227,\n",
       " 'human': 228,\n",
       " 'humanness': 229,\n",
       " 'i': 230,\n",
       " \"i've\": 231,\n",
       " 'if': 232,\n",
       " 'ignorant': 233,\n",
       " 'ill': 234,\n",
       " 'in': 235,\n",
       " 'inand': 236,\n",
       " 'inclose': 237,\n",
       " 'inexorable': 238,\n",
       " 'infernal': 239,\n",
       " 'inquisitor': 240,\n",
       " 'into': 241,\n",
       " 'intolerable': 242,\n",
       " 'it': 243,\n",
       " 'jane': 244,\n",
       " 'kneel': 245,\n",
       " 'know': 246,\n",
       " 'labyrinth': 247,\n",
       " 'land': 248,\n",
       " 'lap': 249,\n",
       " 'lave': 250,\n",
       " 'law': 251,\n",
       " 'lay': 252,\n",
       " 'le': 253,\n",
       " 'leaf': 254,\n",
       " 'leanest': 255,\n",
       " 'leave': 256,\n",
       " 'let': 257,\n",
       " 'lie': 258,\n",
       " 'life': 259,\n",
       " 'lift': 260,\n",
       " 'like': 261,\n",
       " 'line': 262,\n",
       " 'live': 263,\n",
       " 'liveliest': 264,\n",
       " 'load': 265,\n",
       " 'lone': 266,\n",
       " 'lonely': 267,\n",
       " 'lonesome': 268,\n",
       " 'long': 269,\n",
       " 'look': 270,\n",
       " 'lord': 271,\n",
       " 'lose': 272,\n",
       " 'loss': 273,\n",
       " 'lowly': 274,\n",
       " 'lucrece': 275,\n",
       " 'mad': 276,\n",
       " 'madness': 277,\n",
       " 'maidenhead': 278,\n",
       " 'make': 279,\n",
       " 'mankind': 280,\n",
       " 'may': 281,\n",
       " 'me': 282,\n",
       " 'meditate': 283,\n",
       " 'melancholy': 284,\n",
       " 'melt': 285,\n",
       " 'men': 286,\n",
       " 'might': 287,\n",
       " 'mile': 288,\n",
       " 'mine': 289,\n",
       " 'mission': 290,\n",
       " 'moan': 291,\n",
       " 'mole': 292,\n",
       " 'moloch': 293,\n",
       " 'moment': 294,\n",
       " 'monkey': 295,\n",
       " 'monster': 296,\n",
       " 'monstrous': 297,\n",
       " 'mood': 298,\n",
       " 'moon': 299,\n",
       " 'more': 300,\n",
       " 'morn': 301,\n",
       " 'most': 302,\n",
       " 'mourn': 303,\n",
       " 'mournfully': 304,\n",
       " \"mov'd\": 305,\n",
       " 'multiply': 306,\n",
       " 'murmur': 307,\n",
       " 'must': 308,\n",
       " 'my': 309,\n",
       " \"n't\": 310,\n",
       " 'nail': 311,\n",
       " 'name': 312,\n",
       " \"ne'er\": 313,\n",
       " 'nearest': 314,\n",
       " 'nerve': 315,\n",
       " 'nest': 316,\n",
       " 'never': 317,\n",
       " 'nevermore': 318,\n",
       " 'nice': 319,\n",
       " 'night': 320,\n",
       " 'no': 321,\n",
       " 'none': 322,\n",
       " 'not': 323,\n",
       " 'nothing': 324,\n",
       " 'now': 325,\n",
       " 'nymph': 326,\n",
       " 'o': 327,\n",
       " \"o'er\": 328,\n",
       " 'obliterate': 329,\n",
       " \"oblivion's\": 330,\n",
       " 'of': 331,\n",
       " 'off': 332,\n",
       " 'offspring': 333,\n",
       " 'old': 334,\n",
       " 'on': 335,\n",
       " 'once': 336,\n",
       " 'one': 337,\n",
       " 'open': 338,\n",
       " 'or': 339,\n",
       " 'our': 340,\n",
       " 'out': 341,\n",
       " 'pain': 342,\n",
       " 'pallid': 343,\n",
       " 'pang': 344,\n",
       " 'pass': 345,\n",
       " 'pathetic': 346,\n",
       " 'people': 347,\n",
       " 'perish': 348,\n",
       " 'phrase': 349,\n",
       " 'pillar': 350,\n",
       " 'pity': 351,\n",
       " 'poor': 352,\n",
       " 'portend': 353,\n",
       " 'pride': 354,\n",
       " 'priest': 355,\n",
       " 'prison': 356,\n",
       " 'promise': 357,\n",
       " 'pu': 358,\n",
       " 'pursue': 359,\n",
       " 'quicken': 360,\n",
       " 'quit': 361,\n",
       " 'quiver': 362,\n",
       " 'rage': 363,\n",
       " 'range': 364,\n",
       " 'rave': 365,\n",
       " 'receave': 366,\n",
       " 'regin': 367,\n",
       " 'rejection': 368,\n",
       " 'rest': 369,\n",
       " 'return': 370,\n",
       " 'ridiculous': 371,\n",
       " 'right': 372,\n",
       " 'ripe': 373,\n",
       " 'rise': 374,\n",
       " 'rocky': 375,\n",
       " 'roll': 376,\n",
       " 'root': 377,\n",
       " 'rough': 378,\n",
       " 'rude': 379,\n",
       " 'ruin': 380,\n",
       " 'run': 381,\n",
       " 'runt': 382,\n",
       " 'sad': 383,\n",
       " 'satrap': 384,\n",
       " 'savage': 385,\n",
       " 'save': 386,\n",
       " 'say': 387,\n",
       " 'scare': 388,\n",
       " 'scarlet': 389,\n",
       " 'scatter': 390,\n",
       " 'sceptremonstrous': 391,\n",
       " 'scorn': 392,\n",
       " 'sea': 393,\n",
       " 'seditious': 394,\n",
       " 'see': 395,\n",
       " 'seek': 396,\n",
       " 'seem': 397,\n",
       " 'selfishness': 398,\n",
       " 'send': 399,\n",
       " 'serious': 400,\n",
       " 'set': 401,\n",
       " \"sha'\": 402,\n",
       " 'shadow': 403,\n",
       " 'shadowy': 404,\n",
       " 'shall': 405,\n",
       " 'sharp': 406,\n",
       " 'she': 407,\n",
       " 'shepherd': 408,\n",
       " 'shiver': 409,\n",
       " 'shore': 410,\n",
       " 'shout': 411,\n",
       " 'shrill': 412,\n",
       " 'shun': 413,\n",
       " 'silk': 414,\n",
       " 'silly': 415,\n",
       " 'sit': 416,\n",
       " 'sitteth': 417,\n",
       " 'slaughter': 418,\n",
       " 'slave': 419,\n",
       " 'slavery': 420,\n",
       " 'slay': 421,\n",
       " 'sleep': 422,\n",
       " 'slumber': 423,\n",
       " 'smile': 424,\n",
       " 'smother': 425,\n",
       " 'smoulder': 426,\n",
       " 'so': 427,\n",
       " 'soft': 428,\n",
       " 'solemn': 429,\n",
       " 'some': 430,\n",
       " 'sophist': 431,\n",
       " 'sorrow': 432,\n",
       " \"sorrow's\": 433,\n",
       " 'sorrowful': 434,\n",
       " 'sound': 435,\n",
       " 'spake': 436,\n",
       " 'spoil': 437,\n",
       " 'spy': 438,\n",
       " 'star': 439,\n",
       " 'steal': 440,\n",
       " 'steerd': 441,\n",
       " 'stiff': 442,\n",
       " 'stifle': 443,\n",
       " 'still': 444,\n",
       " 'stock': 445,\n",
       " 'stone': 446,\n",
       " 'stormy': 447,\n",
       " 'strange': 448,\n",
       " 'strangle': 449,\n",
       " 'street': 450,\n",
       " 'string': 451,\n",
       " 'strive': 452,\n",
       " 'struggle': 453,\n",
       " 'such': 454,\n",
       " 'sufferd': 455,\n",
       " 'suicide': 456,\n",
       " 'summon': 457,\n",
       " 'sun': 458,\n",
       " 'surprise': 459,\n",
       " 'swallow': 460,\n",
       " 'sweep': 461,\n",
       " 'sword': 462,\n",
       " 'take': 463,\n",
       " 'task': 464,\n",
       " 'teach': 465,\n",
       " 'tear': 466,\n",
       " 'tempest': 467,\n",
       " 'th': 468,\n",
       " 'than': 469,\n",
       " 'that': 470,\n",
       " 'the': 471,\n",
       " 'thee': 472,\n",
       " 'their': 473,\n",
       " 'them': 474,\n",
       " 'then': 475,\n",
       " 'there': 476,\n",
       " 'these': 477,\n",
       " 'they': 478,\n",
       " 'thine': 479,\n",
       " 'thing': 480,\n",
       " 'think': 481,\n",
       " 'this': 482,\n",
       " 'those': 483,\n",
       " 'thou': 484,\n",
       " 'though': 485,\n",
       " 'thousand': 486,\n",
       " 'threaten': 487,\n",
       " 'three': 488,\n",
       " 'thro': 489,\n",
       " 'throb': 490,\n",
       " 'through': 491,\n",
       " 'throughout': 492,\n",
       " 'throw': 493,\n",
       " 'thunder': 494,\n",
       " 'thus': 495,\n",
       " 'thy': 496,\n",
       " 'tide': 497,\n",
       " 'tight': 498,\n",
       " 'till': 499,\n",
       " 'time': 500,\n",
       " \"time's\": 501,\n",
       " 'tinkle': 502,\n",
       " 'tire': 503,\n",
       " 'to': 504,\n",
       " 'torment': 505,\n",
       " 'torture': 506,\n",
       " 'town': 507,\n",
       " 'trail': 508,\n",
       " 'treatin': 509,\n",
       " 'trouble': 510,\n",
       " 'tumble': 511,\n",
       " 'twas': 512,\n",
       " 'twilight': 513,\n",
       " 'two': 514,\n",
       " 'u': 515,\n",
       " 'unloved': 516,\n",
       " 'until': 517,\n",
       " 'up': 518,\n",
       " 'utter': 519,\n",
       " 'vain': 520,\n",
       " 'vapour': 521,\n",
       " 'vehement': 522,\n",
       " 'visual': 523,\n",
       " 'voice': 524,\n",
       " 'wa': 525,\n",
       " 'wake': 526,\n",
       " 'wall': 527,\n",
       " 'wander': 528,\n",
       " 'want': 529,\n",
       " 'war': 530,\n",
       " 'warlike': 531,\n",
       " 'warn': 532,\n",
       " 'waste': 533,\n",
       " 'watch': 534,\n",
       " 'water': 535,\n",
       " 'waver': 536,\n",
       " 'way': 537,\n",
       " 'we': 538,\n",
       " 'weak': 539,\n",
       " 'weight': 540,\n",
       " 'weird': 541,\n",
       " 'what': 542,\n",
       " 'wheeze': 543,\n",
       " 'when': 544,\n",
       " 'where': 545,\n",
       " 'which': 546,\n",
       " 'while': 547,\n",
       " 'who': 548,\n",
       " 'whole': 549,\n",
       " 'why': 550,\n",
       " 'wild': 551,\n",
       " 'will': 552,\n",
       " 'wilt': 553,\n",
       " 'wind': 554,\n",
       " 'wing': 555,\n",
       " 'winter': 556,\n",
       " 'with': 557,\n",
       " 'wither': 558,\n",
       " 'within': 559,\n",
       " 'woe': 560,\n",
       " 'woman': 561,\n",
       " 'word': 562,\n",
       " 'world': 563,\n",
       " 'worship': 564,\n",
       " 'worthless': 565,\n",
       " 'would': 566,\n",
       " 'wrack': 567,\n",
       " 'wreck': 568,\n",
       " 'wrinkle': 569,\n",
       " 'writ': 570,\n",
       " 'wrong': 571,\n",
       " \"yo'\": 572,\n",
       " 'you': 573,\n",
       " 'your': 574,\n",
       " \"youth's\": 575}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create word-index mappings\n",
    "word_indices = dict((word, index) for index, word in enumerate(unique_words))\n",
    "word_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6413184",
   "metadata": {},
   "source": [
    "### Create Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5afa53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x (input): Split text into blocks, where each block has the same amount of words\n",
    "# Create y (targets): For each x input, the y is the word that comes next\n",
    "# The model should learn to predict y from the input x\n",
    "\n",
    "block_size = 2\n",
    "step = 1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(word_tokens) - block_size, step):\n",
    "    x.append(word_tokens[i: i+block_size])\n",
    "    y.append(word_tokens[i + block_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83d0d331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['and', 'that'],\n",
       " ['that', 'be'],\n",
       " ['be', 'why'],\n",
       " ['why', 'the'],\n",
       " ['the', 'lonesome']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect x\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1dd331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of blocks\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c87bed",
   "metadata": {},
   "source": [
    "### Create One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ba28e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoding of x\n",
    "x_encoded = []\n",
    "\n",
    "for x_arr in x:\n",
    "    x_ints = [word_indices[item] for item in x_arr]\n",
    "    \n",
    "    x_row = []\n",
    "    for item in x_ints:\n",
    "        x_vector = np.zeros(len(unique_words))\n",
    "        x_vector[item] = 1\n",
    "        x_row.append(x_vector)\n",
    "        \n",
    "    x_encoded.append(x_row)\n",
    "    \n",
    "x_encoded = np.array(x_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e351e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be', 'why', 'the', 'lonesome', 'day']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect y\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea6c112d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37, 550, 471, 268, 101]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each word in y into their corresponding indices\n",
    "y_ints = [word_indices[item] for item in y]\n",
    "y_ints[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fc0bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoding of y\n",
    "y_encoded = []\n",
    "\n",
    "for item in y_ints:\n",
    "    y_vector = np.zeros(len(unique_words))\n",
    "    y_vector[item] = 1\n",
    "    y_encoded.append(y_vector)\n",
    "\n",
    "y_encoded = np.array(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcda4bf",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caa21464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Linear(input_dim, 2000)\n",
    "        self.hidden = nn.Linear(2000, 1200)\n",
    "        self.output = nn.Linear(1200, output_dim)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.embeddings(x))\n",
    "        x = self.tanh(self.hidden(x))\n",
    "        x = self.softmax(self.output(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6ff9f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n"
     ]
    }
   ],
   "source": [
    "# Get size of input for training the model\n",
    "input_size = x_encoded[0].ravel().shape[0]\n",
    "print(x_encoded[0].ravel().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df58af12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing torch operations on cpu device\n"
     ]
    }
   ],
   "source": [
    "# Allocate tensors to the device used for computation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Performing torch operations on {device} device\")\n",
    "\n",
    "# Create x and y PyTorch tensors\n",
    "x = torch.tensor(x_encoded).float().to(device)\n",
    "y = torch.tensor(y_encoded).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb023182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextGenerator(\n",
       "  (embeddings): Linear(in_features=1152, out_features=2000, bias=True)\n",
       "  (hidden): Linear(in_features=2000, out_features=1200, bias=True)\n",
       "  (output): Linear(in_features=1200, out_features=576, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = TextGenerator(input_size, len(unique_words), block_size).to(device)\n",
    "\n",
    "# Print model configuration\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "433009b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ee59c",
   "metadata": {},
   "source": [
    "### Create Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9e8b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom Dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        self.n_samples = len(x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index].ravel(), self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9228e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset using custom Dataset class\n",
    "training_ds = CustomDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a488527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset into DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    training_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03558f02",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a82f8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to train model\n",
    "def train_fn(loader, model, optimizer, loss_fn, device):\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    ave_loss = 0\n",
    "    count = 0 \n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        # Forward\n",
    "        predictions = model.forward(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update tqdm loading bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        count += 1\n",
    "        ave_loss += loss.item()\n",
    "    \n",
    "    ave_loss = ave_loss / count\n",
    "\n",
    "    return ave_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811b5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 112/112 [00:10<00:00, 10.66it/s, loss=6.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.328499440635953\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████▎                  | 82/112 [00:06<00:02, 14.54it/s, loss=6.16]"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 2 # TODO: CHANGE TO 300 ON FINAL DATA; CURRENTLY 2 FOR TESTING PURPOSES\n",
    "average_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    ave_loss = train_fn(train_loader, model, optimizer, criterion, device)\n",
    "    \n",
    "    print(\"Ave Loss: {}\".format(ave_loss))\n",
    "    average_losses.append(ave_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e315a81",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd01d2-95c1-4264-8a39-4d013120d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, num_words):\n",
    "    device = 'cpu'\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert seed_text to input tensor\n",
    "    seed_encoded = []\n",
    "    for word in seed_text.split():\n",
    "        word_index = word_indices[word]\n",
    "        word_encoded = np.zeros(len(unique_words))\n",
    "        word_encoded[word_index] = 1\n",
    "        seed_encoded.append(word_encoded)\n",
    "    seed_encoded = np.array(seed_encoded)\n",
    "    seed_encoded = np.expand_dims(seed_encoded, axis=0)\n",
    "    seed_tensor = torch.tensor(seed_encoded).float().to(device)\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = seed_text\n",
    "    for i in range(num_words):\n",
    "        predictions = model(seed_tensor)\n",
    "        predicted_index = torch.argmax(predictions, dim=1).item()\n",
    "        predicted_word = indices_words[predicted_index]\n",
    "        generated_text += ' ' + predicted_word\n",
    "        \n",
    "        # Update seed tensor with predicted word\n",
    "        predicted_encoded = np.zeros(len(unique_words))\n",
    "        predicted_encoded[predicted_index] = 1\n",
    "        predicted_encoded = np.expand_dims(predicted_encoded, axis=0)\n",
    "        seed_tensor = torch.cat((seed_tensor[:, 1:, :], torch.tensor(predicted_encoded).float().to(device)), axis=1)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140eb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text sample from model output\n",
    "word_count = 100\n",
    "text = []\n",
    "paragraph_count = 5\n",
    "\n",
    "# Length of phrase should be same as block_size\n",
    "word1, word2 = \"\\n\", \"\\n\"\n",
    "\n",
    "for p in range(paragraph_count):\n",
    "    text.append([])\n",
    "    \n",
    "    for i in range(word_count):\n",
    "        phrase = [word1, word2]\n",
    "        x_ints = [word_indices[item] for item in phrase]\n",
    "        x_vector = []\n",
    "\n",
    "        for item in x_ints:\n",
    "            x_item = np.zeros(len(unique_words))\n",
    "            x_item[item] = 1\n",
    "            x_vector.append(x_item)\n",
    "\n",
    "        initial_input = torch.tensor([np.array([x_vector]).ravel()]).float()\n",
    "\n",
    "        output = model(initial_input)[0].detach().cpu().numpy()\n",
    "\n",
    "        # Workaround to fix occasional sum(pvals[:-1]) > 1.0  bug from implicit casting in np.random.multinomial \n",
    "        output = output.astype(float)\n",
    "        output /= output.sum()\n",
    "\n",
    "        index = np.where(np.random.multinomial(1, output) == 1)[0][0]\n",
    "        word3 = indices_words[index]\n",
    "        text[p].append(word3)\n",
    "\n",
    "        # Use generated word from this run as seed for next run\n",
    "        word1, word2 = word2, word3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd566b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in range(paragraph_count):\n",
    "    print(f\"Generated Paragraph {p}:\")\n",
    "    print(' '.join(text[p]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d46cb7",
   "metadata": {},
   "source": [
    "### Create Feature Vectors from Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "# Tokenize by line\n",
    "for index, row in negative.iterrows():\n",
    "    tokenized_row = row['Text'].split(' ')\n",
    "    \n",
    "    # Preprocess using the same settings as preprocessing done before training model\n",
    "    tokenized_row = regexp_tokenize(' '.join(tokenized_row), pattern=r'[^\\S\\r]+|[\\.,;!?()--_\"]', gaps=True)\n",
    "    tokenized_row = [lemmatizer.lemmatize(token) for token in tokenized_row] # Lemmatize nouns\n",
    "    tokenized_row = [lemmatizer.lemmatize(token, 'v') for token in tokenized_row] # Lemmatize verbs\n",
    "    \n",
    "    sentences.append(tokenized_row)\n",
    "    \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "w2v_model = Word2Vec(sentences, vector_size=vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d010c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = w2v_model.wv.index_to_key\n",
    "vocab_length = len(vocab)\n",
    "\n",
    "# Note: Low vocab size is because Word2Vec model isn't familiar with most of the words in our dataset\n",
    "# From https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "print(f'Vocabulary Size: {format(vocab_length)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2016e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('he', 'his')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector_averaging(sentence, model):\n",
    "    embeddings = [model.wv[word] for word in sentence if word in vocab]\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258efc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    features.append(sentence_to_vector_averaging(sentence, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f'x{i}' for i in range(w2v_model.vector_size)]\n",
    "\n",
    "df_text = pd.DataFrame(features, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb6064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_text['y'] = [float(x) for x in negative['Sentiment']]\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacbab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature vectors as csv\n",
    "df_text.to_csv('negative.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ecc06",
   "metadata": {},
   "source": [
    "### Create Feature Vectors from Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3088f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "w2v_model = Word2Vec(text, vector_size=vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcffb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = w2v_model.wv.index_to_key\n",
    "vocab_length = len(vocab)\n",
    "\n",
    "# Note: Low vocab size is because Word2Vec model isn't familiar with most of the words in our dataset\n",
    "# From https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "print(f'Vocabulary Size: {format(vocab_length)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [w2v_model.wv[word] for word in vocab]\n",
    "len(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1304879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector_averaging(sentence, model):\n",
    "    embeddings = [model.wv[word] for word in sentence if word in vocab]\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a160fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    features.append(sentence_to_vector_averaging(sentence, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f'x{i}' for i in range(w2v_model.vector_size)]\n",
    "\n",
    "df_text = pd.DataFrame(features, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcafa9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_text['y'] = [-1] * len(df_text)\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature vectors as csv\n",
    "df_text.to_csv('negative_generated.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
