{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd12e830",
   "metadata": {},
   "source": [
    "# Negative Tone Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d930",
   "metadata": {},
   "source": [
    "This notebook contains code for an MLP neural network that generates negatively toned data based on the dataset. Feature vectors are then made from the negatively toned data and generated data then saved into .csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f0cda",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eccd3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f00057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c2279",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c222980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with pale blue berries. in these peaceful shad...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it flows so long as falls the rain</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and that is why, the lonesome day</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when i peruse the conquered fame of heroes, an...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of inward strife for truth and liberty.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>to his ears there came a murmur of far seas be...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>the one good man in the world who knows me, --</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>faint voices lifted shrill with pain</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>an', fust you knowed on, back come charles the...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>in the wild glens rough shepherds will deplore</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>892 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Sentiment\n",
       "0    with pale blue berries. in these peaceful shad...        1.0\n",
       "1                   it flows so long as falls the rain        0.0\n",
       "2                    and that is why, the lonesome day       -1.0\n",
       "3    when i peruse the conquered fame of heroes, an...        2.0\n",
       "4              of inward strife for truth and liberty.        2.0\n",
       "..                                                 ...        ...\n",
       "887  to his ears there came a murmur of far seas be...        0.0\n",
       "888     the one good man in the world who knows me, --        1.0\n",
       "889               faint voices lifted shrill with pain       -1.0\n",
       "890  an', fust you knowed on, back come charles the...        0.0\n",
       "891     in the wild glens rough shepherds will deplore       -1.0\n",
       "\n",
       "[892 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('poem_sentiment.csv', header=None, index_col=0, names=['Text', 'Sentiment'])\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a556975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and that is why, the lonesome day</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and so on. then a worthless gaud or two</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sounded o'er earth and sea its blast of war</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>want and woe, which torture us</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>an echo returned on the cold gray morn</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>in town, an' not the leanest runt</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>by death's frequented ways</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>rejection of his humanness</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>faint voices lifted shrill with pain</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>in the wild glens rough shepherds will deplore</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Text  Sentiment\n",
       "2                 and that is why, the lonesome day       -1.0\n",
       "8           and so on. then a worthless gaud or two       -1.0\n",
       "17      sounded o'er earth and sea its blast of war       -1.0\n",
       "37                   want and woe, which torture us       -1.0\n",
       "39           an echo returned on the cold gray morn       -1.0\n",
       "..                                              ...        ...\n",
       "874               in town, an' not the leanest runt       -1.0\n",
       "883                      by death's frequented ways       -1.0\n",
       "885                      rejection of his humanness       -1.0\n",
       "889            faint voices lifted shrill with pain       -1.0\n",
       "891  in the wild glens rough shepherds will deplore       -1.0\n",
       "\n",
       "[155 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative = df_raw[df_raw['Sentiment'] < 0]\n",
    "negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ca11d",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45cd3956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                 and that is why, the lonesome day\\n           and so on. then a worthless gaud or two\\n       sounded o'er earth and sea its blast of war\\n                    want and woe, which torture us\\n            an echo returned on the cold gray morn\\n       while i, ... i built up follies like a wall\\n          ah, what a pang of aching sharp surprise\\n                 and the old swallow-haunted barns\\n     the which she bearing home it burned her nest\\n    the crown of sorrow on their heads, their loss\\n               i lay and watched the lonely gloom;\\n          a sceptremonstrous, winged, intolerable.\\n while the rude winds blow off each shadowy crown.\\n         but o, nevermore can we prison him tight.\\n                 may meditate a whole youth's loss\\n        when thee, the eyes of that harsh long ago\\n        the foes inclosing, and his friend pursued\\nand bow to dread inquisitor and worship lords o...\\n        miles off, three dangerous miles, is home;\\n      else, sufferd, it will se\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = negative['Text'].to_string(index=False)\n",
    "raw_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46bd8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                 and that is why, the lonesome day\\n           and so on. then a worthless gaud or two\\n       sounded o'er earth and sea its blast of war\\n                    want and woe, which torture us\\n            an echo returned on the cold gray morn\\n       while i, ... i built up follies like a wall\\n          ah, what a pang of aching sharp surprise\\n                 and the old swallow-haunted barns\\n     the which she bearing home it burned her nest\\n    the crown of sorrow on their heads, their loss\\n               i lay and watched the lonely gloom;\\n          a sceptremonstrous, winged, intolerable.\\n while the rude winds blow off each shadowy crown.\\n         but o, nevermore can we prison him tight.\\n                 may meditate a whole youth's loss\\n        when thee, the eyes of that harsh long ago\\n        the foes inclosing, and his friend pursued\\nand bow to dread inquisitor and worship lords o...\\n        miles off, three dangerous miles, is home;\\n      else, sufferd, it will se\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all non-ASCII characters\n",
    "processed_text = re.sub(r'[^\\x00-\\x7f]', r'', raw_text).lower()\n",
    "processed_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd9097",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987aadca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word tokens: 1113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['and', 'that', 'is', 'why', 'the', 'lonesome', 'day', 'and', 'so', 'on']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get word tokens from text\n",
    "word_tokens = regexp_tokenize(processed_text, pattern=r'[^\\S\\r]+|[\\.,;!?()--_\"]', gaps=True)\n",
    "word_tokens.append('\\n')\n",
    "print(f\"Number of word tokens: {len(word_tokens)}\")\n",
    "word_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ff426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique word tokens: 615\n"
     ]
    }
   ],
   "source": [
    "# Get unique word tokens from word tokens\n",
    "unique_words = sorted(list(set(word_tokens)))\n",
    "print(f\"Number of unique word tokens: {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86037479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " \"'\",\n",
       " \"'twas\",\n",
       " 'a',\n",
       " 'accomplish',\n",
       " 'aching',\n",
       " 'added',\n",
       " 'adulterate',\n",
       " 'afar',\n",
       " 'after']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vocabulary of word tokens\n",
    "word_vocabulary = unique_words\n",
    "word_vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cf74d",
   "metadata": {},
   "source": [
    "### Create word-index mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81c158c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: \"'\",\n",
       " 2: \"'twas\",\n",
       " 3: 'a',\n",
       " 4: 'accomplish',\n",
       " 5: 'aching',\n",
       " 6: 'added',\n",
       " 7: 'adulterate',\n",
       " 8: 'afar',\n",
       " 9: 'after',\n",
       " 10: 'age',\n",
       " 11: 'ago',\n",
       " 12: 'ah',\n",
       " 13: 'air',\n",
       " 14: 'all',\n",
       " 15: 'altar',\n",
       " 16: 'always',\n",
       " 17: 'am',\n",
       " 18: 'among',\n",
       " 19: 'an',\n",
       " 20: \"an'\",\n",
       " 21: 'and',\n",
       " 22: 'angel',\n",
       " 23: 'answer',\n",
       " 24: 'anxious',\n",
       " 25: 'are',\n",
       " 26: 'around',\n",
       " 27: 'arrows',\n",
       " 28: 'as',\n",
       " 29: 'ashes',\n",
       " 30: 'ask',\n",
       " 31: 'at',\n",
       " 32: 'augurs',\n",
       " 33: 'avenging',\n",
       " 34: 'away',\n",
       " 35: 'b',\n",
       " 36: 'bad',\n",
       " 37: 'barns',\n",
       " 38: 'barrenly',\n",
       " 39: 'bat',\n",
       " 40: 'be',\n",
       " 41: 'beam',\n",
       " 42: 'bearing',\n",
       " 43: 'beat',\n",
       " 44: 'beaten',\n",
       " 45: 'because',\n",
       " 46: 'become',\n",
       " 47: 'beds',\n",
       " 48: 'been',\n",
       " 49: 'beguiled',\n",
       " 50: 'behold',\n",
       " 51: 'bitter',\n",
       " 52: 'black',\n",
       " 53: 'blankness',\n",
       " 54: 'blast',\n",
       " 55: 'bleeding',\n",
       " 56: 'blending',\n",
       " 57: 'blighting',\n",
       " 58: 'blind',\n",
       " 59: 'blindness',\n",
       " 60: 'blood',\n",
       " 61: 'blow',\n",
       " 62: 'body',\n",
       " 63: 'bound',\n",
       " 64: 'bow',\n",
       " 65: 'brand',\n",
       " 66: 'breast',\n",
       " 67: 'briareus',\n",
       " 68: 'broke',\n",
       " 69: 'built',\n",
       " 70: 'burned',\n",
       " 71: 'burning',\n",
       " 72: 'but',\n",
       " 73: 'by',\n",
       " 74: 'calumny',\n",
       " 75: 'came',\n",
       " 76: 'can',\n",
       " 77: \"captive's\",\n",
       " 78: 'changed',\n",
       " 79: 'children',\n",
       " 80: 'claims',\n",
       " 81: 'clatter',\n",
       " 82: 'clouds',\n",
       " 83: 'cloudy',\n",
       " 84: 'co',\n",
       " 85: 'cobweb',\n",
       " 86: 'cold',\n",
       " 87: 'comes',\n",
       " 88: 'corpses',\n",
       " 89: 'cou',\n",
       " 90: 'cough',\n",
       " 91: 'could',\n",
       " 92: 'course',\n",
       " 93: 'cries',\n",
       " 94: 'cripple',\n",
       " 95: 'cross',\n",
       " 96: 'crown',\n",
       " 97: 'cruell',\n",
       " 98: 'crush',\n",
       " 99: 'crushed',\n",
       " 100: 'cry',\n",
       " 101: 'curse',\n",
       " 102: 'daily',\n",
       " 103: 'damned',\n",
       " 104: 'danger',\n",
       " 105: 'dangerous',\n",
       " 106: 'dared',\n",
       " 107: 'dark',\n",
       " 108: 'darkness',\n",
       " 109: 'dawning',\n",
       " 110: 'day',\n",
       " 111: 'dead',\n",
       " 112: 'deadly',\n",
       " 113: 'deaf',\n",
       " 114: 'death',\n",
       " 115: \"death's\",\n",
       " 116: 'delusive',\n",
       " 117: 'deplore',\n",
       " 118: 'despair',\n",
       " 119: 'did',\n",
       " 120: 'didst',\n",
       " 121: 'dipped',\n",
       " 122: 'discontented',\n",
       " 123: 'distress',\n",
       " 124: 'disunion',\n",
       " 125: 'down',\n",
       " 126: 'dread',\n",
       " 127: 'dreadful',\n",
       " 128: 'drear',\n",
       " 129: \"drill's\",\n",
       " 130: 'droop',\n",
       " 131: 'dull',\n",
       " 132: 'dumb',\n",
       " 133: 'dwell',\n",
       " 134: \"e'en\",\n",
       " 135: 'each',\n",
       " 136: 'earth',\n",
       " 137: 'echo',\n",
       " 138: 'echoes',\n",
       " 139: 'else',\n",
       " 140: 'ending',\n",
       " 141: 'enemy',\n",
       " 142: 'enough',\n",
       " 143: 'envy',\n",
       " 144: 'etchings',\n",
       " 145: 'expire',\n",
       " 146: 'eyes',\n",
       " 147: 'faces',\n",
       " 148: 'faint',\n",
       " 149: 'fall',\n",
       " 150: 'false',\n",
       " 151: 'fatal',\n",
       " 152: 'fear',\n",
       " 153: 'fears',\n",
       " 154: 'featureless',\n",
       " 155: \"feel'st\",\n",
       " 156: 'fell',\n",
       " 157: 'fer',\n",
       " 158: 'fetters',\n",
       " 159: 'fiery',\n",
       " 160: 'fight',\n",
       " 161: 'fill',\n",
       " 162: 'fire',\n",
       " 163: 'fires',\n",
       " 164: 'flame',\n",
       " 165: 'flames',\n",
       " 166: 'flaming',\n",
       " 167: 'flicker',\n",
       " 168: 'flight',\n",
       " 169: 'flood',\n",
       " 170: 'flower',\n",
       " 171: 'foemen',\n",
       " 172: 'foes',\n",
       " 173: 'foggy',\n",
       " 174: 'follies',\n",
       " 175: 'food',\n",
       " 176: 'fooled',\n",
       " 177: 'for',\n",
       " 178: 'forcd',\n",
       " 179: 'force',\n",
       " 180: 'foredoomed',\n",
       " 181: 'forelaid',\n",
       " 182: 'forever',\n",
       " 183: 'forget',\n",
       " 184: 'forlorn',\n",
       " 185: 'fountain',\n",
       " 186: 'fraud',\n",
       " 187: 'fray',\n",
       " 188: 'frequented',\n",
       " 189: 'friend',\n",
       " 190: 'from',\n",
       " 191: 'frown',\n",
       " 192: 'frowning',\n",
       " 193: 'frowns',\n",
       " 194: 'fury',\n",
       " 195: 'future',\n",
       " 196: 'gallic',\n",
       " 197: 'gaud',\n",
       " 198: 'generations',\n",
       " 199: 'girl',\n",
       " 200: 'give',\n",
       " 201: 'glance',\n",
       " 202: 'glens',\n",
       " 203: 'gloom',\n",
       " 204: 'goaded',\n",
       " 205: \"god's\",\n",
       " 206: 'gods',\n",
       " 207: 'got',\n",
       " 208: 'gray',\n",
       " 209: 'great',\n",
       " 210: 'groom',\n",
       " 211: 'guarding',\n",
       " 212: 'guile',\n",
       " 213: 'guns',\n",
       " 214: 'had',\n",
       " 215: 'half',\n",
       " 216: 'hand',\n",
       " 217: 'hangings',\n",
       " 218: 'harsh',\n",
       " 219: 'has',\n",
       " 220: 'hast',\n",
       " 221: 'hate',\n",
       " 222: 'haunted',\n",
       " 223: 'have',\n",
       " 224: 'he',\n",
       " 225: 'heads',\n",
       " 226: 'heap',\n",
       " 227: 'heart',\n",
       " 228: 'hearthstone',\n",
       " 229: 'hearts',\n",
       " 230: 'heavy',\n",
       " 231: 'hee',\n",
       " 232: 'hell',\n",
       " 233: 'her',\n",
       " 234: 'here',\n",
       " 235: 'hi',\n",
       " 236: 'high',\n",
       " 237: 'him',\n",
       " 238: 'his',\n",
       " 239: 'hither',\n",
       " 240: 'home',\n",
       " 241: 'homesick',\n",
       " 242: 'hope',\n",
       " 243: 'house',\n",
       " 244: 'how',\n",
       " 245: 'howl',\n",
       " 246: 'howled',\n",
       " 247: 'human',\n",
       " 248: 'humanness',\n",
       " 249: 'hung',\n",
       " 250: 'i',\n",
       " 251: \"i've\",\n",
       " 252: 'if',\n",
       " 253: 'ignorant',\n",
       " 254: 'ill',\n",
       " 255: 'in',\n",
       " 256: 'inand',\n",
       " 257: 'inclosing',\n",
       " 258: 'inexorable',\n",
       " 259: 'infernal',\n",
       " 260: 'inquisitor',\n",
       " 261: 'into',\n",
       " 262: 'intolerable',\n",
       " 263: 'is',\n",
       " 264: 'it',\n",
       " 265: 'its',\n",
       " 266: 'jane',\n",
       " 267: 'kneeling',\n",
       " 268: 'known',\n",
       " 269: 'labyrinth',\n",
       " 270: 'land',\n",
       " 271: 'lap',\n",
       " 272: 'lave',\n",
       " 273: 'law',\n",
       " 274: 'lay',\n",
       " 275: 'leanest',\n",
       " 276: 'leaves',\n",
       " 277: 'left',\n",
       " 278: 'less',\n",
       " 279: 'let',\n",
       " 280: 'lie',\n",
       " 281: 'life',\n",
       " 282: 'lifted',\n",
       " 283: 'like',\n",
       " 284: 'line',\n",
       " 285: 'live',\n",
       " 286: 'liveliest',\n",
       " 287: 'lives',\n",
       " 288: 'load',\n",
       " 289: 'lone',\n",
       " 290: 'lonely',\n",
       " 291: 'lonesome',\n",
       " 292: 'long',\n",
       " 293: 'look',\n",
       " 294: 'lord',\n",
       " 295: 'lords',\n",
       " 296: 'loss',\n",
       " 297: 'lost',\n",
       " 298: 'lowly',\n",
       " 299: 'lucrece',\n",
       " 300: 'mad',\n",
       " 301: 'madness',\n",
       " 302: 'maidenhead',\n",
       " 303: 'make',\n",
       " 304: 'makes',\n",
       " 305: 'mankind',\n",
       " 306: 'may',\n",
       " 307: 'me',\n",
       " 308: 'meditate',\n",
       " 309: 'melancholy',\n",
       " 310: 'melt',\n",
       " 311: 'men',\n",
       " 312: 'might',\n",
       " 313: 'miles',\n",
       " 314: 'mine',\n",
       " 315: 'mission',\n",
       " 316: 'moaned',\n",
       " 317: 'mole',\n",
       " 318: 'moloch',\n",
       " 319: 'moment',\n",
       " 320: 'monkey',\n",
       " 321: 'monsters',\n",
       " 322: 'monstrous',\n",
       " 323: 'moods',\n",
       " 324: 'moon',\n",
       " 325: 'more',\n",
       " 326: 'morn',\n",
       " 327: 'most',\n",
       " 328: 'mourn',\n",
       " 329: 'mournfully',\n",
       " 330: \"mov'd\",\n",
       " 331: 'multiplied',\n",
       " 332: 'murmured',\n",
       " 333: 'murmuring',\n",
       " 334: 'must',\n",
       " 335: 'my',\n",
       " 336: \"n't\",\n",
       " 337: 'nailed',\n",
       " 338: 'name',\n",
       " 339: \"ne'er\",\n",
       " 340: 'nearest',\n",
       " 341: 'nerve',\n",
       " 342: 'nerves',\n",
       " 343: 'nest',\n",
       " 344: 'never',\n",
       " 345: 'nevermore',\n",
       " 346: 'nice',\n",
       " 347: 'night',\n",
       " 348: 'no',\n",
       " 349: 'none',\n",
       " 350: 'not',\n",
       " 351: 'nothing',\n",
       " 352: 'now',\n",
       " 353: 'nymph',\n",
       " 354: 'o',\n",
       " 355: \"o'er\",\n",
       " 356: 'obliterate',\n",
       " 357: \"oblivion's\",\n",
       " 358: 'of',\n",
       " 359: 'off',\n",
       " 360: 'offspring',\n",
       " 361: 'old',\n",
       " 362: 'on',\n",
       " 363: 'once',\n",
       " 364: 'ones',\n",
       " 365: 'open',\n",
       " 366: 'or',\n",
       " 367: 'our',\n",
       " 368: 'out',\n",
       " 369: 'pain',\n",
       " 370: 'pallid',\n",
       " 371: 'pang',\n",
       " 372: 'passing',\n",
       " 373: 'pathetic',\n",
       " 374: 'people',\n",
       " 375: 'perish:',\n",
       " 376: 'phrases',\n",
       " 377: 'pillars',\n",
       " 378: 'pity',\n",
       " 379: 'poor',\n",
       " 380: 'portended',\n",
       " 381: 'pride',\n",
       " 382: 'priests',\n",
       " 383: 'prison',\n",
       " 384: 'promises',\n",
       " 385: 'pu',\n",
       " 386: 'pursued',\n",
       " 387: 'quickened',\n",
       " 388: 'quit',\n",
       " 389: 'quivering',\n",
       " 390: 'rage',\n",
       " 391: 'ran',\n",
       " 392: 'ranges',\n",
       " 393: 'rave',\n",
       " 394: 'receave',\n",
       " 395: 'regin',\n",
       " 396: 'rejection',\n",
       " 397: 'rest',\n",
       " 398: 'returned',\n",
       " 399: 'ridiculous',\n",
       " 400: 'right',\n",
       " 401: 'ripe',\n",
       " 402: 'rise',\n",
       " 403: 'rocky',\n",
       " 404: 'roll',\n",
       " 405: 'root',\n",
       " 406: 'roots',\n",
       " 407: 'rough',\n",
       " 408: 'rude',\n",
       " 409: 'ruin',\n",
       " 410: 'runt',\n",
       " 411: 'sad',\n",
       " 412: 'said',\n",
       " 413: 'sat',\n",
       " 414: 'satraps',\n",
       " 415: 'savages',\n",
       " 416: 'save',\n",
       " 417: 'scare',\n",
       " 418: 'scarlet',\n",
       " 419: 'scatters',\n",
       " 420: 'sceptremonstrous',\n",
       " 421: 'scorn',\n",
       " 422: 'sea',\n",
       " 423: 'seditious',\n",
       " 424: 'see',\n",
       " 425: 'seek',\n",
       " 426: 'seem',\n",
       " 427: 'seemed',\n",
       " 428: 'seen',\n",
       " 429: 'selfishness',\n",
       " 430: 'sent',\n",
       " 431: 'serious',\n",
       " 432: 'set',\n",
       " 433: \"sha'\",\n",
       " 434: 'shadow',\n",
       " 435: 'shadows:',\n",
       " 436: 'shadowy',\n",
       " 437: 'shall',\n",
       " 438: 'sharp',\n",
       " 439: 'she',\n",
       " 440: 'shepherds',\n",
       " 441: 'shivered',\n",
       " 442: 'shores',\n",
       " 443: 'shouting',\n",
       " 444: 'shrill',\n",
       " 445: 'shun',\n",
       " 446: 'silk',\n",
       " 447: 'silly',\n",
       " 448: 'sitteth',\n",
       " 449: 'slaughtering',\n",
       " 450: 'slave',\n",
       " 451: 'slavery',\n",
       " 452: 'slaying',\n",
       " 453: 'sleep',\n",
       " 454: 'sleeps',\n",
       " 455: 'slumber',\n",
       " 456: 'smile',\n",
       " 457: 'smother',\n",
       " 458: 'smouldering',\n",
       " 459: 'so',\n",
       " 460: 'soft',\n",
       " 461: 'solemn',\n",
       " 462: 'some',\n",
       " 463: 'sophists',\n",
       " 464: 'sorrow',\n",
       " 465: \"sorrow's\",\n",
       " 466: 'sorrowful',\n",
       " 467: 'sorrows',\n",
       " 468: 'sounded',\n",
       " 469: 'sounds',\n",
       " 470: 'spake',\n",
       " 471: 'spied',\n",
       " 472: 'spoiled',\n",
       " 473: 'stars',\n",
       " 474: 'steal',\n",
       " 475: 'steerd',\n",
       " 476: 'stiff',\n",
       " 477: 'stifling',\n",
       " 478: 'still',\n",
       " 479: 'stockings',\n",
       " 480: 'stole',\n",
       " 481: 'stone',\n",
       " 482: 'stormy',\n",
       " 483: 'strange',\n",
       " 484: 'strangled',\n",
       " 485: 'streets',\n",
       " 486: 'strings',\n",
       " 487: 'strove',\n",
       " 488: 'struggling',\n",
       " 489: 'such',\n",
       " 490: 'sufferd',\n",
       " 491: 'suicide',\n",
       " 492: 'summons',\n",
       " 493: 'sun',\n",
       " 494: 'surprise',\n",
       " 495: 'swallow',\n",
       " 496: 'sweep',\n",
       " 497: 'sword',\n",
       " 498: 'taken',\n",
       " 499: 'task',\n",
       " 500: 'taught',\n",
       " 501: 'tears',\n",
       " 502: 'tempests',\n",
       " 503: 'th',\n",
       " 504: 'than',\n",
       " 505: 'that',\n",
       " 506: 'the',\n",
       " 507: 'thee',\n",
       " 508: 'their',\n",
       " 509: 'them',\n",
       " 510: 'then',\n",
       " 511: 'there',\n",
       " 512: 'these',\n",
       " 513: 'they',\n",
       " 514: 'thine',\n",
       " 515: 'things',\n",
       " 516: 'think',\n",
       " 517: 'this',\n",
       " 518: 'those',\n",
       " 519: 'thou',\n",
       " 520: 'though',\n",
       " 521: 'thousands',\n",
       " 522: 'threatening',\n",
       " 523: 'three',\n",
       " 524: 'thro',\n",
       " 525: 'throbbing',\n",
       " 526: 'through',\n",
       " 527: 'throughout',\n",
       " 528: 'throw',\n",
       " 529: 'thrown',\n",
       " 530: 'thunder',\n",
       " 531: 'thus',\n",
       " 532: 'thy',\n",
       " 533: 'tide',\n",
       " 534: 'tight',\n",
       " 535: 'till',\n",
       " 536: 'time',\n",
       " 537: \"time's\",\n",
       " 538: 'tinkling',\n",
       " 539: 'tired',\n",
       " 540: 'to',\n",
       " 541: 'tormented',\n",
       " 542: 'torn',\n",
       " 543: 'torture',\n",
       " 544: 'town',\n",
       " 545: 'trailing',\n",
       " 546: 'treatin',\n",
       " 547: 'troubling',\n",
       " 548: 'tumbling',\n",
       " 549: 'twas',\n",
       " 550: 'twilight',\n",
       " 551: 'two',\n",
       " 552: 'unloved',\n",
       " 553: 'until',\n",
       " 554: 'up',\n",
       " 555: 'us',\n",
       " 556: 'utters',\n",
       " 557: 'vain',\n",
       " 558: 'vapours',\n",
       " 559: 'vehement',\n",
       " 560: 'visual',\n",
       " 561: 'voices',\n",
       " 562: 'wake',\n",
       " 563: 'wall',\n",
       " 564: 'wanderings',\n",
       " 565: 'want',\n",
       " 566: 'war',\n",
       " 567: 'warlike',\n",
       " 568: 'warning',\n",
       " 569: 'was',\n",
       " 570: 'waste',\n",
       " 571: 'watched',\n",
       " 572: 'waters',\n",
       " 573: 'waver',\n",
       " 574: 'ways',\n",
       " 575: 'we',\n",
       " 576: 'weak',\n",
       " 577: 'weight:',\n",
       " 578: 'weird',\n",
       " 579: 'were',\n",
       " 580: 'what',\n",
       " 581: 'wheeze',\n",
       " 582: 'when',\n",
       " 583: 'where',\n",
       " 584: 'which',\n",
       " 585: 'while',\n",
       " 586: 'who',\n",
       " 587: 'whole',\n",
       " 588: 'why',\n",
       " 589: 'wild',\n",
       " 590: 'will',\n",
       " 591: 'wilt',\n",
       " 592: 'winds',\n",
       " 593: 'winged',\n",
       " 594: 'winter',\n",
       " 595: 'with',\n",
       " 596: 'withered',\n",
       " 597: 'within',\n",
       " 598: 'woe',\n",
       " 599: 'woman',\n",
       " 600: 'word',\n",
       " 601: 'words',\n",
       " 602: 'world',\n",
       " 603: 'worship',\n",
       " 604: 'worthless',\n",
       " 605: 'would',\n",
       " 606: 'wrack',\n",
       " 607: 'wrecked',\n",
       " 608: 'wrinkles',\n",
       " 609: 'writ',\n",
       " 610: 'wrong',\n",
       " 611: \"yo'\",\n",
       " 612: 'you',\n",
       " 613: 'your',\n",
       " 614: \"youth's\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create index-word mappings \n",
    "indices_words = dict((index, word) for index, word in enumerate(unique_words))\n",
    "indices_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e485f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " \"'\": 1,\n",
       " \"'twas\": 2,\n",
       " 'a': 3,\n",
       " 'accomplish': 4,\n",
       " 'aching': 5,\n",
       " 'added': 6,\n",
       " 'adulterate': 7,\n",
       " 'afar': 8,\n",
       " 'after': 9,\n",
       " 'age': 10,\n",
       " 'ago': 11,\n",
       " 'ah': 12,\n",
       " 'air': 13,\n",
       " 'all': 14,\n",
       " 'altar': 15,\n",
       " 'always': 16,\n",
       " 'am': 17,\n",
       " 'among': 18,\n",
       " 'an': 19,\n",
       " \"an'\": 20,\n",
       " 'and': 21,\n",
       " 'angel': 22,\n",
       " 'answer': 23,\n",
       " 'anxious': 24,\n",
       " 'are': 25,\n",
       " 'around': 26,\n",
       " 'arrows': 27,\n",
       " 'as': 28,\n",
       " 'ashes': 29,\n",
       " 'ask': 30,\n",
       " 'at': 31,\n",
       " 'augurs': 32,\n",
       " 'avenging': 33,\n",
       " 'away': 34,\n",
       " 'b': 35,\n",
       " 'bad': 36,\n",
       " 'barns': 37,\n",
       " 'barrenly': 38,\n",
       " 'bat': 39,\n",
       " 'be': 40,\n",
       " 'beam': 41,\n",
       " 'bearing': 42,\n",
       " 'beat': 43,\n",
       " 'beaten': 44,\n",
       " 'because': 45,\n",
       " 'become': 46,\n",
       " 'beds': 47,\n",
       " 'been': 48,\n",
       " 'beguiled': 49,\n",
       " 'behold': 50,\n",
       " 'bitter': 51,\n",
       " 'black': 52,\n",
       " 'blankness': 53,\n",
       " 'blast': 54,\n",
       " 'bleeding': 55,\n",
       " 'blending': 56,\n",
       " 'blighting': 57,\n",
       " 'blind': 58,\n",
       " 'blindness': 59,\n",
       " 'blood': 60,\n",
       " 'blow': 61,\n",
       " 'body': 62,\n",
       " 'bound': 63,\n",
       " 'bow': 64,\n",
       " 'brand': 65,\n",
       " 'breast': 66,\n",
       " 'briareus': 67,\n",
       " 'broke': 68,\n",
       " 'built': 69,\n",
       " 'burned': 70,\n",
       " 'burning': 71,\n",
       " 'but': 72,\n",
       " 'by': 73,\n",
       " 'calumny': 74,\n",
       " 'came': 75,\n",
       " 'can': 76,\n",
       " \"captive's\": 77,\n",
       " 'changed': 78,\n",
       " 'children': 79,\n",
       " 'claims': 80,\n",
       " 'clatter': 81,\n",
       " 'clouds': 82,\n",
       " 'cloudy': 83,\n",
       " 'co': 84,\n",
       " 'cobweb': 85,\n",
       " 'cold': 86,\n",
       " 'comes': 87,\n",
       " 'corpses': 88,\n",
       " 'cou': 89,\n",
       " 'cough': 90,\n",
       " 'could': 91,\n",
       " 'course': 92,\n",
       " 'cries': 93,\n",
       " 'cripple': 94,\n",
       " 'cross': 95,\n",
       " 'crown': 96,\n",
       " 'cruell': 97,\n",
       " 'crush': 98,\n",
       " 'crushed': 99,\n",
       " 'cry': 100,\n",
       " 'curse': 101,\n",
       " 'daily': 102,\n",
       " 'damned': 103,\n",
       " 'danger': 104,\n",
       " 'dangerous': 105,\n",
       " 'dared': 106,\n",
       " 'dark': 107,\n",
       " 'darkness': 108,\n",
       " 'dawning': 109,\n",
       " 'day': 110,\n",
       " 'dead': 111,\n",
       " 'deadly': 112,\n",
       " 'deaf': 113,\n",
       " 'death': 114,\n",
       " \"death's\": 115,\n",
       " 'delusive': 116,\n",
       " 'deplore': 117,\n",
       " 'despair': 118,\n",
       " 'did': 119,\n",
       " 'didst': 120,\n",
       " 'dipped': 121,\n",
       " 'discontented': 122,\n",
       " 'distress': 123,\n",
       " 'disunion': 124,\n",
       " 'down': 125,\n",
       " 'dread': 126,\n",
       " 'dreadful': 127,\n",
       " 'drear': 128,\n",
       " \"drill's\": 129,\n",
       " 'droop': 130,\n",
       " 'dull': 131,\n",
       " 'dumb': 132,\n",
       " 'dwell': 133,\n",
       " \"e'en\": 134,\n",
       " 'each': 135,\n",
       " 'earth': 136,\n",
       " 'echo': 137,\n",
       " 'echoes': 138,\n",
       " 'else': 139,\n",
       " 'ending': 140,\n",
       " 'enemy': 141,\n",
       " 'enough': 142,\n",
       " 'envy': 143,\n",
       " 'etchings': 144,\n",
       " 'expire': 145,\n",
       " 'eyes': 146,\n",
       " 'faces': 147,\n",
       " 'faint': 148,\n",
       " 'fall': 149,\n",
       " 'false': 150,\n",
       " 'fatal': 151,\n",
       " 'fear': 152,\n",
       " 'fears': 153,\n",
       " 'featureless': 154,\n",
       " \"feel'st\": 155,\n",
       " 'fell': 156,\n",
       " 'fer': 157,\n",
       " 'fetters': 158,\n",
       " 'fiery': 159,\n",
       " 'fight': 160,\n",
       " 'fill': 161,\n",
       " 'fire': 162,\n",
       " 'fires': 163,\n",
       " 'flame': 164,\n",
       " 'flames': 165,\n",
       " 'flaming': 166,\n",
       " 'flicker': 167,\n",
       " 'flight': 168,\n",
       " 'flood': 169,\n",
       " 'flower': 170,\n",
       " 'foemen': 171,\n",
       " 'foes': 172,\n",
       " 'foggy': 173,\n",
       " 'follies': 174,\n",
       " 'food': 175,\n",
       " 'fooled': 176,\n",
       " 'for': 177,\n",
       " 'forcd': 178,\n",
       " 'force': 179,\n",
       " 'foredoomed': 180,\n",
       " 'forelaid': 181,\n",
       " 'forever': 182,\n",
       " 'forget': 183,\n",
       " 'forlorn': 184,\n",
       " 'fountain': 185,\n",
       " 'fraud': 186,\n",
       " 'fray': 187,\n",
       " 'frequented': 188,\n",
       " 'friend': 189,\n",
       " 'from': 190,\n",
       " 'frown': 191,\n",
       " 'frowning': 192,\n",
       " 'frowns': 193,\n",
       " 'fury': 194,\n",
       " 'future': 195,\n",
       " 'gallic': 196,\n",
       " 'gaud': 197,\n",
       " 'generations': 198,\n",
       " 'girl': 199,\n",
       " 'give': 200,\n",
       " 'glance': 201,\n",
       " 'glens': 202,\n",
       " 'gloom': 203,\n",
       " 'goaded': 204,\n",
       " \"god's\": 205,\n",
       " 'gods': 206,\n",
       " 'got': 207,\n",
       " 'gray': 208,\n",
       " 'great': 209,\n",
       " 'groom': 210,\n",
       " 'guarding': 211,\n",
       " 'guile': 212,\n",
       " 'guns': 213,\n",
       " 'had': 214,\n",
       " 'half': 215,\n",
       " 'hand': 216,\n",
       " 'hangings': 217,\n",
       " 'harsh': 218,\n",
       " 'has': 219,\n",
       " 'hast': 220,\n",
       " 'hate': 221,\n",
       " 'haunted': 222,\n",
       " 'have': 223,\n",
       " 'he': 224,\n",
       " 'heads': 225,\n",
       " 'heap': 226,\n",
       " 'heart': 227,\n",
       " 'hearthstone': 228,\n",
       " 'hearts': 229,\n",
       " 'heavy': 230,\n",
       " 'hee': 231,\n",
       " 'hell': 232,\n",
       " 'her': 233,\n",
       " 'here': 234,\n",
       " 'hi': 235,\n",
       " 'high': 236,\n",
       " 'him': 237,\n",
       " 'his': 238,\n",
       " 'hither': 239,\n",
       " 'home': 240,\n",
       " 'homesick': 241,\n",
       " 'hope': 242,\n",
       " 'house': 243,\n",
       " 'how': 244,\n",
       " 'howl': 245,\n",
       " 'howled': 246,\n",
       " 'human': 247,\n",
       " 'humanness': 248,\n",
       " 'hung': 249,\n",
       " 'i': 250,\n",
       " \"i've\": 251,\n",
       " 'if': 252,\n",
       " 'ignorant': 253,\n",
       " 'ill': 254,\n",
       " 'in': 255,\n",
       " 'inand': 256,\n",
       " 'inclosing': 257,\n",
       " 'inexorable': 258,\n",
       " 'infernal': 259,\n",
       " 'inquisitor': 260,\n",
       " 'into': 261,\n",
       " 'intolerable': 262,\n",
       " 'is': 263,\n",
       " 'it': 264,\n",
       " 'its': 265,\n",
       " 'jane': 266,\n",
       " 'kneeling': 267,\n",
       " 'known': 268,\n",
       " 'labyrinth': 269,\n",
       " 'land': 270,\n",
       " 'lap': 271,\n",
       " 'lave': 272,\n",
       " 'law': 273,\n",
       " 'lay': 274,\n",
       " 'leanest': 275,\n",
       " 'leaves': 276,\n",
       " 'left': 277,\n",
       " 'less': 278,\n",
       " 'let': 279,\n",
       " 'lie': 280,\n",
       " 'life': 281,\n",
       " 'lifted': 282,\n",
       " 'like': 283,\n",
       " 'line': 284,\n",
       " 'live': 285,\n",
       " 'liveliest': 286,\n",
       " 'lives': 287,\n",
       " 'load': 288,\n",
       " 'lone': 289,\n",
       " 'lonely': 290,\n",
       " 'lonesome': 291,\n",
       " 'long': 292,\n",
       " 'look': 293,\n",
       " 'lord': 294,\n",
       " 'lords': 295,\n",
       " 'loss': 296,\n",
       " 'lost': 297,\n",
       " 'lowly': 298,\n",
       " 'lucrece': 299,\n",
       " 'mad': 300,\n",
       " 'madness': 301,\n",
       " 'maidenhead': 302,\n",
       " 'make': 303,\n",
       " 'makes': 304,\n",
       " 'mankind': 305,\n",
       " 'may': 306,\n",
       " 'me': 307,\n",
       " 'meditate': 308,\n",
       " 'melancholy': 309,\n",
       " 'melt': 310,\n",
       " 'men': 311,\n",
       " 'might': 312,\n",
       " 'miles': 313,\n",
       " 'mine': 314,\n",
       " 'mission': 315,\n",
       " 'moaned': 316,\n",
       " 'mole': 317,\n",
       " 'moloch': 318,\n",
       " 'moment': 319,\n",
       " 'monkey': 320,\n",
       " 'monsters': 321,\n",
       " 'monstrous': 322,\n",
       " 'moods': 323,\n",
       " 'moon': 324,\n",
       " 'more': 325,\n",
       " 'morn': 326,\n",
       " 'most': 327,\n",
       " 'mourn': 328,\n",
       " 'mournfully': 329,\n",
       " \"mov'd\": 330,\n",
       " 'multiplied': 331,\n",
       " 'murmured': 332,\n",
       " 'murmuring': 333,\n",
       " 'must': 334,\n",
       " 'my': 335,\n",
       " \"n't\": 336,\n",
       " 'nailed': 337,\n",
       " 'name': 338,\n",
       " \"ne'er\": 339,\n",
       " 'nearest': 340,\n",
       " 'nerve': 341,\n",
       " 'nerves': 342,\n",
       " 'nest': 343,\n",
       " 'never': 344,\n",
       " 'nevermore': 345,\n",
       " 'nice': 346,\n",
       " 'night': 347,\n",
       " 'no': 348,\n",
       " 'none': 349,\n",
       " 'not': 350,\n",
       " 'nothing': 351,\n",
       " 'now': 352,\n",
       " 'nymph': 353,\n",
       " 'o': 354,\n",
       " \"o'er\": 355,\n",
       " 'obliterate': 356,\n",
       " \"oblivion's\": 357,\n",
       " 'of': 358,\n",
       " 'off': 359,\n",
       " 'offspring': 360,\n",
       " 'old': 361,\n",
       " 'on': 362,\n",
       " 'once': 363,\n",
       " 'ones': 364,\n",
       " 'open': 365,\n",
       " 'or': 366,\n",
       " 'our': 367,\n",
       " 'out': 368,\n",
       " 'pain': 369,\n",
       " 'pallid': 370,\n",
       " 'pang': 371,\n",
       " 'passing': 372,\n",
       " 'pathetic': 373,\n",
       " 'people': 374,\n",
       " 'perish:': 375,\n",
       " 'phrases': 376,\n",
       " 'pillars': 377,\n",
       " 'pity': 378,\n",
       " 'poor': 379,\n",
       " 'portended': 380,\n",
       " 'pride': 381,\n",
       " 'priests': 382,\n",
       " 'prison': 383,\n",
       " 'promises': 384,\n",
       " 'pu': 385,\n",
       " 'pursued': 386,\n",
       " 'quickened': 387,\n",
       " 'quit': 388,\n",
       " 'quivering': 389,\n",
       " 'rage': 390,\n",
       " 'ran': 391,\n",
       " 'ranges': 392,\n",
       " 'rave': 393,\n",
       " 'receave': 394,\n",
       " 'regin': 395,\n",
       " 'rejection': 396,\n",
       " 'rest': 397,\n",
       " 'returned': 398,\n",
       " 'ridiculous': 399,\n",
       " 'right': 400,\n",
       " 'ripe': 401,\n",
       " 'rise': 402,\n",
       " 'rocky': 403,\n",
       " 'roll': 404,\n",
       " 'root': 405,\n",
       " 'roots': 406,\n",
       " 'rough': 407,\n",
       " 'rude': 408,\n",
       " 'ruin': 409,\n",
       " 'runt': 410,\n",
       " 'sad': 411,\n",
       " 'said': 412,\n",
       " 'sat': 413,\n",
       " 'satraps': 414,\n",
       " 'savages': 415,\n",
       " 'save': 416,\n",
       " 'scare': 417,\n",
       " 'scarlet': 418,\n",
       " 'scatters': 419,\n",
       " 'sceptremonstrous': 420,\n",
       " 'scorn': 421,\n",
       " 'sea': 422,\n",
       " 'seditious': 423,\n",
       " 'see': 424,\n",
       " 'seek': 425,\n",
       " 'seem': 426,\n",
       " 'seemed': 427,\n",
       " 'seen': 428,\n",
       " 'selfishness': 429,\n",
       " 'sent': 430,\n",
       " 'serious': 431,\n",
       " 'set': 432,\n",
       " \"sha'\": 433,\n",
       " 'shadow': 434,\n",
       " 'shadows:': 435,\n",
       " 'shadowy': 436,\n",
       " 'shall': 437,\n",
       " 'sharp': 438,\n",
       " 'she': 439,\n",
       " 'shepherds': 440,\n",
       " 'shivered': 441,\n",
       " 'shores': 442,\n",
       " 'shouting': 443,\n",
       " 'shrill': 444,\n",
       " 'shun': 445,\n",
       " 'silk': 446,\n",
       " 'silly': 447,\n",
       " 'sitteth': 448,\n",
       " 'slaughtering': 449,\n",
       " 'slave': 450,\n",
       " 'slavery': 451,\n",
       " 'slaying': 452,\n",
       " 'sleep': 453,\n",
       " 'sleeps': 454,\n",
       " 'slumber': 455,\n",
       " 'smile': 456,\n",
       " 'smother': 457,\n",
       " 'smouldering': 458,\n",
       " 'so': 459,\n",
       " 'soft': 460,\n",
       " 'solemn': 461,\n",
       " 'some': 462,\n",
       " 'sophists': 463,\n",
       " 'sorrow': 464,\n",
       " \"sorrow's\": 465,\n",
       " 'sorrowful': 466,\n",
       " 'sorrows': 467,\n",
       " 'sounded': 468,\n",
       " 'sounds': 469,\n",
       " 'spake': 470,\n",
       " 'spied': 471,\n",
       " 'spoiled': 472,\n",
       " 'stars': 473,\n",
       " 'steal': 474,\n",
       " 'steerd': 475,\n",
       " 'stiff': 476,\n",
       " 'stifling': 477,\n",
       " 'still': 478,\n",
       " 'stockings': 479,\n",
       " 'stole': 480,\n",
       " 'stone': 481,\n",
       " 'stormy': 482,\n",
       " 'strange': 483,\n",
       " 'strangled': 484,\n",
       " 'streets': 485,\n",
       " 'strings': 486,\n",
       " 'strove': 487,\n",
       " 'struggling': 488,\n",
       " 'such': 489,\n",
       " 'sufferd': 490,\n",
       " 'suicide': 491,\n",
       " 'summons': 492,\n",
       " 'sun': 493,\n",
       " 'surprise': 494,\n",
       " 'swallow': 495,\n",
       " 'sweep': 496,\n",
       " 'sword': 497,\n",
       " 'taken': 498,\n",
       " 'task': 499,\n",
       " 'taught': 500,\n",
       " 'tears': 501,\n",
       " 'tempests': 502,\n",
       " 'th': 503,\n",
       " 'than': 504,\n",
       " 'that': 505,\n",
       " 'the': 506,\n",
       " 'thee': 507,\n",
       " 'their': 508,\n",
       " 'them': 509,\n",
       " 'then': 510,\n",
       " 'there': 511,\n",
       " 'these': 512,\n",
       " 'they': 513,\n",
       " 'thine': 514,\n",
       " 'things': 515,\n",
       " 'think': 516,\n",
       " 'this': 517,\n",
       " 'those': 518,\n",
       " 'thou': 519,\n",
       " 'though': 520,\n",
       " 'thousands': 521,\n",
       " 'threatening': 522,\n",
       " 'three': 523,\n",
       " 'thro': 524,\n",
       " 'throbbing': 525,\n",
       " 'through': 526,\n",
       " 'throughout': 527,\n",
       " 'throw': 528,\n",
       " 'thrown': 529,\n",
       " 'thunder': 530,\n",
       " 'thus': 531,\n",
       " 'thy': 532,\n",
       " 'tide': 533,\n",
       " 'tight': 534,\n",
       " 'till': 535,\n",
       " 'time': 536,\n",
       " \"time's\": 537,\n",
       " 'tinkling': 538,\n",
       " 'tired': 539,\n",
       " 'to': 540,\n",
       " 'tormented': 541,\n",
       " 'torn': 542,\n",
       " 'torture': 543,\n",
       " 'town': 544,\n",
       " 'trailing': 545,\n",
       " 'treatin': 546,\n",
       " 'troubling': 547,\n",
       " 'tumbling': 548,\n",
       " 'twas': 549,\n",
       " 'twilight': 550,\n",
       " 'two': 551,\n",
       " 'unloved': 552,\n",
       " 'until': 553,\n",
       " 'up': 554,\n",
       " 'us': 555,\n",
       " 'utters': 556,\n",
       " 'vain': 557,\n",
       " 'vapours': 558,\n",
       " 'vehement': 559,\n",
       " 'visual': 560,\n",
       " 'voices': 561,\n",
       " 'wake': 562,\n",
       " 'wall': 563,\n",
       " 'wanderings': 564,\n",
       " 'want': 565,\n",
       " 'war': 566,\n",
       " 'warlike': 567,\n",
       " 'warning': 568,\n",
       " 'was': 569,\n",
       " 'waste': 570,\n",
       " 'watched': 571,\n",
       " 'waters': 572,\n",
       " 'waver': 573,\n",
       " 'ways': 574,\n",
       " 'we': 575,\n",
       " 'weak': 576,\n",
       " 'weight:': 577,\n",
       " 'weird': 578,\n",
       " 'were': 579,\n",
       " 'what': 580,\n",
       " 'wheeze': 581,\n",
       " 'when': 582,\n",
       " 'where': 583,\n",
       " 'which': 584,\n",
       " 'while': 585,\n",
       " 'who': 586,\n",
       " 'whole': 587,\n",
       " 'why': 588,\n",
       " 'wild': 589,\n",
       " 'will': 590,\n",
       " 'wilt': 591,\n",
       " 'winds': 592,\n",
       " 'winged': 593,\n",
       " 'winter': 594,\n",
       " 'with': 595,\n",
       " 'withered': 596,\n",
       " 'within': 597,\n",
       " 'woe': 598,\n",
       " 'woman': 599,\n",
       " 'word': 600,\n",
       " 'words': 601,\n",
       " 'world': 602,\n",
       " 'worship': 603,\n",
       " 'worthless': 604,\n",
       " 'would': 605,\n",
       " 'wrack': 606,\n",
       " 'wrecked': 607,\n",
       " 'wrinkles': 608,\n",
       " 'writ': 609,\n",
       " 'wrong': 610,\n",
       " \"yo'\": 611,\n",
       " 'you': 612,\n",
       " 'your': 613,\n",
       " \"youth's\": 614}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create word-index mappings\n",
    "word_indices = dict((word, index) for index, word in enumerate(unique_words))\n",
    "word_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6413184",
   "metadata": {},
   "source": [
    "### Create Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5afa53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x (input): Split text into blocks, where each block has the same amount of words\n",
    "# Create y (targets): For each x input, the y is the word that comes next\n",
    "# The model should learn to predict y from the input x\n",
    "\n",
    "block_size = 2\n",
    "step = 1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(word_tokens) - block_size, step):\n",
    "    x.append(word_tokens[i: i+block_size])\n",
    "    y.append(word_tokens[i + block_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83d0d331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['and', 'that'],\n",
       " ['that', 'is'],\n",
       " ['is', 'why'],\n",
       " ['why', 'the'],\n",
       " ['the', 'lonesome']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect x\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d1dd331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of blocks\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c87bed",
   "metadata": {},
   "source": [
    "### Create One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ba28e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoding of x\n",
    "x_encoded = []\n",
    "\n",
    "for x_arr in x:\n",
    "    x_ints = [word_indices[item] for item in x_arr]\n",
    "    \n",
    "    x_row = []\n",
    "    for item in x_ints:\n",
    "        x_vector = np.zeros(len(unique_words))\n",
    "        x_vector[item] = 1\n",
    "        x_row.append(x_vector)\n",
    "        \n",
    "    x_encoded.append(x_row)\n",
    "    \n",
    "x_encoded = np.array(x_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e351e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'why', 'the', 'lonesome', 'day']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect y\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea6c112d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[263, 588, 506, 291, 110]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each word in y into their corresponding indices\n",
    "y_ints = [word_indices[item] for item in y]\n",
    "y_ints[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fc0bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoding of y\n",
    "y_encoded = []\n",
    "\n",
    "for item in y_ints:\n",
    "    y_vector = np.zeros(len(unique_words))\n",
    "    y_vector[item] = 1\n",
    "    y_encoded.append(y_vector)\n",
    "\n",
    "y_encoded = np.array(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcda4bf",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caa21464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Linear(input_dim, 2000)\n",
    "        self.hidden = nn.Linear(2000, 1200)\n",
    "        self.output = nn.Linear(1200, output_dim)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.embeddings(x))\n",
    "        x = self.tanh(self.hidden(x))\n",
    "        x = self.softmax(self.output(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6ff9f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1230\n"
     ]
    }
   ],
   "source": [
    "# Get size of input for training the model\n",
    "input_size = x_encoded[0].ravel().shape[0]\n",
    "print(x_encoded[0].ravel().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df58af12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing torch operations on cpu device\n"
     ]
    }
   ],
   "source": [
    "# Allocate tensors to the device used for computation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Performing torch operations on {device} device\")\n",
    "\n",
    "# Create x and y PyTorch tensors\n",
    "x = torch.tensor(x_encoded).float().to(device)\n",
    "y = torch.tensor(y_encoded).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb023182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextGenerator(\n",
       "  (embeddings): Linear(in_features=1230, out_features=2000, bias=True)\n",
       "  (hidden): Linear(in_features=2000, out_features=1200, bias=True)\n",
       "  (output): Linear(in_features=1200, out_features=615, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = TextGenerator(input_size, len(unique_words), block_size).to(device)\n",
    "\n",
    "# Print model configuration\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "433009b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ee59c",
   "metadata": {},
   "source": [
    "### Create Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9e8b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom Dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        self.n_samples = len(x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index].ravel(), self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9228e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset using custom Dataset class\n",
    "training_ds = CustomDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a488527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset into DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    training_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03558f02",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a82f8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to train model\n",
    "def train_fn(loader, model, optimizer, loss_fn, device):\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    ave_loss = 0\n",
    "    count = 0 \n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        # Forward\n",
    "        predictions = model.forward(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update tqdm loading bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        count += 1\n",
    "        ave_loss += loss.item()\n",
    "    \n",
    "    ave_loss = ave_loss / count\n",
    "\n",
    "    return ave_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7811b5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 112/112 [00:06<00:00, 17.82it/s, loss=6.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.400432322706495\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 112/112 [00:06<00:00, 17.50it/s, loss=6.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.363841950893402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 2 # TODO: CHANGE TO 300 ON FINAL DATA; CURRENTLY 2 FOR TESTING PURPOSES\n",
    "average_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    ave_loss = train_fn(train_loader, model, optimizer, criterion, device)\n",
    "    \n",
    "    print(\"Ave Loss: {}\".format(ave_loss))\n",
    "    average_losses.append(ave_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e315a81",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06dd01d2-95c1-4264-8a39-4d013120d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, num_words):\n",
    "    device = 'cpu'\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert seed_text to input tensor\n",
    "    seed_encoded = []\n",
    "    for word in seed_text.split():\n",
    "        word_index = word_indices[word]\n",
    "        word_encoded = np.zeros(len(unique_words))\n",
    "        word_encoded[word_index] = 1\n",
    "        seed_encoded.append(word_encoded)\n",
    "    seed_encoded = np.array(seed_encoded)\n",
    "    seed_encoded = np.expand_dims(seed_encoded, axis=0)\n",
    "    seed_tensor = torch.tensor(seed_encoded).float().to(device)\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = seed_text\n",
    "    for i in range(num_words):\n",
    "        predictions = model(seed_tensor)\n",
    "        predicted_index = torch.argmax(predictions, dim=1).item()\n",
    "        predicted_word = indices_words[predicted_index]\n",
    "        generated_text += ' ' + predicted_word\n",
    "        \n",
    "        # Update seed tensor with predicted word\n",
    "        predicted_encoded = np.zeros(len(unique_words))\n",
    "        predicted_encoded[predicted_index] = 1\n",
    "        predicted_encoded = np.expand_dims(predicted_encoded, axis=0)\n",
    "        seed_tensor = torch.cat((seed_tensor[:, 1:, :], torch.tensor(predicted_encoded).float().to(device)), axis=1)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "140eb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text sample from model output\n",
    "word_count = 100\n",
    "text = []\n",
    "paragraph_count = 5\n",
    "\n",
    "# Length of phrase should be same as block_size\n",
    "word1, word2 = \"\\n\", \"\\n\"\n",
    "\n",
    "for p in range(paragraph_count):\n",
    "    text.append([])\n",
    "    \n",
    "    for i in range(word_count):\n",
    "        phrase = [word1, word2]\n",
    "        x_ints = [word_indices[item] for item in phrase]\n",
    "        x_vector = []\n",
    "\n",
    "        for item in x_ints:\n",
    "            x_item = np.zeros(len(unique_words))\n",
    "            x_item[item] = 1\n",
    "            x_vector.append(x_item)\n",
    "\n",
    "        initial_input = torch.tensor([np.array([x_vector]).ravel()]).float()\n",
    "\n",
    "        output = model(initial_input)[0].detach().cpu().numpy()\n",
    "\n",
    "        # Workaround to fix occasional sum(pvals[:-1]) > 1.0  bug from implicit casting in np.random.multinomial \n",
    "        output = output.astype(float)\n",
    "        output /= output.sum()\n",
    "\n",
    "        index = np.where(np.random.multinomial(1, output) == 1)[0][0]\n",
    "        word3 = indices_words[index]\n",
    "        text[p].append(word3)\n",
    "\n",
    "        # Use generated word from this run as seed for next run\n",
    "        word1, word2 = word2, word3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bd566b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Paragraph 0:\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the moods the\n",
      "Generated Paragraph 1:\n",
      "the the the the the the the the the the the the the the the the the the the the the the dwell the the the the and the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Generated Paragraph 2:\n",
      "the death's the the the the the the the the the the the the the the the the the the the the the the the the the the the the the and the the the the the the the the the the the the and the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Generated Paragraph 3:\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the world the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the and the the the the the the the the the the the the the the the the the the the\n",
      "Generated Paragraph 4:\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "for p in range(paragraph_count):\n",
    "    print(f\"Generated Paragraph {p}:\")\n",
    "    print(' '.join(text[p]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d46cb7",
   "metadata": {},
   "source": [
    "### Create Feature Vectors from Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7342e5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['and', 'that', 'is', 'why', 'the', 'lonesome', 'day'],\n",
       " ['and', 'so', 'on', 'then', 'a', 'worthless', 'gaud', 'or', 'two'],\n",
       " ['sounded', \"o'er\", 'earth', 'and', 'sea', 'its', 'blast', 'of', 'war'],\n",
       " ['want', 'and', 'woe', 'which', 'torture', 'us'],\n",
       " ['an', 'echo', 'returned', 'on', 'the', 'cold', 'gray', 'morn'],\n",
       " ['while', 'i', 'i', 'built', 'up', 'follies', 'like', 'a', 'wall'],\n",
       " ['ah', 'what', 'a', 'pang', 'of', 'aching', 'sharp', 'surprise'],\n",
       " ['and', 'the', 'old', 'swallow', 'haunted', 'barns'],\n",
       " ['the', 'which', 'she', 'bearing', 'home', 'it', 'burned', 'her', 'nest'],\n",
       " ['the', 'crown', 'of', 'sorrow', 'on', 'their', 'heads', 'their', 'loss'],\n",
       " ['i', 'lay', 'and', 'watched', 'the', 'lonely', 'gloom'],\n",
       " ['a', 'sceptremonstrous', 'winged', 'intolerable'],\n",
       " ['while', 'the', 'rude', 'winds', 'blow', 'off', 'each', 'shadowy', 'crown'],\n",
       " ['but', 'o', 'nevermore', 'can', 'we', 'prison', 'him', 'tight'],\n",
       " ['may', 'meditate', 'a', 'whole', \"youth's\", 'loss'],\n",
       " ['when', 'thee', 'the', 'eyes', 'of', 'that', 'harsh', 'long', 'ago'],\n",
       " ['the', 'foes', 'inclosing', 'and', 'his', 'friend', 'pursued'],\n",
       " ['and',\n",
       "  'bow',\n",
       "  'to',\n",
       "  'dread',\n",
       "  'inquisitor',\n",
       "  'and',\n",
       "  'worship',\n",
       "  'lords',\n",
       "  'of',\n",
       "  'dust'],\n",
       " ['miles', 'off', 'three', 'dangerous', 'miles', 'is', 'home'],\n",
       " ['else', 'sufferd', 'it', 'will', 'set', 'the', 'heart', 'on', 'fire'],\n",
       " ['what', 'gods', 'what', 'madness', 'hither', 'steerd', 'your', 'course'],\n",
       " ['when', 'the', 'glance', 'hast', 'lost', 'its', 'beam'],\n",
       " ['in',\n",
       "  'the',\n",
       "  'shadow',\n",
       "  'of',\n",
       "  'the',\n",
       "  'shores',\n",
       "  'as',\n",
       "  'dead',\n",
       "  'leaves',\n",
       "  'wake'],\n",
       " ['thy', 'sleep', 'makes', 'ridiculous'],\n",
       " ['mine', 'eyes', 'were', 'of', 'their', 'madness', 'half', 'beguiled'],\n",
       " ['you', \"sha'\", \"n't\", 'roll', \"yo'\", 'eyes', 'at', 'me'],\n",
       " ['from', 'flight', 'seditious', 'angel', 'to', 'receave'],\n",
       " ['afar', 'the', 'melancholy', 'thunder', 'moaned'],\n",
       " ['their', 'hate', 'and', 'selfishness', 'and', 'pride'],\n",
       " ['and', 'sorrowful', 'to', 'day', 'thy', 'children', 'set'],\n",
       " ['save', 'for', 'a', 'cry', 'that', 'echoes', 'shrill'],\n",
       " ['the', 'adulterate', 'death', 'of', 'lucrece', 'and', 'her', 'groom'],\n",
       " ['to', 'accomplish', 'suicide'],\n",
       " ['each', 'by', 'his', 'fiery', 'torture', 'howl', 'and', 'rave'],\n",
       " ['at', 'once', 'comes', 'tumbling', 'down', 'the', 'rocky', 'wall'],\n",
       " ['sat', 'mournfully', 'guarding', 'their', 'corpses', 'there'],\n",
       " ['twas', 'when', 'you', 'stole', 'my', 'maidenhead'],\n",
       " ['flood', 'his', 'black', 'hearthstone', 'till', 'its', 'flames', 'expire'],\n",
       " ['men', 'said', 'into', 'a', 'smile', 'which', 'guile', 'portended'],\n",
       " ['deaf', 'and', 'dumb', 'and', 'blind', 'and', 'cold'],\n",
       " ['the', 'visual', 'nerve', 'is', 'withered', 'to', 'the', 'root'],\n",
       " ['there', 'is', 'nothing', 'to', 'hope', 'for', 'i', 'am', 'tired'],\n",
       " ['have', 'seen', 'the', 'danger', 'which', 'i', 'dared', 'not', 'look'],\n",
       " ['all', 'foredoomed', 'to', 'melt', 'away'],\n",
       " ['but',\n",
       "  'thrown',\n",
       "  'in',\n",
       "  'a',\n",
       "  'heap',\n",
       "  'with',\n",
       "  'a',\n",
       "  'crush',\n",
       "  'and',\n",
       "  'a',\n",
       "  'clatter'],\n",
       " ['but', 'homesick', 'tears', 'would', 'fill', 'the', 'eyes'],\n",
       " ['of', 'course', 'throw', 'monstrous', 'shadows:', 'those', 'who', 'think'],\n",
       " ['the', 'mad', 'briareus', 'of', 'disunion', 'rise'],\n",
       " ['i', 'see', 'them', 'torn', 'by', 'gallic', 'guns'],\n",
       " ['thou', \"feel'st\", 'it', 'burning', 'in', 'and', 'inand', 'fear'],\n",
       " ['but', 'all', 'of', 'them', 'are', 'bad', 'enough'],\n",
       " ['has', 'it', 'become', 'to', 'thee', 'a', 'labyrinth', 'never', 'ending'],\n",
       " ['on', 'her', 'changed', 'world', 'of', 'ruin', 'waste', 'and', 'wrack'],\n",
       " ['kneeling', \"ne'er\", 'spoiled', 'silk', 'stockings', 'quit', 'thy', 'state'],\n",
       " ['from', 'earth', 'with', 'the', 'waters', 'of', 'pain'],\n",
       " ['is', 'writ', 'in', 'moods', 'and', 'frowns', 'and', 'wrinkles', 'strange'],\n",
       " ['those', 'cobweb', 'nerves', 'he', 'could', 'not', 'dull', 'within'],\n",
       " ['fooled', 'with', 'your', 'promises'],\n",
       " ['thy', 'mission', 'to', 'a', 'world', 'of', 'woe'],\n",
       " ['our', 'frowning', 'foemen', 'of', 'the', 'night'],\n",
       " ['when', 'dreadful', 'to', 'behold', 'from', 'sea', 'we', 'spied'],\n",
       " ['some', 'moment', 'nailed', 'on', \"sorrow's\", 'cross'],\n",
       " ['the', 'nymph', 'who', 'scatters', 'flaming', 'fires', 'around'],\n",
       " ['for',\n",
       "  \"'twas\",\n",
       "  \"e'en\",\n",
       "  'as',\n",
       "  'a',\n",
       "  'great',\n",
       "  \"god's\",\n",
       "  'slaying',\n",
       "  'and',\n",
       "  'they',\n",
       "  'feared',\n",
       "  'the',\n",
       "  'wrath',\n",
       "  'of',\n",
       "  'the',\n",
       "  'sky'],\n",
       " ['and', \"i've\", 'been', 'like', 'that', 'silly', 'girl'],\n",
       " ['to', 'make', 'a', 'body', 'curse'],\n",
       " ['were', 'murmuring', 'on', 'the', 'stifling', 'air'],\n",
       " ['how', 'poor', 'these', 'pallid', 'phrases', 'seem'],\n",
       " ['these', 'monsters', 'set', 'out', 'in', 'the', 'open', 'sun'],\n",
       " ['blindness', 'like', 'that', 'would', 'scare', 'the', 'mole', 'and', 'bat'],\n",
       " ['did', 'from', 'the', 'altar', 'steal', 'a', 'smouldering', 'brand'],\n",
       " ['is', 'beaten', 'by', 'the', 'winds', 'with', 'foggy', 'vapours', 'bound'],\n",
       " ['until', 'the', 'bitter', 'summons', 'fell'],\n",
       " ['in', 'slumber', 'for', 'thine', 'enemy', 'never', 'sleeps'],\n",
       " ['if', 'men', 'are', 'always', 'at', 'a', 'loss'],\n",
       " ['with', 'warning', 'cough', 'and', 'threatening', 'wheeze'],\n",
       " ['daily', 'struggling', 'though', 'unloved', 'and', 'lonely'],\n",
       " ['how', 'heavy', 'it', 'seemed', 'as', 'heavy', 'as', 'a', 'stone'],\n",
       " ['heart', 'as', 'though', 'with', 'ashes', 'blending'],\n",
       " ['passing', 'to', 'lap', 'thy', 'waters', 'crushed', 'the', 'flower'],\n",
       " ['his', 'hand', 'the', \"captive's\", 'fetters', 'broke'],\n",
       " ['got',\n",
       "  'the',\n",
       "  'ill',\n",
       "  'name',\n",
       "  'of',\n",
       "  'augurs',\n",
       "  'because',\n",
       "  'they',\n",
       "  'were',\n",
       "  'bores',\n",
       "  '—'],\n",
       " ['then', 'thro', 'his', 'breast', 'his', 'fatal', 'sword', 'he', 'sent'],\n",
       " ['the', 'pain', 'when', 'it', 'did', 'live'],\n",
       " ['soft', 'discontented', 'eyes'],\n",
       " ['and', 'the', 'rude', 'people', 'rage', 'with', 'ignorant', 'cries'],\n",
       " ['o', 'lord', 'that', 'didst', 'smother', 'mankind', 'in', 'thy', 'flood'],\n",
       " ['is', 'this', 'a', 'time', 'to', 'be', 'cloudy', 'and', 'sad'],\n",
       " ['blood', 'dipped', 'arrows', 'which', 'savages', 'make'],\n",
       " ['the', 'moon', 'and', 'the', 'stars', 'were', 'anxious'],\n",
       " ['in',\n",
       "  'the',\n",
       "  'twilight',\n",
       "  'of',\n",
       "  'age',\n",
       "  'all',\n",
       "  'things',\n",
       "  'seem',\n",
       "  'strange',\n",
       "  'and',\n",
       "  'phantasmal'],\n",
       " ['a', 'wild', 'and', 'stormy', 'sea'],\n",
       " ['dark', 'with', 'more', 'clouds', 'than', 'tempests', 'are'],\n",
       " ['here',\n",
       "  'comes',\n",
       "  'the',\n",
       "  'cripple',\n",
       "  'jane',\n",
       "  'and',\n",
       "  'by',\n",
       "  'a',\n",
       "  \"fountain's\",\n",
       "  'side'],\n",
       " ['which', 'goaded', 'him', 'in', 'his', 'distress'],\n",
       " ['forelaid', 'and', 'taken', 'while', 'he', 'strove', 'in', 'vain'],\n",
       " ['when', 'blighting', 'was', 'nearest'],\n",
       " ['pillars', 'by', 'madness', 'multiplied'],\n",
       " ['no', 'rest', 'that', 'throbbing', 'slave', 'may', 'ask'],\n",
       " [\"o'er\", \"time's\", 'delusive', 'tide'],\n",
       " ['no',\n",
       "  'word',\n",
       "  'for',\n",
       "  'a',\n",
       "  'while',\n",
       "  'spake',\n",
       "  'regin',\n",
       "  'but',\n",
       "  'he',\n",
       "  'hung',\n",
       "  'his',\n",
       "  'head',\n",
       "  'adown'],\n",
       " ['envy', 'and', 'calumny', 'and', 'hate', 'and', 'pain'],\n",
       " ['and', 'murmured', 'a', 'strange', 'and', 'solemn', 'air'],\n",
       " ['taught', 'by', 'the', 'sorrows', 'that', 'his', 'age', 'had', 'known'],\n",
       " ['let',\n",
       "  'sophists',\n",
       "  'give',\n",
       "  'the',\n",
       "  'lie',\n",
       "  'hearts',\n",
       "  'droop',\n",
       "  'and',\n",
       "  'courtiers',\n",
       "  'play',\n",
       "  'the',\n",
       "  'worm'],\n",
       " ['the', 'fraud', 'of', 'priests', 'the', 'wrong', 'of', 'law'],\n",
       " ['obliterate', 'the', 'etchings'],\n",
       " ['and', 'leaves', 'the', 'world', 'to', 'darkness', 'and', 'to', 'me'],\n",
       " ['that', 'satraps', 'would', 'have', 'shivered', 'at', 'his', 'frown'],\n",
       " ['harsh', 'featureless', 'and', 'rude', 'barrenly', 'perish:'],\n",
       " [\"oblivion's\", 'blankness', 'claims'],\n",
       " ['tormented', 'by', 'the', 'quickened', 'blood', 'of', 'roots'],\n",
       " ['where',\n",
       "  'the',\n",
       "  'cloudy',\n",
       "  'hangings',\n",
       "  'waver',\n",
       "  'and',\n",
       "  'the',\n",
       "  'flickering',\n",
       "  'shadows',\n",
       "  'fall'],\n",
       " ['left',\n",
       "  'the',\n",
       "  'torn',\n",
       "  'human',\n",
       "  'heart',\n",
       "  'their',\n",
       "  'food',\n",
       "  'and',\n",
       "  'dwelling',\n",
       "  'place'],\n",
       " ['inexorable', 'death', 'and', 'claims', 'his', 'right'],\n",
       " ['trailing', 'wrecked', 'it', 'came', 'to', 'land'],\n",
       " ['dead', 'among', 'the', 'shouting', 'people'],\n",
       " ['from',\n",
       "  'the',\n",
       "  'slaughtering',\n",
       "  'of',\n",
       "  'my',\n",
       "  'offspring',\n",
       "  'and',\n",
       "  'the',\n",
       "  'spoiling',\n",
       "  'of',\n",
       "  'my',\n",
       "  'land'],\n",
       " ['no', 'answer', 'came', 'but', 'faint', 'and', 'forlorn'],\n",
       " ['and', 'heavy', 'as', 'the', 'dead'],\n",
       " ['a', 'woman', 'has', 'been', 'strangled', 'with', 'less', 'weight:'],\n",
       " ['howled', 'through', 'the', 'dark', 'like', 'sounds', 'from', 'hell'],\n",
       " ['troubling', 'with', 'life', 'the', 'waters', 'of', 'the', 'world'],\n",
       " ['and',\n",
       "  'the',\n",
       "  'words',\n",
       "  'which',\n",
       "  'he',\n",
       "  'utters',\n",
       "  'are',\n",
       "  'worship',\n",
       "  'or',\n",
       "  'die'],\n",
       " ['the', 'weird', 'pathetic', 'scarlet', 'of', 'day', 'dawning'],\n",
       " ['but',\n",
       "  'your',\n",
       "  'dead',\n",
       "  'ripe',\n",
       "  'ones',\n",
       "  'ranges',\n",
       "  'high',\n",
       "  'fer',\n",
       "  \"treatin'\",\n",
       "  'nothun',\n",
       "  'bretherin'],\n",
       " ['it', 'is', 'a', 'lie', 'a', 'damned', 'infernal', 'lie'],\n",
       " ['and', 'after', 'that', 'the', 'winter', 'cold', 'and', 'drear'],\n",
       " ['thus', 'hee', 'in', 'scorn', 'the', 'warlike', 'angel', \"mov'd\"],\n",
       " ['false', 'faces', 'hung', 'on', 'strings'],\n",
       " ['that',\n",
       "  'in',\n",
       "  'their',\n",
       "  'lives',\n",
       "  'such',\n",
       "  'deadly',\n",
       "  'fray',\n",
       "  'they',\n",
       "  \"ne'er\",\n",
       "  'had',\n",
       "  'seen',\n",
       "  'before'],\n",
       " ['her', 'not', 'nice', 'load'],\n",
       " ['and', 'fears', 'are', 'added', 'and', 'avenging', 'flame'],\n",
       " ['and', 'stiff', 'in', 'fight', 'but', 'serious', \"drill's\", 'despair'],\n",
       " ['till', 'the', 'deaf', 'fury', 'comes', 'your', 'house', 'to', 'sweep', \"'\"],\n",
       " ['but', 'now', 'i', 'see', 'most', 'cruell', 'hee'],\n",
       " ['how', 'weak', 'this', 'tinkling', 'line'],\n",
       " ['and', 'make', 'the', 'liveliest', 'monkey', 'melancholy'],\n",
       " ['where', 'the', 'moloch', 'of', 'slavery', 'sitteth', 'on', 'high'],\n",
       " ['for', 'wanderings', 'sad', 'and', 'lone'],\n",
       " ['none',\n",
       "  'will',\n",
       "  'forget',\n",
       "  'it',\n",
       "  'till',\n",
       "  'shall',\n",
       "  'fall',\n",
       "  'the',\n",
       "  'deadly',\n",
       "  'dart'],\n",
       " ['forever', 'quivering', \"o'er\", 'his', 'task'],\n",
       " ['wilt',\n",
       "  'thou',\n",
       "  'our',\n",
       "  'lowly',\n",
       "  'beds',\n",
       "  'with',\n",
       "  'tears',\n",
       "  'of',\n",
       "  'pity',\n",
       "  'lave',\n",
       "  \"'\"],\n",
       " ['and', 'seek', 'the', 'danger', 'i', 'was', 'forcd', 'to', 'shun'],\n",
       " ['down', 'the', 'dark', 'future', 'through', 'long', 'generations'],\n",
       " ['but', 'she', 'always', 'ran', 'away', 'and', 'left'],\n",
       " ['with', 'such', 'vehement', 'force', 'and', 'might'],\n",
       " ['vain', 'cries', 'throughout', 'the', 'streets', 'thousands', 'pursued'],\n",
       " ['and', 'all', 'their', 'echoes', 'mourn'],\n",
       " ['still', 'must', 'mine', 'though', 'bleeding', 'beat'],\n",
       " ['in', 'town', \"an'\", 'not', 'the', 'leanest', 'runt'],\n",
       " ['by', \"death's\", 'frequented', 'ways'],\n",
       " ['rejection', 'of', 'his', 'humanness'],\n",
       " ['faint', 'voices', 'lifted', 'shrill', 'with', 'pain'],\n",
       " ['in', 'the', 'wild', 'glens', 'rough', 'shepherds', 'will', 'deplore']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "# Tokenize by line\n",
    "for index, row in negative.iterrows():\n",
    "    tokenized_row = row['Text'].split(' ')\n",
    "    \n",
    "    # Preprocess using the same settings as preprocessing done before training model\n",
    "    tokenized_row = regexp_tokenize(' '.join(tokenized_row), pattern=r'[^\\S\\r]+|[\\.,;!?()--_\"]', gaps=True)\n",
    "    sentences.append(tokenized_row)\n",
    "    \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "227e0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "w2v_model = Word2Vec(sentences, vector_size=vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d010c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 25\n"
     ]
    }
   ],
   "source": [
    "vocab = w2v_model.wv.index_to_key\n",
    "vocab_length = len(vocab)\n",
    "\n",
    "# Note: Low vocab size is because Word2Vec model isn't familiar with most of the words in our dataset\n",
    "# From https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "print(f'Vocabulary Size: {format(vocab_length)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6d8b0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'and',\n",
       " 'of',\n",
       " 'a',\n",
       " 'to',\n",
       " 'in',\n",
       " 'his',\n",
       " 'with',\n",
       " 'that',\n",
       " 'but',\n",
       " 'on',\n",
       " 'it',\n",
       " 'is',\n",
       " 'by',\n",
       " 'their',\n",
       " 'i',\n",
       " 'which',\n",
       " 'when',\n",
       " 'as',\n",
       " 'thy',\n",
       " 'from',\n",
       " 'for',\n",
       " 'are',\n",
       " 'eyes',\n",
       " 'he']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff2016e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.048020355"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('he', 'his')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c98eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector_averaging(sentence, model):\n",
    "    embeddings = [model.wv[word] for word in sentence if word in vocab]\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "258efc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    features.append(sentence_to_vector_averaging(sentence, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f1a7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f'x{i}' for i in range(w2v_model.vector_size)]\n",
    "\n",
    "df_text = pd.DataFrame(features, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76eb6064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.005187</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>-0.001249</td>\n",
       "      <td>-0.002851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000705</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>-0.004248</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.004191</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003266</td>\n",
       "      <td>0.003844</td>\n",
       "      <td>0.004352</td>\n",
       "      <td>-0.001904</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>-0.005685</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.006106</td>\n",
       "      <td>-0.000243</td>\n",
       "      <td>-0.007390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000379</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>-0.001590</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.006901</td>\n",
       "      <td>-0.005118</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>-0.003019</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.004293</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>-0.000727</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.007601</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>-0.001241</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>-0.005658</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.005807</td>\n",
       "      <td>-0.006058</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>-0.000969</td>\n",
       "      <td>-0.000661</td>\n",
       "      <td>-0.001191</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.003681</td>\n",
       "      <td>-0.003024</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>-0.002247</td>\n",
       "      <td>-0.000984</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>-0.000704</td>\n",
       "      <td>-0.003113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>-0.000442</td>\n",
       "      <td>-0.004109</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>-0.001963</td>\n",
       "      <td>-0.002506</td>\n",
       "      <td>-0.008639</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003262</td>\n",
       "      <td>-0.000633</td>\n",
       "      <td>0.006558</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>-0.008644</td>\n",
       "      <td>-0.006919</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>-0.004451</td>\n",
       "      <td>-0.006085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>-0.006272</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>-0.000836</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>-0.004648</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>-0.009341</td>\n",
       "      <td>-0.004318</td>\n",
       "      <td>0.005480</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>-0.005813</td>\n",
       "      <td>-0.005368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>-0.002304</td>\n",
       "      <td>-0.001423</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>0.005124</td>\n",
       "      <td>-0.005257</td>\n",
       "      <td>-0.007879</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>-0.009499</td>\n",
       "      <td>0.009568</td>\n",
       "      <td>-0.007775</td>\n",
       "      <td>-0.002644</td>\n",
       "      <td>-0.004905</td>\n",
       "      <td>-0.004970</td>\n",
       "      <td>-0.008018</td>\n",
       "      <td>-0.007785</td>\n",
       "      <td>-0.004550</td>\n",
       "      <td>-0.001279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007232</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>-0.001341</td>\n",
       "      <td>-0.005889</td>\n",
       "      <td>-0.004546</td>\n",
       "      <td>0.008649</td>\n",
       "      <td>-0.003129</td>\n",
       "      <td>-0.006338</td>\n",
       "      <td>0.009869</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.004094</td>\n",
       "      <td>-0.000664</td>\n",
       "      <td>-0.003886</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>0.003750</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>-0.005821</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>0.003684</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>-0.002630</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>-0.000688</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.008161</td>\n",
       "      <td>-0.004431</td>\n",
       "      <td>0.008996</td>\n",
       "      <td>0.008260</td>\n",
       "      <td>-0.004433</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>-0.003907</td>\n",
       "      <td>-0.005568</td>\n",
       "      <td>-0.006529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004011</td>\n",
       "      <td>-0.008236</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>-0.001939</td>\n",
       "      <td>-0.000666</td>\n",
       "      <td>-0.001764</td>\n",
       "      <td>-0.004539</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>-0.004265</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>-0.004648</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>-0.009341</td>\n",
       "      <td>-0.004318</td>\n",
       "      <td>0.005480</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>-0.005813</td>\n",
       "      <td>-0.005368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>-0.002304</td>\n",
       "      <td>-0.001423</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>0.005124</td>\n",
       "      <td>-0.005257</td>\n",
       "      <td>-0.007879</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x0        x1        x2        x3        x4        x5        x6  \\\n",
       "0   -0.005187  0.001937  0.006015  0.003685  0.002358 -0.001292  0.004020   \n",
       "1   -0.003266  0.003844  0.004352 -0.001904  0.001380 -0.005685 -0.000023   \n",
       "2   -0.004293  0.003431 -0.000727  0.002193  0.007601  0.000509 -0.001241   \n",
       "3   -0.003681 -0.003024  0.004939  0.002612  0.006926 -0.002247 -0.000984   \n",
       "4    0.003262 -0.000633  0.006558 -0.000230 -0.008644 -0.006919  0.001255   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "150 -0.004648  0.001231  0.002151 -0.000129 -0.009341 -0.004318  0.005480   \n",
       "151 -0.009499  0.009568 -0.007775 -0.002644 -0.004905 -0.004970 -0.008018   \n",
       "152  0.004094 -0.000664 -0.003886 -0.000176  0.003750  0.004204  0.001244   \n",
       "153  0.008161 -0.004431  0.008996  0.008260 -0.004433  0.000286  0.004288   \n",
       "154 -0.004648  0.001231  0.002151 -0.000129 -0.009341 -0.004318  0.005480   \n",
       "\n",
       "           x7        x8        x9  ...       x91       x92       x93  \\\n",
       "0    0.005023 -0.001249 -0.002851  ... -0.000705  0.003824 -0.004248   \n",
       "1    0.006106 -0.000243 -0.007390  ... -0.000379  0.000116 -0.001590   \n",
       "2    0.004525 -0.005658 -0.000064  ...  0.002059  0.005807 -0.006058   \n",
       "3    0.007034 -0.000704 -0.003113  ...  0.001620  0.005519 -0.000442   \n",
       "4    0.007071 -0.004451 -0.006085  ...  0.000844  0.001158  0.002972   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "150  0.006452 -0.005813 -0.005368  ...  0.004563 -0.002304 -0.001423   \n",
       "151 -0.007785 -0.004550 -0.001279  ...  0.007232  0.001728 -0.001341   \n",
       "152  0.001401 -0.005821  0.002305  ...  0.003069  0.002882 -0.002296   \n",
       "153 -0.003907 -0.005568 -0.006529  ... -0.004011 -0.008236  0.006273   \n",
       "154  0.006452 -0.005813 -0.005368  ...  0.004563 -0.002304 -0.001423   \n",
       "\n",
       "          x94       x95       x96       x97       x98       x99    y  \n",
       "0    0.005150  0.004191  0.001900  0.000904  0.001022  0.003322 -1.0  \n",
       "1    0.000530  0.000797  0.006901 -0.005118 -0.002738 -0.003019 -1.0  \n",
       "2    0.002748  0.004122  0.005618 -0.000969 -0.000661 -0.001191 -1.0  \n",
       "3   -0.004109  0.005016 -0.001963 -0.002506 -0.008639  0.004395 -1.0  \n",
       "4    0.002057  0.002585  0.000266 -0.006272  0.002355 -0.000836 -1.0  \n",
       "..        ...       ...       ...       ...       ...       ...  ...  \n",
       "150  0.009843  0.005124 -0.005257 -0.007879  0.001926 -0.000112 -1.0  \n",
       "151 -0.005889 -0.004546  0.008649 -0.003129 -0.006338  0.009869 -1.0  \n",
       "152  0.003684  0.003156  0.007034 -0.002630  0.003268 -0.000688 -1.0  \n",
       "153 -0.001939 -0.000666 -0.001764 -0.004539  0.004059 -0.004265 -1.0  \n",
       "154  0.009843  0.005124 -0.005257 -0.007879  0.001926 -0.000112 -1.0  \n",
       "\n",
       "[155 rows x 101 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text['y'] = [float(x) for x in negative['Sentiment']]\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffacbab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature vectors as csv\n",
    "df_text.to_csv('negative.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ecc06",
   "metadata": {},
   "source": [
    "### Create Feature Vectors from Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b3088f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "w2v_model = Word2Vec(text, vector_size=vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3fcffb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1\n"
     ]
    }
   ],
   "source": [
    "vocab = w2v_model.wv.index_to_key\n",
    "vocab_length = len(vocab)\n",
    "\n",
    "# Note: Low vocab size is because Word2Vec model isn't familiar with most of the words in our dataset\n",
    "# From https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "print(f'Vocabulary Size: {format(vocab_length)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7a4d247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a6b1d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = [w2v_model.wv[word] for word in vocab]\n",
    "len(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1304879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector_averaging(sentence, model):\n",
    "    embeddings = [model.wv[word] for word in sentence if word in vocab]\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3a160fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    features.append(sentence_to_vector_averaging(sentence, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a7b18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f'x{i}' for i in range(w2v_model.vector_size)]\n",
    "\n",
    "df_text = pd.DataFrame(features, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8bcafa9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000658</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.00626</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>-0.011411</td>\n",
       "      <td>-0.00873</td>\n",
       "      <td>0.007923</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>-0.006152</td>\n",
       "      <td>-0.004616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.006207</td>\n",
       "      <td>-0.010938</td>\n",
       "      <td>-0.008637</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.007841</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000658</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.00626</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>-0.011411</td>\n",
       "      <td>-0.00873</td>\n",
       "      <td>0.007923</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>-0.006152</td>\n",
       "      <td>-0.004616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.006207</td>\n",
       "      <td>-0.010938</td>\n",
       "      <td>-0.008637</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.007841</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>-0.000658</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.00626</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>-0.011411</td>\n",
       "      <td>-0.00873</td>\n",
       "      <td>0.007923</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>-0.006152</td>\n",
       "      <td>-0.004616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.006207</td>\n",
       "      <td>-0.010938</td>\n",
       "      <td>-0.008637</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.007841</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>-0.000658</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.00626</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>-0.011411</td>\n",
       "      <td>-0.00873</td>\n",
       "      <td>0.007923</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>-0.006152</td>\n",
       "      <td>-0.004616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.006207</td>\n",
       "      <td>-0.010938</td>\n",
       "      <td>-0.008637</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.007841</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x0       x1       x2        x3        x4       x5        x6  \\\n",
       "0   -0.000658  0.00029  0.00626  0.011051 -0.011411 -0.00873  0.007923   \n",
       "1    0.000000  0.00000  0.00000  0.000000  0.000000  0.00000  0.000000   \n",
       "2    0.000000  0.00000  0.00000  0.000000  0.000000  0.00000  0.000000   \n",
       "3    0.000000  0.00000  0.00000  0.000000  0.000000  0.00000  0.000000   \n",
       "4   -0.000658  0.00029  0.00626  0.011051 -0.011411 -0.00873  0.007923   \n",
       "..        ...      ...      ...       ...       ...      ...       ...   \n",
       "150 -0.000658  0.00029  0.00626  0.011051 -0.011411 -0.00873  0.007923   \n",
       "151  0.000000  0.00000  0.00000  0.000000  0.000000  0.00000  0.000000   \n",
       "152  0.000000  0.00000  0.00000  0.000000  0.000000  0.00000  0.000000   \n",
       "153  0.000000  0.00000  0.00000  0.000000  0.000000  0.00000  0.000000   \n",
       "154 -0.000658  0.00029  0.00626  0.011051 -0.011411 -0.00873  0.007923   \n",
       "\n",
       "           x7        x8        x9  ...       x91       x92       x93  \\\n",
       "0    0.011006 -0.006152 -0.004616  ...  0.000233  0.004261  0.000267   \n",
       "1    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4    0.011006 -0.006152 -0.004616  ...  0.000233  0.004261  0.000267   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "150  0.011006 -0.006152 -0.004616  ...  0.000233  0.004261  0.000267   \n",
       "151  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "152  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "153  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "154  0.011006 -0.006152 -0.004616  ...  0.000233  0.004261  0.000267   \n",
       "\n",
       "          x94       x95       x96       x97       x98       x99  y  \n",
       "0    0.011799  0.006207 -0.010938 -0.008637  0.001106  0.007841 -1  \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000 -1  \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000 -1  \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000 -1  \n",
       "4    0.011799  0.006207 -0.010938 -0.008637  0.001106  0.007841 -1  \n",
       "..        ...       ...       ...       ...       ...       ... ..  \n",
       "150  0.011799  0.006207 -0.010938 -0.008637  0.001106  0.007841 -1  \n",
       "151  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000 -1  \n",
       "152  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000 -1  \n",
       "153  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000 -1  \n",
       "154  0.011799  0.006207 -0.010938 -0.008637  0.001106  0.007841 -1  \n",
       "\n",
       "[155 rows x 101 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text['y'] = [-1] * len(df_text)\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfa7016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature vectors as csv\n",
    "df_text.to_csv('negative_generated.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
