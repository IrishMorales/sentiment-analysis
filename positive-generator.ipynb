{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd12e830",
   "metadata": {},
   "source": [
    "# Positive Tone Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d930",
   "metadata": {},
   "source": [
    "This notebook contains code for an MLP neural network that generates positively toned data based on the dataset. Feature vectors are then made from the positively toned data and generated data then saved into .csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f0cda",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eccd3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import regexp_tokenize, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f00057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c2279",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c222980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with pale blue berries. in these peaceful shad...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it flows so long as falls the rain</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and that is why, the lonesome day</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when i peruse the conquered fame of heroes, an...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of inward strife for truth and liberty.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>to his ears there came a murmur of far seas be...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>the one good man in the world who knows me, --</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>faint voices lifted shrill with pain</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>an', fust you knowed on, back come charles the...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>in the wild glens rough shepherds will deplore</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>892 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Sentiment\n",
       "0    with pale blue berries. in these peaceful shad...        1.0\n",
       "1                   it flows so long as falls the rain        0.0\n",
       "2                    and that is why, the lonesome day       -1.0\n",
       "3    when i peruse the conquered fame of heroes, an...        2.0\n",
       "4              of inward strife for truth and liberty.        2.0\n",
       "..                                                 ...        ...\n",
       "887  to his ears there came a murmur of far seas be...        0.0\n",
       "888     the one good man in the world who knows me, --        1.0\n",
       "889               faint voices lifted shrill with pain       -1.0\n",
       "890  an', fust you knowed on, back come charles the...        0.0\n",
       "891     in the wild glens rough shepherds will deplore       -1.0\n",
       "\n",
       "[892 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('poem_sentiment.csv', header=None, index_col=0, names=['Text', 'Sentiment'])\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a556975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with pale blue berries. in these peaceful shad...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when i peruse the conquered fame of heroes, an...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of inward strife for truth and liberty.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the red sword sealed their vows!</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>that has a charmingly bourbon air.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>their first-born brother as a god.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>and so i should be loved and mourned to-night.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>and _channing_, with his bland, superior look</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>how your soft opera-music changed, and the dru...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>the one good man in the world who knows me, --</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Sentiment\n",
       "0    with pale blue berries. in these peaceful shad...        1.0\n",
       "3    when i peruse the conquered fame of heroes, an...        2.0\n",
       "4              of inward strife for truth and liberty.        2.0\n",
       "5                     the red sword sealed their vows!        2.0\n",
       "16                  that has a charmingly bourbon air.        1.0\n",
       "..                                                 ...        ...\n",
       "870                 their first-born brother as a god.        1.0\n",
       "876     and so i should be loved and mourned to-night.        2.0\n",
       "877      and _channing_, with his bland, superior look        2.0\n",
       "884  how your soft opera-music changed, and the dru...        2.0\n",
       "888     the one good man in the world who knows me, --        1.0\n",
       "\n",
       "[182 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = df_raw[df_raw['Sentiment'] > 0]\n",
    "positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ca11d",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45cd3956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with pale blue berries. in these peaceful shades--\\nwhen i peruse the conquered fame of heroes, and...\\n           of inward strife for truth and liberty.\\n                  the red sword sealed their vows!\\n                that has a charmingly bourbon air.\\n          brightly expressive as the twins of leda\\n               in monumental pomp! no grecian drop\\n                    the hostile cohorts melt away;\\nand lips where heavenly smiles would hang and b...\\n                         honour to the bugle-horn!\\n                       if the pure and holy angels\\n        upon the thought of perfect noon. and when\\n      thy hands all cunning arts that women prize.\\n             reasoning to admiration, and with mee\\n           it shines superior on a throne of gold:\\n    take the warm welcome of new friends with thee\\n                  augmented, sweet, a hundred fold\\n                every day a rich reward will give;\\n                                 gay little heart!\\n         among the sources of t'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = positive['Text'].to_string(index=False)\n",
    "raw_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46bd8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with pale blue berries. in these peaceful shades--\\nwhen i peruse the conquered fame of heroes, and...\\n           of inward strife for truth and liberty.\\n                  the red sword sealed their vows!\\n                that has a charmingly bourbon air.\\n          brightly expressive as the twins of leda\\n               in monumental pomp! no grecian drop\\n                    the hostile cohorts melt away;\\nand lips where heavenly smiles would hang and b...\\n                         honour to the bugle-horn!\\n                       if the pure and holy angels\\n        upon the thought of perfect noon. and when\\n      thy hands all cunning arts that women prize.\\n             reasoning to admiration, and with mee\\n           it shines superior on a throne of gold:\\n    take the warm welcome of new friends with thee\\n                  augmented, sweet, a hundred fold\\n                every day a rich reward will give;\\n                                 gay little heart!\\n         among the sources of t'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all non-ASCII characters\n",
    "processed_text = re.sub(r'[^\\x00-\\x7f]', r'', raw_text).lower()\n",
    "processed_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd9097",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987aadca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word tokens: 1293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'pale',\n",
       " 'blue',\n",
       " 'berries',\n",
       " 'in',\n",
       " 'these',\n",
       " 'peaceful',\n",
       " 'shades',\n",
       " 'when',\n",
       " 'i']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get word tokens from text\n",
    "word_tokens = regexp_tokenize(processed_text, pattern=r'[^\\S\\r]+|[\\.,:;!?()--_\"]', gaps=True)\n",
    "word_tokens.append('\\n')\n",
    "print(f\"Number of word tokens: {len(word_tokens)}\")\n",
    "word_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7942a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization done to make uncommon words more likely to be recognized by \n",
    "# Word2Vec model later when converting to feature vectors\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word_tokens = [lemmatizer.lemmatize(token) for token in word_tokens] # Lemmatize nouns\n",
    "word_tokens = [lemmatizer.lemmatize(token, 'v') for token in word_tokens] # Lemmatize verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ff426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique word tokens: 629\n"
     ]
    }
   ],
   "source": [
    "# Get unique word tokens from word tokens\n",
    "unique_words = sorted(list(set(word_tokens)))\n",
    "print(f\"Number of unique word tokens: {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86037479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " \"'tis\",\n",
       " 'a',\n",
       " 'abide',\n",
       " 'abloom',\n",
       " 'about',\n",
       " 'accordance',\n",
       " 'adam',\n",
       " 'adept',\n",
       " 'admiration']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vocabulary of word tokens\n",
    "word_vocabulary = unique_words\n",
    "word_vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cf74d",
   "metadata": {},
   "source": [
    "### Create word-index mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d81c158c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: \"'tis\",\n",
       " 2: 'a',\n",
       " 3: 'abide',\n",
       " 4: 'abloom',\n",
       " 5: 'about',\n",
       " 6: 'accordance',\n",
       " 7: 'adam',\n",
       " 8: 'adept',\n",
       " 9: 'admiration',\n",
       " 10: 'after',\n",
       " 11: 'again',\n",
       " 12: 'ah',\n",
       " 13: 'air',\n",
       " 14: 'all',\n",
       " 15: 'already',\n",
       " 16: 'amidst',\n",
       " 17: 'among',\n",
       " 18: 'an',\n",
       " 19: 'and',\n",
       " 20: 'angel',\n",
       " 21: 'angry',\n",
       " 22: 'arm',\n",
       " 23: 'around',\n",
       " 24: 'art',\n",
       " 25: 'ascend',\n",
       " 26: 'ash',\n",
       " 27: 'aspire',\n",
       " 28: 'assay',\n",
       " 29: 'augment',\n",
       " 30: 'away',\n",
       " 31: 'awe',\n",
       " 32: 'ay',\n",
       " 33: 'b',\n",
       " 34: 'bare',\n",
       " 35: 'be',\n",
       " 36: 'bear',\n",
       " 37: 'beauteous',\n",
       " 38: 'beautiful',\n",
       " 39: 'beauty',\n",
       " 40: \"beauty'\",\n",
       " 41: 'because',\n",
       " 42: 'before',\n",
       " 43: 'bell',\n",
       " 44: 'bend',\n",
       " 45: 'beneath',\n",
       " 46: 'berry',\n",
       " 47: 'best',\n",
       " 48: 'betray',\n",
       " 49: 'between',\n",
       " 50: 'blade',\n",
       " 51: 'bland',\n",
       " 52: 'blaze',\n",
       " 53: 'bless',\n",
       " 54: 'blind',\n",
       " 55: 'blue',\n",
       " 56: 'bolt',\n",
       " 57: 'borrow',\n",
       " 58: 'boston',\n",
       " 59: 'bourbon',\n",
       " 60: 'bow',\n",
       " 61: 'brave',\n",
       " 62: 'braver',\n",
       " 63: 'breast',\n",
       " 64: 'bright',\n",
       " 65: 'brightly',\n",
       " 66: 'brilliant',\n",
       " 67: 'brother',\n",
       " 68: \"brynhilda's\",\n",
       " 69: 'bugle',\n",
       " 70: 'burn',\n",
       " 71: 'burst',\n",
       " 72: 'but',\n",
       " 73: 'by',\n",
       " 74: 'cabinet',\n",
       " 75: 'call',\n",
       " 76: \"call'd\",\n",
       " 77: 'calm',\n",
       " 78: 'can',\n",
       " 79: \"ceas'd\",\n",
       " 80: 'chair',\n",
       " 81: 'change',\n",
       " 82: 'channing',\n",
       " 83: 'chariot',\n",
       " 84: 'charmingly',\n",
       " 85: 'cheer',\n",
       " 86: \"childhood's\",\n",
       " 87: 'chime',\n",
       " 88: 'china',\n",
       " 89: 'circle',\n",
       " 90: 'civil',\n",
       " 91: 'clear',\n",
       " 92: 'clearer',\n",
       " 93: 'climax',\n",
       " 94: 'close',\n",
       " 95: 'cloud',\n",
       " 96: 'cohort',\n",
       " 97: 'cometh',\n",
       " 98: 'command',\n",
       " 99: 'commend',\n",
       " 100: 'concentric',\n",
       " 101: 'conquer',\n",
       " 102: 'conviction',\n",
       " 103: 'corpse',\n",
       " 104: 'costume',\n",
       " 105: 'country',\n",
       " 106: 'course',\n",
       " 107: 'creed',\n",
       " 108: 'creep',\n",
       " 109: 'crown',\n",
       " 110: 'cunning',\n",
       " 111: 'cup',\n",
       " 112: 'curl',\n",
       " 113: 'cycle',\n",
       " 114: 'dark',\n",
       " 115: 'darling',\n",
       " 116: 'day',\n",
       " 117: 'de',\n",
       " 118: 'dead',\n",
       " 119: 'dear',\n",
       " 120: 'dearest',\n",
       " 121: 'death',\n",
       " 122: 'deathward',\n",
       " 123: 'declare',\n",
       " 124: 'delicious',\n",
       " 125: 'delight',\n",
       " 126: 'despise',\n",
       " 127: 'dim',\n",
       " 128: 'distil',\n",
       " 129: 'do',\n",
       " 130: 'doe',\n",
       " 131: 'doth',\n",
       " 132: 'down',\n",
       " 133: 'dream',\n",
       " 134: 'drop',\n",
       " 135: 'drum',\n",
       " 136: 'dumb',\n",
       " 137: 'dustless',\n",
       " 138: 'duty',\n",
       " 139: 'earl',\n",
       " 140: 'embrace',\n",
       " 141: 'enfold',\n",
       " 142: 'enstamped',\n",
       " 143: 'entrance',\n",
       " 144: 'ere',\n",
       " 145: 'estrange',\n",
       " 146: 'eve',\n",
       " 147: 'even',\n",
       " 148: 'ever',\n",
       " 149: 'every',\n",
       " 150: 'expressive',\n",
       " 151: 'eye',\n",
       " 152: 'face',\n",
       " 153: 'fade',\n",
       " 154: 'faint',\n",
       " 155: 'fair',\n",
       " 156: 'fairer',\n",
       " 157: 'fame',\n",
       " 158: 'far',\n",
       " 159: 'farr',\n",
       " 160: 'faster',\n",
       " 161: 'father',\n",
       " 162: 'fear',\n",
       " 163: 'fearless',\n",
       " 164: 'feel',\n",
       " 165: 'fell',\n",
       " 166: 'felt',\n",
       " 167: 'field',\n",
       " 168: 'fill',\n",
       " 169: 'first',\n",
       " 170: 'flame',\n",
       " 171: 'flow',\n",
       " 172: 'flower',\n",
       " 173: 'foe',\n",
       " 174: 'fold',\n",
       " 175: 'folk',\n",
       " 176: 'fond',\n",
       " 177: 'foot',\n",
       " 178: 'for',\n",
       " 179: 'force',\n",
       " 180: 'forget',\n",
       " 181: 'fragrance',\n",
       " 182: 'free',\n",
       " 183: \"freedom's\",\n",
       " 184: 'friend',\n",
       " 185: \"friendship'\",\n",
       " 186: \"friendship's\",\n",
       " 187: 'from',\n",
       " 188: 'full',\n",
       " 189: 'gay',\n",
       " 190: 'gem',\n",
       " 191: 'give',\n",
       " 192: 'glad',\n",
       " 193: 'glorious',\n",
       " 194: 'glory',\n",
       " 195: \"glory's\",\n",
       " 196: 'glow',\n",
       " 197: 'goal',\n",
       " 198: 'god',\n",
       " 199: 'gold',\n",
       " 200: 'golden',\n",
       " 201: 'good',\n",
       " 202: 'goodness',\n",
       " 203: 'gore',\n",
       " 204: 'goth',\n",
       " 205: 'grace',\n",
       " 206: 'gracious',\n",
       " 207: 'graciousness',\n",
       " 208: 'grander',\n",
       " 209: 'grave',\n",
       " 210: 'gre',\n",
       " 211: 'great',\n",
       " 212: 'grecian',\n",
       " 213: 'grimy',\n",
       " 214: 'grind',\n",
       " 215: 'gudrun',\n",
       " 216: 'ha',\n",
       " 217: 'hadst',\n",
       " 218: 'halcyon',\n",
       " 219: 'hale',\n",
       " 220: 'hand',\n",
       " 221: 'hang',\n",
       " 222: 'happy',\n",
       " 223: 'harmless',\n",
       " 224: 'harriet',\n",
       " 225: 'hast',\n",
       " 226: 'hate',\n",
       " 227: 'have',\n",
       " 228: 'he',\n",
       " 229: 'head',\n",
       " 230: 'heart',\n",
       " 231: \"heav'n\",\n",
       " 232: 'heaven',\n",
       " 233: \"heaven's\",\n",
       " 234: 'heavenly',\n",
       " 235: 'heed',\n",
       " 236: 'her',\n",
       " 237: 'here',\n",
       " 238: 'hero',\n",
       " 239: 'high',\n",
       " 240: 'hint',\n",
       " 241: 'his',\n",
       " 242: 'hoarse',\n",
       " 243: 'holy',\n",
       " 244: 'home',\n",
       " 245: 'honest',\n",
       " 246: 'honour',\n",
       " 247: 'hope',\n",
       " 248: 'horn',\n",
       " 249: 'hostile',\n",
       " 250: 'how',\n",
       " 251: 'hue',\n",
       " 252: 'human',\n",
       " 253: 'hundred',\n",
       " 254: \"hunter's\",\n",
       " 255: 'i',\n",
       " 256: 'if',\n",
       " 257: 'image',\n",
       " 258: 'in',\n",
       " 259: 'inward',\n",
       " 260: 'isadore',\n",
       " 261: 'it',\n",
       " 262: 'itself',\n",
       " 263: 'jet',\n",
       " 264: 'jewel',\n",
       " 265: 'john',\n",
       " 266: 'joy',\n",
       " 267: 'joyous',\n",
       " 268: 'just',\n",
       " 269: 'k',\n",
       " 270: 'keen',\n",
       " 271: 'keep',\n",
       " 272: 'kindle',\n",
       " 273: 'king',\n",
       " 274: \"king's\",\n",
       " 275: 'kiss',\n",
       " 276: 'kneel',\n",
       " 277: 'knife',\n",
       " 278: 'knightly',\n",
       " 279: 'know',\n",
       " 280: 'lamp',\n",
       " 281: 'land',\n",
       " 282: 'lap',\n",
       " 283: 'large',\n",
       " 284: 'last',\n",
       " 285: 'laugh',\n",
       " 286: 'leda',\n",
       " 287: 'leprous',\n",
       " 288: 'liberty',\n",
       " 289: 'life',\n",
       " 290: 'light',\n",
       " 291: 'like',\n",
       " 292: 'limpid',\n",
       " 293: 'lion',\n",
       " 294: 'lip',\n",
       " 295: 'little',\n",
       " 296: 'live',\n",
       " 297: 'long',\n",
       " 298: 'look',\n",
       " 299: 'love',\n",
       " 300: 'loveliest',\n",
       " 301: 'lovely',\n",
       " 302: 'low',\n",
       " 303: 'lulld',\n",
       " 304: 'luminous',\n",
       " 305: 'lure',\n",
       " 306: 'luting',\n",
       " 307: 'majestical',\n",
       " 308: 'make',\n",
       " 309: 'man',\n",
       " 310: 'manner',\n",
       " 311: 'martineau',\n",
       " 312: 'master',\n",
       " 313: 'matter',\n",
       " 314: 'may',\n",
       " 315: 'me',\n",
       " 316: 'mee',\n",
       " 317: 'melt',\n",
       " 318: 'memory',\n",
       " 319: 'men',\n",
       " 320: 'merciful',\n",
       " 321: 'merit',\n",
       " 322: 'might',\n",
       " 323: 'mightily',\n",
       " 324: 'mighty',\n",
       " 325: 'million',\n",
       " 326: 'mind',\n",
       " 327: 'mine',\n",
       " 328: 'mirror',\n",
       " 329: 'miscall',\n",
       " 330: 'miss',\n",
       " 331: 'monumental',\n",
       " 332: 'moonstruck',\n",
       " 333: 'more',\n",
       " 334: 'morn',\n",
       " 335: 'morning',\n",
       " 336: 'moses',\n",
       " 337: 'most',\n",
       " 338: 'mother',\n",
       " 339: 'mount',\n",
       " 340: 'mourn',\n",
       " 341: 'music',\n",
       " 342: 'my',\n",
       " 343: 'myriad',\n",
       " 344: 'nature',\n",
       " 345: \"nature's\",\n",
       " 346: 'never',\n",
       " 347: 'new',\n",
       " 348: 'night',\n",
       " 349: 'no',\n",
       " 350: 'noble',\n",
       " 351: 'nobler',\n",
       " 352: 'noon',\n",
       " 353: 'nor',\n",
       " 354: 'o',\n",
       " 355: \"o'er\",\n",
       " 356: 'obscurity',\n",
       " 357: 'ocean',\n",
       " 358: 'of',\n",
       " 359: 'off',\n",
       " 360: 'oh',\n",
       " 361: 'old',\n",
       " 362: 'on',\n",
       " 363: 'onaway',\n",
       " 364: 'once',\n",
       " 365: 'one',\n",
       " 366: 'only',\n",
       " 367: 'onward',\n",
       " 368: 'opera',\n",
       " 369: 'or',\n",
       " 370: 'orchestra',\n",
       " 371: 'ordain',\n",
       " 372: 'our',\n",
       " 373: 'out',\n",
       " 374: 'outshine',\n",
       " 375: 'outward',\n",
       " 376: 'overcome',\n",
       " 377: 'own',\n",
       " 378: 'paint',\n",
       " 379: 'pale',\n",
       " 380: 'palm',\n",
       " 381: 'pas',\n",
       " 382: 'pass',\n",
       " 383: 'passionate',\n",
       " 384: 'past',\n",
       " 385: 'paynim',\n",
       " 386: 'peace',\n",
       " 387: 'peaceful',\n",
       " 388: 'peak',\n",
       " 389: 'pen',\n",
       " 390: 'perfect',\n",
       " 391: 'period',\n",
       " 392: 'peruse',\n",
       " 393: 'pilot',\n",
       " 394: 'place',\n",
       " 395: 'play',\n",
       " 396: \"playmates'\",\n",
       " 397: \"pleas'd\",\n",
       " 398: 'please',\n",
       " 399: 'pluck',\n",
       " 400: 'plume',\n",
       " 401: 'poem',\n",
       " 402: 'poesy',\n",
       " 403: 'pomp',\n",
       " 404: 'port',\n",
       " 405: 'potent',\n",
       " 406: 'praise',\n",
       " 407: 'pray',\n",
       " 408: 'prayer',\n",
       " 409: 'presidency',\n",
       " 410: 'president',\n",
       " 411: 'pride',\n",
       " 412: 'privilege',\n",
       " 413: 'prize',\n",
       " 414: 'profound',\n",
       " 415: 'profounds',\n",
       " 416: 'prosper',\n",
       " 417: 'prosperous',\n",
       " 418: 'pull',\n",
       " 419: 'pulse',\n",
       " 420: 'pure',\n",
       " 421: 'quaver',\n",
       " 422: 'queen',\n",
       " 423: 'quoth',\n",
       " 424: 'radiant',\n",
       " 425: 'raise',\n",
       " 426: 'rapid',\n",
       " 427: 'rapture',\n",
       " 428: 'reason',\n",
       " 429: 'record',\n",
       " 430: 'red',\n",
       " 431: 'regal',\n",
       " 432: 'remain',\n",
       " 433: 'remembrance',\n",
       " 434: 'renew',\n",
       " 435: 'repay',\n",
       " 436: 'rest',\n",
       " 437: 'revive',\n",
       " 438: 'reward',\n",
       " 439: 'rhyme',\n",
       " 440: 'ri',\n",
       " 441: 'rich',\n",
       " 442: 'ride',\n",
       " 443: 'right',\n",
       " 444: 'ring',\n",
       " 445: 'ringer',\n",
       " 446: 'rise',\n",
       " 447: 'river',\n",
       " 448: 'robe',\n",
       " 449: 'rochambeau',\n",
       " 450: 'rome',\n",
       " 451: 'round',\n",
       " 452: 'row',\n",
       " 453: 'ruby',\n",
       " 454: 'run',\n",
       " 455: 's',\n",
       " 456: 'sacred',\n",
       " 457: 'saintly',\n",
       " 458: 'say',\n",
       " 459: 'science',\n",
       " 460: 'sea',\n",
       " 461: 'seal',\n",
       " 462: 'seat',\n",
       " 463: 'see',\n",
       " 464: 'seed',\n",
       " 465: 'seem',\n",
       " 466: 'serene',\n",
       " 467: 'seven',\n",
       " 468: 'sex',\n",
       " 469: 'shade',\n",
       " 470: 'shall',\n",
       " 471: 'sharpness',\n",
       " 472: 'she',\n",
       " 473: 'shin',\n",
       " 474: 'shine',\n",
       " 475: 'shore',\n",
       " 476: 'should',\n",
       " 477: 'sigurd',\n",
       " 478: 'silent',\n",
       " 479: 'simpler',\n",
       " 480: 'sincerest',\n",
       " 481: 'sing',\n",
       " 482: 'sleep',\n",
       " 483: 'slight',\n",
       " 484: 'slumber',\n",
       " 485: 'smile',\n",
       " 486: 'snow',\n",
       " 487: 'so',\n",
       " 488: 'soft',\n",
       " 489: 'some',\n",
       " 490: 'sometimes',\n",
       " 491: 'song',\n",
       " 492: 'soul',\n",
       " 493: 'source',\n",
       " 494: 'spark',\n",
       " 495: 'speech',\n",
       " 496: 'spirit',\n",
       " 497: 'spoil',\n",
       " 498: 'spring',\n",
       " 499: 'stand',\n",
       " 500: 'star',\n",
       " 501: 'stately',\n",
       " 502: 'stept',\n",
       " 503: 'still',\n",
       " 504: 'stir',\n",
       " 505: 'stormy',\n",
       " 506: 'straight',\n",
       " 507: 'stream',\n",
       " 508: 'strife',\n",
       " 509: 'strike',\n",
       " 510: 'string',\n",
       " 511: 'strive',\n",
       " 512: 'such',\n",
       " 513: 'suddenly',\n",
       " 514: \"summer's\",\n",
       " 515: 'sun',\n",
       " 516: \"sundown's\",\n",
       " 517: 'superior',\n",
       " 518: 'surround',\n",
       " 519: 'sweet',\n",
       " 520: 'sweeter',\n",
       " 521: 'sweetly',\n",
       " 522: 'sweetness',\n",
       " 523: 'swifter',\n",
       " 524: 'sword',\n",
       " 525: 'sympathy',\n",
       " 526: 'symphony',\n",
       " 527: 'take',\n",
       " 528: 'teach',\n",
       " 529: 'tender',\n",
       " 530: 'tenderest',\n",
       " 531: 'thames',\n",
       " 532: 'than',\n",
       " 533: 'that',\n",
       " 534: 'the',\n",
       " 535: 'thee',\n",
       " 536: 'their',\n",
       " 537: 'then',\n",
       " 538: 'there',\n",
       " 539: 'thereupon',\n",
       " 540: 'these',\n",
       " 541: 'they',\n",
       " 542: 'thing',\n",
       " 543: 'think',\n",
       " 544: 'this',\n",
       " 545: 'those',\n",
       " 546: 'thou',\n",
       " 547: 'though',\n",
       " 548: 'thro',\n",
       " 549: 'throne',\n",
       " 550: 'throug',\n",
       " 551: 'through',\n",
       " 552: 'throw',\n",
       " 553: 'thunder',\n",
       " 554: 'thus',\n",
       " 555: 'thy',\n",
       " 556: 'till',\n",
       " 557: 'time',\n",
       " 558: 'tinkle',\n",
       " 559: 'to',\n",
       " 560: 'tongue',\n",
       " 561: 'too',\n",
       " 562: 'torch',\n",
       " 563: 'touch',\n",
       " 564: 'towards',\n",
       " 565: 'trail',\n",
       " 566: 'train',\n",
       " 567: 'tranquil',\n",
       " 568: 'trouble',\n",
       " 569: 'trust',\n",
       " 570: 'truth',\n",
       " 571: 'try',\n",
       " 572: 'turn',\n",
       " 573: 'twin',\n",
       " 574: 'u',\n",
       " 575: 'unity',\n",
       " 576: 'unrest',\n",
       " 577: 'unresting',\n",
       " 578: 'upon',\n",
       " 579: 'uppe',\n",
       " 580: 'utter',\n",
       " 581: 'vast',\n",
       " 582: 'very',\n",
       " 583: 'victor',\n",
       " 584: \"virtue's\",\n",
       " 585: 'voice',\n",
       " 586: 'vow',\n",
       " 587: 'wa',\n",
       " 588: 'wan',\n",
       " 589: 'war',\n",
       " 590: 'warm',\n",
       " 591: 'water',\n",
       " 592: 'we',\n",
       " 593: 'weary',\n",
       " 594: 'welcome',\n",
       " 595: 'what',\n",
       " 596: 'when',\n",
       " 597: 'where',\n",
       " 598: 'which',\n",
       " 599: 'whimper',\n",
       " 600: 'whine',\n",
       " 601: 'who',\n",
       " 602: 'whose',\n",
       " 603: 'why',\n",
       " 604: 'wild',\n",
       " 605: 'will',\n",
       " 606: 'wilt',\n",
       " 607: \"wisdom's\",\n",
       " 608: 'wise',\n",
       " 609: 'wit',\n",
       " 610: 'with',\n",
       " 611: 'woman',\n",
       " 612: 'wonder',\n",
       " 613: 'world',\n",
       " 614: 'worship',\n",
       " 615: 'worst',\n",
       " 616: 'worth',\n",
       " 617: 'worthy',\n",
       " 618: 'would',\n",
       " 619: 'wouldst',\n",
       " 620: 'wovest',\n",
       " 621: 'wrath',\n",
       " 622: 'wrong',\n",
       " 623: 'year',\n",
       " 624: 'yet',\n",
       " 625: 'you',\n",
       " 626: 'your',\n",
       " 627: 'yours',\n",
       " 628: \"youth's\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create index-word mappings \n",
    "indices_words = dict((index, word) for index, word in enumerate(unique_words))\n",
    "indices_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e485f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " \"'tis\": 1,\n",
       " 'a': 2,\n",
       " 'abide': 3,\n",
       " 'abloom': 4,\n",
       " 'about': 5,\n",
       " 'accordance': 6,\n",
       " 'adam': 7,\n",
       " 'adept': 8,\n",
       " 'admiration': 9,\n",
       " 'after': 10,\n",
       " 'again': 11,\n",
       " 'ah': 12,\n",
       " 'air': 13,\n",
       " 'all': 14,\n",
       " 'already': 15,\n",
       " 'amidst': 16,\n",
       " 'among': 17,\n",
       " 'an': 18,\n",
       " 'and': 19,\n",
       " 'angel': 20,\n",
       " 'angry': 21,\n",
       " 'arm': 22,\n",
       " 'around': 23,\n",
       " 'art': 24,\n",
       " 'ascend': 25,\n",
       " 'ash': 26,\n",
       " 'aspire': 27,\n",
       " 'assay': 28,\n",
       " 'augment': 29,\n",
       " 'away': 30,\n",
       " 'awe': 31,\n",
       " 'ay': 32,\n",
       " 'b': 33,\n",
       " 'bare': 34,\n",
       " 'be': 35,\n",
       " 'bear': 36,\n",
       " 'beauteous': 37,\n",
       " 'beautiful': 38,\n",
       " 'beauty': 39,\n",
       " \"beauty'\": 40,\n",
       " 'because': 41,\n",
       " 'before': 42,\n",
       " 'bell': 43,\n",
       " 'bend': 44,\n",
       " 'beneath': 45,\n",
       " 'berry': 46,\n",
       " 'best': 47,\n",
       " 'betray': 48,\n",
       " 'between': 49,\n",
       " 'blade': 50,\n",
       " 'bland': 51,\n",
       " 'blaze': 52,\n",
       " 'bless': 53,\n",
       " 'blind': 54,\n",
       " 'blue': 55,\n",
       " 'bolt': 56,\n",
       " 'borrow': 57,\n",
       " 'boston': 58,\n",
       " 'bourbon': 59,\n",
       " 'bow': 60,\n",
       " 'brave': 61,\n",
       " 'braver': 62,\n",
       " 'breast': 63,\n",
       " 'bright': 64,\n",
       " 'brightly': 65,\n",
       " 'brilliant': 66,\n",
       " 'brother': 67,\n",
       " \"brynhilda's\": 68,\n",
       " 'bugle': 69,\n",
       " 'burn': 70,\n",
       " 'burst': 71,\n",
       " 'but': 72,\n",
       " 'by': 73,\n",
       " 'cabinet': 74,\n",
       " 'call': 75,\n",
       " \"call'd\": 76,\n",
       " 'calm': 77,\n",
       " 'can': 78,\n",
       " \"ceas'd\": 79,\n",
       " 'chair': 80,\n",
       " 'change': 81,\n",
       " 'channing': 82,\n",
       " 'chariot': 83,\n",
       " 'charmingly': 84,\n",
       " 'cheer': 85,\n",
       " \"childhood's\": 86,\n",
       " 'chime': 87,\n",
       " 'china': 88,\n",
       " 'circle': 89,\n",
       " 'civil': 90,\n",
       " 'clear': 91,\n",
       " 'clearer': 92,\n",
       " 'climax': 93,\n",
       " 'close': 94,\n",
       " 'cloud': 95,\n",
       " 'cohort': 96,\n",
       " 'cometh': 97,\n",
       " 'command': 98,\n",
       " 'commend': 99,\n",
       " 'concentric': 100,\n",
       " 'conquer': 101,\n",
       " 'conviction': 102,\n",
       " 'corpse': 103,\n",
       " 'costume': 104,\n",
       " 'country': 105,\n",
       " 'course': 106,\n",
       " 'creed': 107,\n",
       " 'creep': 108,\n",
       " 'crown': 109,\n",
       " 'cunning': 110,\n",
       " 'cup': 111,\n",
       " 'curl': 112,\n",
       " 'cycle': 113,\n",
       " 'dark': 114,\n",
       " 'darling': 115,\n",
       " 'day': 116,\n",
       " 'de': 117,\n",
       " 'dead': 118,\n",
       " 'dear': 119,\n",
       " 'dearest': 120,\n",
       " 'death': 121,\n",
       " 'deathward': 122,\n",
       " 'declare': 123,\n",
       " 'delicious': 124,\n",
       " 'delight': 125,\n",
       " 'despise': 126,\n",
       " 'dim': 127,\n",
       " 'distil': 128,\n",
       " 'do': 129,\n",
       " 'doe': 130,\n",
       " 'doth': 131,\n",
       " 'down': 132,\n",
       " 'dream': 133,\n",
       " 'drop': 134,\n",
       " 'drum': 135,\n",
       " 'dumb': 136,\n",
       " 'dustless': 137,\n",
       " 'duty': 138,\n",
       " 'earl': 139,\n",
       " 'embrace': 140,\n",
       " 'enfold': 141,\n",
       " 'enstamped': 142,\n",
       " 'entrance': 143,\n",
       " 'ere': 144,\n",
       " 'estrange': 145,\n",
       " 'eve': 146,\n",
       " 'even': 147,\n",
       " 'ever': 148,\n",
       " 'every': 149,\n",
       " 'expressive': 150,\n",
       " 'eye': 151,\n",
       " 'face': 152,\n",
       " 'fade': 153,\n",
       " 'faint': 154,\n",
       " 'fair': 155,\n",
       " 'fairer': 156,\n",
       " 'fame': 157,\n",
       " 'far': 158,\n",
       " 'farr': 159,\n",
       " 'faster': 160,\n",
       " 'father': 161,\n",
       " 'fear': 162,\n",
       " 'fearless': 163,\n",
       " 'feel': 164,\n",
       " 'fell': 165,\n",
       " 'felt': 166,\n",
       " 'field': 167,\n",
       " 'fill': 168,\n",
       " 'first': 169,\n",
       " 'flame': 170,\n",
       " 'flow': 171,\n",
       " 'flower': 172,\n",
       " 'foe': 173,\n",
       " 'fold': 174,\n",
       " 'folk': 175,\n",
       " 'fond': 176,\n",
       " 'foot': 177,\n",
       " 'for': 178,\n",
       " 'force': 179,\n",
       " 'forget': 180,\n",
       " 'fragrance': 181,\n",
       " 'free': 182,\n",
       " \"freedom's\": 183,\n",
       " 'friend': 184,\n",
       " \"friendship'\": 185,\n",
       " \"friendship's\": 186,\n",
       " 'from': 187,\n",
       " 'full': 188,\n",
       " 'gay': 189,\n",
       " 'gem': 190,\n",
       " 'give': 191,\n",
       " 'glad': 192,\n",
       " 'glorious': 193,\n",
       " 'glory': 194,\n",
       " \"glory's\": 195,\n",
       " 'glow': 196,\n",
       " 'goal': 197,\n",
       " 'god': 198,\n",
       " 'gold': 199,\n",
       " 'golden': 200,\n",
       " 'good': 201,\n",
       " 'goodness': 202,\n",
       " 'gore': 203,\n",
       " 'goth': 204,\n",
       " 'grace': 205,\n",
       " 'gracious': 206,\n",
       " 'graciousness': 207,\n",
       " 'grander': 208,\n",
       " 'grave': 209,\n",
       " 'gre': 210,\n",
       " 'great': 211,\n",
       " 'grecian': 212,\n",
       " 'grimy': 213,\n",
       " 'grind': 214,\n",
       " 'gudrun': 215,\n",
       " 'ha': 216,\n",
       " 'hadst': 217,\n",
       " 'halcyon': 218,\n",
       " 'hale': 219,\n",
       " 'hand': 220,\n",
       " 'hang': 221,\n",
       " 'happy': 222,\n",
       " 'harmless': 223,\n",
       " 'harriet': 224,\n",
       " 'hast': 225,\n",
       " 'hate': 226,\n",
       " 'have': 227,\n",
       " 'he': 228,\n",
       " 'head': 229,\n",
       " 'heart': 230,\n",
       " \"heav'n\": 231,\n",
       " 'heaven': 232,\n",
       " \"heaven's\": 233,\n",
       " 'heavenly': 234,\n",
       " 'heed': 235,\n",
       " 'her': 236,\n",
       " 'here': 237,\n",
       " 'hero': 238,\n",
       " 'high': 239,\n",
       " 'hint': 240,\n",
       " 'his': 241,\n",
       " 'hoarse': 242,\n",
       " 'holy': 243,\n",
       " 'home': 244,\n",
       " 'honest': 245,\n",
       " 'honour': 246,\n",
       " 'hope': 247,\n",
       " 'horn': 248,\n",
       " 'hostile': 249,\n",
       " 'how': 250,\n",
       " 'hue': 251,\n",
       " 'human': 252,\n",
       " 'hundred': 253,\n",
       " \"hunter's\": 254,\n",
       " 'i': 255,\n",
       " 'if': 256,\n",
       " 'image': 257,\n",
       " 'in': 258,\n",
       " 'inward': 259,\n",
       " 'isadore': 260,\n",
       " 'it': 261,\n",
       " 'itself': 262,\n",
       " 'jet': 263,\n",
       " 'jewel': 264,\n",
       " 'john': 265,\n",
       " 'joy': 266,\n",
       " 'joyous': 267,\n",
       " 'just': 268,\n",
       " 'k': 269,\n",
       " 'keen': 270,\n",
       " 'keep': 271,\n",
       " 'kindle': 272,\n",
       " 'king': 273,\n",
       " \"king's\": 274,\n",
       " 'kiss': 275,\n",
       " 'kneel': 276,\n",
       " 'knife': 277,\n",
       " 'knightly': 278,\n",
       " 'know': 279,\n",
       " 'lamp': 280,\n",
       " 'land': 281,\n",
       " 'lap': 282,\n",
       " 'large': 283,\n",
       " 'last': 284,\n",
       " 'laugh': 285,\n",
       " 'leda': 286,\n",
       " 'leprous': 287,\n",
       " 'liberty': 288,\n",
       " 'life': 289,\n",
       " 'light': 290,\n",
       " 'like': 291,\n",
       " 'limpid': 292,\n",
       " 'lion': 293,\n",
       " 'lip': 294,\n",
       " 'little': 295,\n",
       " 'live': 296,\n",
       " 'long': 297,\n",
       " 'look': 298,\n",
       " 'love': 299,\n",
       " 'loveliest': 300,\n",
       " 'lovely': 301,\n",
       " 'low': 302,\n",
       " 'lulld': 303,\n",
       " 'luminous': 304,\n",
       " 'lure': 305,\n",
       " 'luting': 306,\n",
       " 'majestical': 307,\n",
       " 'make': 308,\n",
       " 'man': 309,\n",
       " 'manner': 310,\n",
       " 'martineau': 311,\n",
       " 'master': 312,\n",
       " 'matter': 313,\n",
       " 'may': 314,\n",
       " 'me': 315,\n",
       " 'mee': 316,\n",
       " 'melt': 317,\n",
       " 'memory': 318,\n",
       " 'men': 319,\n",
       " 'merciful': 320,\n",
       " 'merit': 321,\n",
       " 'might': 322,\n",
       " 'mightily': 323,\n",
       " 'mighty': 324,\n",
       " 'million': 325,\n",
       " 'mind': 326,\n",
       " 'mine': 327,\n",
       " 'mirror': 328,\n",
       " 'miscall': 329,\n",
       " 'miss': 330,\n",
       " 'monumental': 331,\n",
       " 'moonstruck': 332,\n",
       " 'more': 333,\n",
       " 'morn': 334,\n",
       " 'morning': 335,\n",
       " 'moses': 336,\n",
       " 'most': 337,\n",
       " 'mother': 338,\n",
       " 'mount': 339,\n",
       " 'mourn': 340,\n",
       " 'music': 341,\n",
       " 'my': 342,\n",
       " 'myriad': 343,\n",
       " 'nature': 344,\n",
       " \"nature's\": 345,\n",
       " 'never': 346,\n",
       " 'new': 347,\n",
       " 'night': 348,\n",
       " 'no': 349,\n",
       " 'noble': 350,\n",
       " 'nobler': 351,\n",
       " 'noon': 352,\n",
       " 'nor': 353,\n",
       " 'o': 354,\n",
       " \"o'er\": 355,\n",
       " 'obscurity': 356,\n",
       " 'ocean': 357,\n",
       " 'of': 358,\n",
       " 'off': 359,\n",
       " 'oh': 360,\n",
       " 'old': 361,\n",
       " 'on': 362,\n",
       " 'onaway': 363,\n",
       " 'once': 364,\n",
       " 'one': 365,\n",
       " 'only': 366,\n",
       " 'onward': 367,\n",
       " 'opera': 368,\n",
       " 'or': 369,\n",
       " 'orchestra': 370,\n",
       " 'ordain': 371,\n",
       " 'our': 372,\n",
       " 'out': 373,\n",
       " 'outshine': 374,\n",
       " 'outward': 375,\n",
       " 'overcome': 376,\n",
       " 'own': 377,\n",
       " 'paint': 378,\n",
       " 'pale': 379,\n",
       " 'palm': 380,\n",
       " 'pas': 381,\n",
       " 'pass': 382,\n",
       " 'passionate': 383,\n",
       " 'past': 384,\n",
       " 'paynim': 385,\n",
       " 'peace': 386,\n",
       " 'peaceful': 387,\n",
       " 'peak': 388,\n",
       " 'pen': 389,\n",
       " 'perfect': 390,\n",
       " 'period': 391,\n",
       " 'peruse': 392,\n",
       " 'pilot': 393,\n",
       " 'place': 394,\n",
       " 'play': 395,\n",
       " \"playmates'\": 396,\n",
       " \"pleas'd\": 397,\n",
       " 'please': 398,\n",
       " 'pluck': 399,\n",
       " 'plume': 400,\n",
       " 'poem': 401,\n",
       " 'poesy': 402,\n",
       " 'pomp': 403,\n",
       " 'port': 404,\n",
       " 'potent': 405,\n",
       " 'praise': 406,\n",
       " 'pray': 407,\n",
       " 'prayer': 408,\n",
       " 'presidency': 409,\n",
       " 'president': 410,\n",
       " 'pride': 411,\n",
       " 'privilege': 412,\n",
       " 'prize': 413,\n",
       " 'profound': 414,\n",
       " 'profounds': 415,\n",
       " 'prosper': 416,\n",
       " 'prosperous': 417,\n",
       " 'pull': 418,\n",
       " 'pulse': 419,\n",
       " 'pure': 420,\n",
       " 'quaver': 421,\n",
       " 'queen': 422,\n",
       " 'quoth': 423,\n",
       " 'radiant': 424,\n",
       " 'raise': 425,\n",
       " 'rapid': 426,\n",
       " 'rapture': 427,\n",
       " 'reason': 428,\n",
       " 'record': 429,\n",
       " 'red': 430,\n",
       " 'regal': 431,\n",
       " 'remain': 432,\n",
       " 'remembrance': 433,\n",
       " 'renew': 434,\n",
       " 'repay': 435,\n",
       " 'rest': 436,\n",
       " 'revive': 437,\n",
       " 'reward': 438,\n",
       " 'rhyme': 439,\n",
       " 'ri': 440,\n",
       " 'rich': 441,\n",
       " 'ride': 442,\n",
       " 'right': 443,\n",
       " 'ring': 444,\n",
       " 'ringer': 445,\n",
       " 'rise': 446,\n",
       " 'river': 447,\n",
       " 'robe': 448,\n",
       " 'rochambeau': 449,\n",
       " 'rome': 450,\n",
       " 'round': 451,\n",
       " 'row': 452,\n",
       " 'ruby': 453,\n",
       " 'run': 454,\n",
       " 's': 455,\n",
       " 'sacred': 456,\n",
       " 'saintly': 457,\n",
       " 'say': 458,\n",
       " 'science': 459,\n",
       " 'sea': 460,\n",
       " 'seal': 461,\n",
       " 'seat': 462,\n",
       " 'see': 463,\n",
       " 'seed': 464,\n",
       " 'seem': 465,\n",
       " 'serene': 466,\n",
       " 'seven': 467,\n",
       " 'sex': 468,\n",
       " 'shade': 469,\n",
       " 'shall': 470,\n",
       " 'sharpness': 471,\n",
       " 'she': 472,\n",
       " 'shin': 473,\n",
       " 'shine': 474,\n",
       " 'shore': 475,\n",
       " 'should': 476,\n",
       " 'sigurd': 477,\n",
       " 'silent': 478,\n",
       " 'simpler': 479,\n",
       " 'sincerest': 480,\n",
       " 'sing': 481,\n",
       " 'sleep': 482,\n",
       " 'slight': 483,\n",
       " 'slumber': 484,\n",
       " 'smile': 485,\n",
       " 'snow': 486,\n",
       " 'so': 487,\n",
       " 'soft': 488,\n",
       " 'some': 489,\n",
       " 'sometimes': 490,\n",
       " 'song': 491,\n",
       " 'soul': 492,\n",
       " 'source': 493,\n",
       " 'spark': 494,\n",
       " 'speech': 495,\n",
       " 'spirit': 496,\n",
       " 'spoil': 497,\n",
       " 'spring': 498,\n",
       " 'stand': 499,\n",
       " 'star': 500,\n",
       " 'stately': 501,\n",
       " 'stept': 502,\n",
       " 'still': 503,\n",
       " 'stir': 504,\n",
       " 'stormy': 505,\n",
       " 'straight': 506,\n",
       " 'stream': 507,\n",
       " 'strife': 508,\n",
       " 'strike': 509,\n",
       " 'string': 510,\n",
       " 'strive': 511,\n",
       " 'such': 512,\n",
       " 'suddenly': 513,\n",
       " \"summer's\": 514,\n",
       " 'sun': 515,\n",
       " \"sundown's\": 516,\n",
       " 'superior': 517,\n",
       " 'surround': 518,\n",
       " 'sweet': 519,\n",
       " 'sweeter': 520,\n",
       " 'sweetly': 521,\n",
       " 'sweetness': 522,\n",
       " 'swifter': 523,\n",
       " 'sword': 524,\n",
       " 'sympathy': 525,\n",
       " 'symphony': 526,\n",
       " 'take': 527,\n",
       " 'teach': 528,\n",
       " 'tender': 529,\n",
       " 'tenderest': 530,\n",
       " 'thames': 531,\n",
       " 'than': 532,\n",
       " 'that': 533,\n",
       " 'the': 534,\n",
       " 'thee': 535,\n",
       " 'their': 536,\n",
       " 'then': 537,\n",
       " 'there': 538,\n",
       " 'thereupon': 539,\n",
       " 'these': 540,\n",
       " 'they': 541,\n",
       " 'thing': 542,\n",
       " 'think': 543,\n",
       " 'this': 544,\n",
       " 'those': 545,\n",
       " 'thou': 546,\n",
       " 'though': 547,\n",
       " 'thro': 548,\n",
       " 'throne': 549,\n",
       " 'throug': 550,\n",
       " 'through': 551,\n",
       " 'throw': 552,\n",
       " 'thunder': 553,\n",
       " 'thus': 554,\n",
       " 'thy': 555,\n",
       " 'till': 556,\n",
       " 'time': 557,\n",
       " 'tinkle': 558,\n",
       " 'to': 559,\n",
       " 'tongue': 560,\n",
       " 'too': 561,\n",
       " 'torch': 562,\n",
       " 'touch': 563,\n",
       " 'towards': 564,\n",
       " 'trail': 565,\n",
       " 'train': 566,\n",
       " 'tranquil': 567,\n",
       " 'trouble': 568,\n",
       " 'trust': 569,\n",
       " 'truth': 570,\n",
       " 'try': 571,\n",
       " 'turn': 572,\n",
       " 'twin': 573,\n",
       " 'u': 574,\n",
       " 'unity': 575,\n",
       " 'unrest': 576,\n",
       " 'unresting': 577,\n",
       " 'upon': 578,\n",
       " 'uppe': 579,\n",
       " 'utter': 580,\n",
       " 'vast': 581,\n",
       " 'very': 582,\n",
       " 'victor': 583,\n",
       " \"virtue's\": 584,\n",
       " 'voice': 585,\n",
       " 'vow': 586,\n",
       " 'wa': 587,\n",
       " 'wan': 588,\n",
       " 'war': 589,\n",
       " 'warm': 590,\n",
       " 'water': 591,\n",
       " 'we': 592,\n",
       " 'weary': 593,\n",
       " 'welcome': 594,\n",
       " 'what': 595,\n",
       " 'when': 596,\n",
       " 'where': 597,\n",
       " 'which': 598,\n",
       " 'whimper': 599,\n",
       " 'whine': 600,\n",
       " 'who': 601,\n",
       " 'whose': 602,\n",
       " 'why': 603,\n",
       " 'wild': 604,\n",
       " 'will': 605,\n",
       " 'wilt': 606,\n",
       " \"wisdom's\": 607,\n",
       " 'wise': 608,\n",
       " 'wit': 609,\n",
       " 'with': 610,\n",
       " 'woman': 611,\n",
       " 'wonder': 612,\n",
       " 'world': 613,\n",
       " 'worship': 614,\n",
       " 'worst': 615,\n",
       " 'worth': 616,\n",
       " 'worthy': 617,\n",
       " 'would': 618,\n",
       " 'wouldst': 619,\n",
       " 'wovest': 620,\n",
       " 'wrath': 621,\n",
       " 'wrong': 622,\n",
       " 'year': 623,\n",
       " 'yet': 624,\n",
       " 'you': 625,\n",
       " 'your': 626,\n",
       " 'yours': 627,\n",
       " \"youth's\": 628}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create word-index mappings\n",
    "word_indices = dict((word, index) for index, word in enumerate(unique_words))\n",
    "word_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6413184",
   "metadata": {},
   "source": [
    "### Create Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5afa53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x (input): Split text into blocks, where each block has the same amount of words\n",
    "# Create y (targets): For each x input, the y is the word that comes next\n",
    "# The model should learn to predict y from the input x\n",
    "\n",
    "block_size = 2\n",
    "step = 1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(word_tokens) - block_size, step):\n",
    "    x.append(word_tokens[i: i+block_size])\n",
    "    y.append(word_tokens[i + block_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83d0d331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['with', 'pale'],\n",
       " ['pale', 'blue'],\n",
       " ['blue', 'berry'],\n",
       " ['berry', 'in'],\n",
       " ['in', 'these']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect x\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1dd331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of blocks\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c87bed",
   "metadata": {},
   "source": [
    "### Create One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ba28e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoding of x\n",
    "x_encoded = []\n",
    "\n",
    "for x_arr in x:\n",
    "    x_ints = [word_indices[item] for item in x_arr]\n",
    "    \n",
    "    x_row = []\n",
    "    for item in x_ints:\n",
    "        x_vector = np.zeros(len(unique_words))\n",
    "        x_vector[item] = 1\n",
    "        x_row.append(x_vector)\n",
    "        \n",
    "    x_encoded.append(x_row)\n",
    "    \n",
    "x_encoded = np.array(x_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e351e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue', 'berry', 'in', 'these', 'peaceful']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect y\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea6c112d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55, 46, 258, 540, 387]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each word in y into their corresponding indices\n",
    "y_ints = [word_indices[item] for item in y]\n",
    "y_ints[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fc0bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoding of y\n",
    "y_encoded = []\n",
    "\n",
    "for item in y_ints:\n",
    "    y_vector = np.zeros(len(unique_words))\n",
    "    y_vector[item] = 1\n",
    "    y_encoded.append(y_vector)\n",
    "\n",
    "y_encoded = np.array(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcda4bf",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caa21464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Linear(input_dim, 2000)\n",
    "        self.hidden = nn.Linear(2000, 1200)\n",
    "        self.output = nn.Linear(1200, output_dim)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.embeddings(x))\n",
    "        x = self.tanh(self.hidden(x))\n",
    "        x = self.softmax(self.output(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6ff9f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258\n"
     ]
    }
   ],
   "source": [
    "# Get size of input for training the model\n",
    "input_size = x_encoded[0].ravel().shape[0]\n",
    "print(x_encoded[0].ravel().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df58af12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing torch operations on cpu device\n"
     ]
    }
   ],
   "source": [
    "# Allocate tensors to the device used for computation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Performing torch operations on {device} device\")\n",
    "\n",
    "# Create x and y PyTorch tensors\n",
    "x = torch.tensor(x_encoded).float().to(device)\n",
    "y = torch.tensor(y_encoded).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb023182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextGenerator(\n",
       "  (embeddings): Linear(in_features=1258, out_features=2000, bias=True)\n",
       "  (hidden): Linear(in_features=2000, out_features=1200, bias=True)\n",
       "  (output): Linear(in_features=1200, out_features=629, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = TextGenerator(input_size, len(unique_words), block_size).to(device)\n",
    "\n",
    "# Print model configuration\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "433009b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ee59c",
   "metadata": {},
   "source": [
    "### Create Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9e8b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom Dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        self.n_samples = len(x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index].ravel(), self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9228e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset using custom Dataset class\n",
    "training_ds = CustomDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a488527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset into DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    training_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03558f02",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a82f8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to train model\n",
    "def train_fn(loader, model, optimizer, loss_fn, device):\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    ave_loss = 0\n",
    "    count = 0 \n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        # Forward\n",
    "        predictions = model.forward(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update tqdm loading bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        count += 1\n",
    "        ave_loss += loss.item()\n",
    "    \n",
    "    ave_loss = ave_loss / count\n",
    "\n",
    "    return ave_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811b5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████████████████████████▊            | 107/130 [00:06<00:01, 13.38it/s, loss=6.45]"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 2 # TODO: CHANGE TO 300 ON FINAL DATA; CURRENTLY 2 FOR TESTING PURPOSES\n",
    "average_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    ave_loss = train_fn(train_loader, model, optimizer, criterion, device)\n",
    "    \n",
    "    print(\"Ave Loss: {}\".format(ave_loss))\n",
    "    average_losses.append(ave_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e315a81",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd01d2-95c1-4264-8a39-4d013120d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, num_words):\n",
    "    device = 'cpu'\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert seed_text to input tensor\n",
    "    seed_encoded = []\n",
    "    for word in seed_text.split():\n",
    "        word_index = word_indices[word]\n",
    "        word_encoded = np.zeros(len(unique_words))\n",
    "        word_encoded[word_index] = 1\n",
    "        seed_encoded.append(word_encoded)\n",
    "    seed_encoded = np.array(seed_encoded)\n",
    "    seed_encoded = np.expand_dims(seed_encoded, axis=0)\n",
    "    seed_tensor = torch.tensor(seed_encoded).float().to(device)\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = seed_text\n",
    "    for i in range(num_words):\n",
    "        predictions = model(seed_tensor)\n",
    "        predicted_index = torch.argmax(predictions, dim=1).item()\n",
    "        predicted_word = indices_words[predicted_index]\n",
    "        generated_text += ' ' + predicted_word\n",
    "        \n",
    "        # Update seed tensor with predicted word\n",
    "        predicted_encoded = np.zeros(len(unique_words))\n",
    "        predicted_encoded[predicted_index] = 1\n",
    "        predicted_encoded = np.expand_dims(predicted_encoded, axis=0)\n",
    "        seed_tensor = torch.cat((seed_tensor[:, 1:, :], torch.tensor(predicted_encoded).float().to(device)), axis=1)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140eb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text sample from model output\n",
    "word_count = 100\n",
    "text = []\n",
    "paragraph_count = 5\n",
    "\n",
    "# Length of phrase should be same as block_size\n",
    "word1, word2 = \"\\n\", \"\\n\"\n",
    "\n",
    "for p in range(paragraph_count):\n",
    "    text.append([])\n",
    "    \n",
    "    for i in range(word_count):\n",
    "        phrase = [word1, word2]\n",
    "        x_ints = [word_indices[item] for item in phrase]\n",
    "        x_vector = []\n",
    "\n",
    "        for item in x_ints:\n",
    "            x_item = np.zeros(len(unique_words))\n",
    "            x_item[item] = 1\n",
    "            x_vector.append(x_item)\n",
    "\n",
    "        initial_input = torch.tensor([np.array([x_vector]).ravel()]).float()\n",
    "\n",
    "        output = model(initial_input)[0].detach().cpu().numpy()\n",
    "\n",
    "        # Workaround to fix occasional sum(pvals[:-1]) > 1.0  bug from implicit casting in np.random.multinomial \n",
    "        output = output.astype(float)\n",
    "        output /= output.sum()\n",
    "\n",
    "        index = np.where(np.random.multinomial(1, output) == 1)[0][0]\n",
    "        word3 = indices_words[index]\n",
    "        text[p].append(word3)\n",
    "\n",
    "        # Use generated word from this run as seed for next run\n",
    "        word1, word2 = word2, word3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd566b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in range(paragraph_count):\n",
    "    print(f\"Generated Paragraph {p}:\")\n",
    "    print(' '.join(text[p]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d46cb7",
   "metadata": {},
   "source": [
    "### Create Feature Vectors from Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "# Tokenize by line\n",
    "for index, row in positive.iterrows():\n",
    "    tokenized_row = row['Text'].split(' ')\n",
    "    \n",
    "    # Preprocess using the same settings as preprocessing done before training model\n",
    "    tokenized_row = regexp_tokenize(' '.join(tokenized_row), pattern=r'[^\\S\\r]+|[\\.,;!?()--_\"]', gaps=True)\n",
    "    tokenized_row = [lemmatizer.lemmatize(token) for token in tokenized_row] # Lemmatize nouns\n",
    "    tokenized_row = [lemmatizer.lemmatize(token, 'v') for token in tokenized_row] # Lemmatize verbs\n",
    "    \n",
    "    sentences.append(tokenized_row)\n",
    "    \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "w2v_model = Word2Vec(sentences, vector_size=vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d010c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = w2v_model.wv.index_to_key\n",
    "vocab_length = len(vocab)\n",
    "\n",
    "# Note: Low vocab size is because Word2Vec model isn't familiar with most of the words in our dataset\n",
    "# From https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "print(f'Vocabulary Size: {format(vocab_length)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2016e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('my', 'me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector_averaging(sentence, model):\n",
    "    embeddings = [model.wv[word] for word in sentence if word in vocab]\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258efc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    features.append(sentence_to_vector_averaging(sentence, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f'x{i}' for i in range(w2v_model.vector_size)]\n",
    "\n",
    "df_text = pd.DataFrame(features, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb6064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_text['y'] = [float(x) for x in positive['Sentiment']]\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacbab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature vectors as csv\n",
    "df_text.to_csv('positive.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ecc06",
   "metadata": {},
   "source": [
    "### Create Feature Vectors from Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3088f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "w2v_model = Word2Vec(text, vector_size=vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcffb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = w2v_model.wv.index_to_key\n",
    "vocab_length = len(vocab)\n",
    "\n",
    "# Note: Low vocab size is because Word2Vec model isn't familiar with most of the words in our dataset\n",
    "# From https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "print(f'Vocabulary Size: {format(vocab_length)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [w2v_model.wv[word] for word in vocab]\n",
    "len(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1304879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector_averaging(sentence, model):\n",
    "    embeddings = [model.wv[word] for word in sentence if word in vocab]\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a160fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    features.append(sentence_to_vector_averaging(sentence, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f'x{i}' for i in range(w2v_model.vector_size)]\n",
    "\n",
    "df_text = pd.DataFrame(features, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcafa9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_text['y'] = [1] * len(df_text)\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature vectors as csv\n",
    "df_text.to_csv('positive_generated.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
