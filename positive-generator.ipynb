{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd12e830",
   "metadata": {},
   "source": [
    "# Positive Tone Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d930",
   "metadata": {},
   "source": [
    "This notebook contains code for an MLP neural network that generates positively toned data based on the dataset. Feature vectors are then made from the positively toned data and generated data then saved into .csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f0cda",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eccd3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import regexp_tokenize, WordNetLemmatizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f00057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c2279",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c222980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with pale blue berries. in these peaceful shad...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it flows so long as falls the rain</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and that is why, the lonesome day</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when i peruse the conquered fame of heroes, an...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of inward strife for truth and liberty.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>to his ears there came a murmur of far seas be...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>the one good man in the world who knows me, --</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>faint voices lifted shrill with pain</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>an', fust you knowed on, back come charles the...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>in the wild glens rough shepherds will deplore</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>892 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Sentiment\n",
       "0    with pale blue berries. in these peaceful shad...        1.0\n",
       "1                   it flows so long as falls the rain        0.0\n",
       "2                    and that is why, the lonesome day       -1.0\n",
       "3    when i peruse the conquered fame of heroes, an...        2.0\n",
       "4              of inward strife for truth and liberty.        2.0\n",
       "..                                                 ...        ...\n",
       "887  to his ears there came a murmur of far seas be...        0.0\n",
       "888     the one good man in the world who knows me, --        1.0\n",
       "889               faint voices lifted shrill with pain       -1.0\n",
       "890  an', fust you knowed on, back come charles the...        0.0\n",
       "891     in the wild glens rough shepherds will deplore       -1.0\n",
       "\n",
       "[892 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('poem_sentiment.csv', header=None, index_col=0, names=['Text', 'Sentiment'])\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a556975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with pale blue berries. in these peaceful shad...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when i peruse the conquered fame of heroes, an...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of inward strife for truth and liberty.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the red sword sealed their vows!</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>that has a charmingly bourbon air.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>their first-born brother as a god.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>and so i should be loved and mourned to-night.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>and _channing_, with his bland, superior look</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>how your soft opera-music changed, and the dru...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>the one good man in the world who knows me, --</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Sentiment\n",
       "0    with pale blue berries. in these peaceful shad...        1.0\n",
       "3    when i peruse the conquered fame of heroes, an...        2.0\n",
       "4              of inward strife for truth and liberty.        2.0\n",
       "5                     the red sword sealed their vows!        2.0\n",
       "16                  that has a charmingly bourbon air.        1.0\n",
       "..                                                 ...        ...\n",
       "870                 their first-born brother as a god.        1.0\n",
       "876     and so i should be loved and mourned to-night.        2.0\n",
       "877      and _channing_, with his bland, superior look        2.0\n",
       "884  how your soft opera-music changed, and the dru...        2.0\n",
       "888     the one good man in the world who knows me, --        1.0\n",
       "\n",
       "[182 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = df_raw[df_raw['Sentiment'] > 0]\n",
    "positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ca11d",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45cd3956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with pale blue berries. in these peaceful shades--\\nwhen i peruse the conquered fame of heroes, and...\\n           of inward strife for truth and liberty.\\n                  the red sword sealed their vows!\\n                that has a charmingly bourbon air.\\n          brightly expressive as the twins of leda\\n               in monumental pomp! no grecian drop\\n                    the hostile cohorts melt away;\\nand lips where heavenly smiles would hang and b...\\n                         honour to the bugle-horn!\\n                       if the pure and holy angels\\n        upon the thought of perfect noon. and when\\n      thy hands all cunning arts that women prize.\\n             reasoning to admiration, and with mee\\n           it shines superior on a throne of gold:\\n    take the warm welcome of new friends with thee\\n                  augmented, sweet, a hundred fold\\n                every day a rich reward will give;\\n                                 gay little heart!\\n         among the sources of t'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = positive['Text'].to_string(index=False)\n",
    "raw_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46bd8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with pale blue berries. in these peaceful shades--\\nwhen i peruse the conquered fame of heroes, and...\\n           of inward strife for truth and liberty.\\n                  the red sword sealed their vows!\\n                that has a charmingly bourbon air.\\n          brightly expressive as the twins of leda\\n               in monumental pomp! no grecian drop\\n                    the hostile cohorts melt away;\\nand lips where heavenly smiles would hang and b...\\n                         honour to the bugle-horn!\\n                       if the pure and holy angels\\n        upon the thought of perfect noon. and when\\n      thy hands all cunning arts that women prize.\\n             reasoning to admiration, and with mee\\n           it shines superior on a throne of gold:\\n    take the warm welcome of new friends with thee\\n                  augmented, sweet, a hundred fold\\n                every day a rich reward will give;\\n                                 gay little heart!\\n         among the sources of t'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all non-ASCII characters\n",
    "processed_text = re.sub(r'[^\\x00-\\x7f]', r'', raw_text).lower()\n",
    "processed_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd9097",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987aadca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word tokens: 1292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'pale',\n",
       " 'blue',\n",
       " 'berries',\n",
       " 'in',\n",
       " 'these',\n",
       " 'peaceful',\n",
       " 'shades',\n",
       " 'when',\n",
       " 'i']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get word tokens from text\n",
    "word_tokens = regexp_tokenize(processed_text, pattern=r'[^\\S\\r]+|[\\.,:;!?()--_\"]', gaps=True)\n",
    "print(f\"Number of word tokens: {len(word_tokens)}\")\n",
    "word_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea893c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization done to make uncommon words more likely to be recognized by \n",
    "# Word2Vec model later when converting to feature vectors\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word_tokens = [lemmatizer.lemmatize(token) for token in word_tokens] # Lemmatize nouns\n",
    "word_tokens = [lemmatizer.lemmatize(token, 'v') for token in word_tokens] # Lemmatize verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ff426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique word tokens: 628\n"
     ]
    }
   ],
   "source": [
    "# Get unique word tokens from word tokens\n",
    "unique_words = sorted(list(set(word_tokens)))\n",
    "print(f\"Number of unique word tokens: {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86037479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'tis\",\n",
       " 'a',\n",
       " 'abide',\n",
       " 'abloom',\n",
       " 'about',\n",
       " 'accordance',\n",
       " 'adam',\n",
       " 'adept',\n",
       " 'admiration',\n",
       " 'after']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vocabulary of word tokens\n",
    "word_vocabulary = unique_words\n",
    "word_vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cf74d",
   "metadata": {},
   "source": [
    "### Create word-index mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d81c158c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: \"'tis\",\n",
       " 1: 'a',\n",
       " 2: 'abide',\n",
       " 3: 'abloom',\n",
       " 4: 'about',\n",
       " 5: 'accordance',\n",
       " 6: 'adam',\n",
       " 7: 'adept',\n",
       " 8: 'admiration',\n",
       " 9: 'after',\n",
       " 10: 'again',\n",
       " 11: 'ah',\n",
       " 12: 'air',\n",
       " 13: 'all',\n",
       " 14: 'already',\n",
       " 15: 'amidst',\n",
       " 16: 'among',\n",
       " 17: 'an',\n",
       " 18: 'and',\n",
       " 19: 'angel',\n",
       " 20: 'angry',\n",
       " 21: 'arm',\n",
       " 22: 'around',\n",
       " 23: 'art',\n",
       " 24: 'ascend',\n",
       " 25: 'ash',\n",
       " 26: 'aspire',\n",
       " 27: 'assay',\n",
       " 28: 'augment',\n",
       " 29: 'away',\n",
       " 30: 'awe',\n",
       " 31: 'ay',\n",
       " 32: 'b',\n",
       " 33: 'bare',\n",
       " 34: 'be',\n",
       " 35: 'bear',\n",
       " 36: 'beauteous',\n",
       " 37: 'beautiful',\n",
       " 38: 'beauty',\n",
       " 39: \"beauty'\",\n",
       " 40: 'because',\n",
       " 41: 'before',\n",
       " 42: 'bell',\n",
       " 43: 'bend',\n",
       " 44: 'beneath',\n",
       " 45: 'berry',\n",
       " 46: 'best',\n",
       " 47: 'betray',\n",
       " 48: 'between',\n",
       " 49: 'blade',\n",
       " 50: 'bland',\n",
       " 51: 'blaze',\n",
       " 52: 'bless',\n",
       " 53: 'blind',\n",
       " 54: 'blue',\n",
       " 55: 'bolt',\n",
       " 56: 'borrow',\n",
       " 57: 'boston',\n",
       " 58: 'bourbon',\n",
       " 59: 'bow',\n",
       " 60: 'brave',\n",
       " 61: 'braver',\n",
       " 62: 'breast',\n",
       " 63: 'bright',\n",
       " 64: 'brightly',\n",
       " 65: 'brilliant',\n",
       " 66: 'brother',\n",
       " 67: \"brynhilda's\",\n",
       " 68: 'bugle',\n",
       " 69: 'burn',\n",
       " 70: 'burst',\n",
       " 71: 'but',\n",
       " 72: 'by',\n",
       " 73: 'cabinet',\n",
       " 74: 'call',\n",
       " 75: \"call'd\",\n",
       " 76: 'calm',\n",
       " 77: 'can',\n",
       " 78: \"ceas'd\",\n",
       " 79: 'chair',\n",
       " 80: 'change',\n",
       " 81: 'channing',\n",
       " 82: 'chariot',\n",
       " 83: 'charmingly',\n",
       " 84: 'cheer',\n",
       " 85: \"childhood's\",\n",
       " 86: 'chime',\n",
       " 87: 'china',\n",
       " 88: 'circle',\n",
       " 89: 'civil',\n",
       " 90: 'clear',\n",
       " 91: 'clearer',\n",
       " 92: 'climax',\n",
       " 93: 'close',\n",
       " 94: 'cloud',\n",
       " 95: 'cohort',\n",
       " 96: 'cometh',\n",
       " 97: 'command',\n",
       " 98: 'commend',\n",
       " 99: 'concentric',\n",
       " 100: 'conquer',\n",
       " 101: 'conviction',\n",
       " 102: 'corpse',\n",
       " 103: 'costume',\n",
       " 104: 'country',\n",
       " 105: 'course',\n",
       " 106: 'creed',\n",
       " 107: 'creep',\n",
       " 108: 'crown',\n",
       " 109: 'cunning',\n",
       " 110: 'cup',\n",
       " 111: 'curl',\n",
       " 112: 'cycle',\n",
       " 113: 'dark',\n",
       " 114: 'darling',\n",
       " 115: 'day',\n",
       " 116: 'de',\n",
       " 117: 'dead',\n",
       " 118: 'dear',\n",
       " 119: 'dearest',\n",
       " 120: 'death',\n",
       " 121: 'deathward',\n",
       " 122: 'declare',\n",
       " 123: 'delicious',\n",
       " 124: 'delight',\n",
       " 125: 'despise',\n",
       " 126: 'dim',\n",
       " 127: 'distil',\n",
       " 128: 'do',\n",
       " 129: 'doe',\n",
       " 130: 'doth',\n",
       " 131: 'down',\n",
       " 132: 'dream',\n",
       " 133: 'drop',\n",
       " 134: 'drum',\n",
       " 135: 'dumb',\n",
       " 136: 'dustless',\n",
       " 137: 'duty',\n",
       " 138: 'earl',\n",
       " 139: 'embrace',\n",
       " 140: 'enfold',\n",
       " 141: 'enstamped',\n",
       " 142: 'entrance',\n",
       " 143: 'ere',\n",
       " 144: 'estrange',\n",
       " 145: 'eve',\n",
       " 146: 'even',\n",
       " 147: 'ever',\n",
       " 148: 'every',\n",
       " 149: 'expressive',\n",
       " 150: 'eye',\n",
       " 151: 'face',\n",
       " 152: 'fade',\n",
       " 153: 'faint',\n",
       " 154: 'fair',\n",
       " 155: 'fairer',\n",
       " 156: 'fame',\n",
       " 157: 'far',\n",
       " 158: 'farr',\n",
       " 159: 'faster',\n",
       " 160: 'father',\n",
       " 161: 'fear',\n",
       " 162: 'fearless',\n",
       " 163: 'feel',\n",
       " 164: 'fell',\n",
       " 165: 'felt',\n",
       " 166: 'field',\n",
       " 167: 'fill',\n",
       " 168: 'first',\n",
       " 169: 'flame',\n",
       " 170: 'flow',\n",
       " 171: 'flower',\n",
       " 172: 'foe',\n",
       " 173: 'fold',\n",
       " 174: 'folk',\n",
       " 175: 'fond',\n",
       " 176: 'foot',\n",
       " 177: 'for',\n",
       " 178: 'force',\n",
       " 179: 'forget',\n",
       " 180: 'fragrance',\n",
       " 181: 'free',\n",
       " 182: \"freedom's\",\n",
       " 183: 'friend',\n",
       " 184: \"friendship'\",\n",
       " 185: \"friendship's\",\n",
       " 186: 'from',\n",
       " 187: 'full',\n",
       " 188: 'gay',\n",
       " 189: 'gem',\n",
       " 190: 'give',\n",
       " 191: 'glad',\n",
       " 192: 'glorious',\n",
       " 193: 'glory',\n",
       " 194: \"glory's\",\n",
       " 195: 'glow',\n",
       " 196: 'goal',\n",
       " 197: 'god',\n",
       " 198: 'gold',\n",
       " 199: 'golden',\n",
       " 200: 'good',\n",
       " 201: 'goodness',\n",
       " 202: 'gore',\n",
       " 203: 'goth',\n",
       " 204: 'grace',\n",
       " 205: 'gracious',\n",
       " 206: 'graciousness',\n",
       " 207: 'grander',\n",
       " 208: 'grave',\n",
       " 209: 'gre',\n",
       " 210: 'great',\n",
       " 211: 'grecian',\n",
       " 212: 'grimy',\n",
       " 213: 'grind',\n",
       " 214: 'gudrun',\n",
       " 215: 'ha',\n",
       " 216: 'hadst',\n",
       " 217: 'halcyon',\n",
       " 218: 'hale',\n",
       " 219: 'hand',\n",
       " 220: 'hang',\n",
       " 221: 'happy',\n",
       " 222: 'harmless',\n",
       " 223: 'harriet',\n",
       " 224: 'hast',\n",
       " 225: 'hate',\n",
       " 226: 'have',\n",
       " 227: 'he',\n",
       " 228: 'head',\n",
       " 229: 'heart',\n",
       " 230: \"heav'n\",\n",
       " 231: 'heaven',\n",
       " 232: \"heaven's\",\n",
       " 233: 'heavenly',\n",
       " 234: 'heed',\n",
       " 235: 'her',\n",
       " 236: 'here',\n",
       " 237: 'hero',\n",
       " 238: 'high',\n",
       " 239: 'hint',\n",
       " 240: 'his',\n",
       " 241: 'hoarse',\n",
       " 242: 'holy',\n",
       " 243: 'home',\n",
       " 244: 'honest',\n",
       " 245: 'honour',\n",
       " 246: 'hope',\n",
       " 247: 'horn',\n",
       " 248: 'hostile',\n",
       " 249: 'how',\n",
       " 250: 'hue',\n",
       " 251: 'human',\n",
       " 252: 'hundred',\n",
       " 253: \"hunter's\",\n",
       " 254: 'i',\n",
       " 255: 'if',\n",
       " 256: 'image',\n",
       " 257: 'in',\n",
       " 258: 'inward',\n",
       " 259: 'isadore',\n",
       " 260: 'it',\n",
       " 261: 'itself',\n",
       " 262: 'jet',\n",
       " 263: 'jewel',\n",
       " 264: 'john',\n",
       " 265: 'joy',\n",
       " 266: 'joyous',\n",
       " 267: 'just',\n",
       " 268: 'k',\n",
       " 269: 'keen',\n",
       " 270: 'keep',\n",
       " 271: 'kindle',\n",
       " 272: 'king',\n",
       " 273: \"king's\",\n",
       " 274: 'kiss',\n",
       " 275: 'kneel',\n",
       " 276: 'knife',\n",
       " 277: 'knightly',\n",
       " 278: 'know',\n",
       " 279: 'lamp',\n",
       " 280: 'land',\n",
       " 281: 'lap',\n",
       " 282: 'large',\n",
       " 283: 'last',\n",
       " 284: 'laugh',\n",
       " 285: 'leda',\n",
       " 286: 'leprous',\n",
       " 287: 'liberty',\n",
       " 288: 'life',\n",
       " 289: 'light',\n",
       " 290: 'like',\n",
       " 291: 'limpid',\n",
       " 292: 'lion',\n",
       " 293: 'lip',\n",
       " 294: 'little',\n",
       " 295: 'live',\n",
       " 296: 'long',\n",
       " 297: 'look',\n",
       " 298: 'love',\n",
       " 299: 'loveliest',\n",
       " 300: 'lovely',\n",
       " 301: 'low',\n",
       " 302: 'lulld',\n",
       " 303: 'luminous',\n",
       " 304: 'lure',\n",
       " 305: 'luting',\n",
       " 306: 'majestical',\n",
       " 307: 'make',\n",
       " 308: 'man',\n",
       " 309: 'manner',\n",
       " 310: 'martineau',\n",
       " 311: 'master',\n",
       " 312: 'matter',\n",
       " 313: 'may',\n",
       " 314: 'me',\n",
       " 315: 'mee',\n",
       " 316: 'melt',\n",
       " 317: 'memory',\n",
       " 318: 'men',\n",
       " 319: 'merciful',\n",
       " 320: 'merit',\n",
       " 321: 'might',\n",
       " 322: 'mightily',\n",
       " 323: 'mighty',\n",
       " 324: 'million',\n",
       " 325: 'mind',\n",
       " 326: 'mine',\n",
       " 327: 'mirror',\n",
       " 328: 'miscall',\n",
       " 329: 'miss',\n",
       " 330: 'monumental',\n",
       " 331: 'moonstruck',\n",
       " 332: 'more',\n",
       " 333: 'morn',\n",
       " 334: 'morning',\n",
       " 335: 'moses',\n",
       " 336: 'most',\n",
       " 337: 'mother',\n",
       " 338: 'mount',\n",
       " 339: 'mourn',\n",
       " 340: 'music',\n",
       " 341: 'my',\n",
       " 342: 'myriad',\n",
       " 343: 'nature',\n",
       " 344: \"nature's\",\n",
       " 345: 'never',\n",
       " 346: 'new',\n",
       " 347: 'night',\n",
       " 348: 'no',\n",
       " 349: 'noble',\n",
       " 350: 'nobler',\n",
       " 351: 'noon',\n",
       " 352: 'nor',\n",
       " 353: 'o',\n",
       " 354: \"o'er\",\n",
       " 355: 'obscurity',\n",
       " 356: 'ocean',\n",
       " 357: 'of',\n",
       " 358: 'off',\n",
       " 359: 'oh',\n",
       " 360: 'old',\n",
       " 361: 'on',\n",
       " 362: 'onaway',\n",
       " 363: 'once',\n",
       " 364: 'one',\n",
       " 365: 'only',\n",
       " 366: 'onward',\n",
       " 367: 'opera',\n",
       " 368: 'or',\n",
       " 369: 'orchestra',\n",
       " 370: 'ordain',\n",
       " 371: 'our',\n",
       " 372: 'out',\n",
       " 373: 'outshine',\n",
       " 374: 'outward',\n",
       " 375: 'overcome',\n",
       " 376: 'own',\n",
       " 377: 'paint',\n",
       " 378: 'pale',\n",
       " 379: 'palm',\n",
       " 380: 'pas',\n",
       " 381: 'pass',\n",
       " 382: 'passionate',\n",
       " 383: 'past',\n",
       " 384: 'paynim',\n",
       " 385: 'peace',\n",
       " 386: 'peaceful',\n",
       " 387: 'peak',\n",
       " 388: 'pen',\n",
       " 389: 'perfect',\n",
       " 390: 'period',\n",
       " 391: 'peruse',\n",
       " 392: 'pilot',\n",
       " 393: 'place',\n",
       " 394: 'play',\n",
       " 395: \"playmates'\",\n",
       " 396: \"pleas'd\",\n",
       " 397: 'please',\n",
       " 398: 'pluck',\n",
       " 399: 'plume',\n",
       " 400: 'poem',\n",
       " 401: 'poesy',\n",
       " 402: 'pomp',\n",
       " 403: 'port',\n",
       " 404: 'potent',\n",
       " 405: 'praise',\n",
       " 406: 'pray',\n",
       " 407: 'prayer',\n",
       " 408: 'presidency',\n",
       " 409: 'president',\n",
       " 410: 'pride',\n",
       " 411: 'privilege',\n",
       " 412: 'prize',\n",
       " 413: 'profound',\n",
       " 414: 'profounds',\n",
       " 415: 'prosper',\n",
       " 416: 'prosperous',\n",
       " 417: 'pull',\n",
       " 418: 'pulse',\n",
       " 419: 'pure',\n",
       " 420: 'quaver',\n",
       " 421: 'queen',\n",
       " 422: 'quoth',\n",
       " 423: 'radiant',\n",
       " 424: 'raise',\n",
       " 425: 'rapid',\n",
       " 426: 'rapture',\n",
       " 427: 'reason',\n",
       " 428: 'record',\n",
       " 429: 'red',\n",
       " 430: 'regal',\n",
       " 431: 'remain',\n",
       " 432: 'remembrance',\n",
       " 433: 'renew',\n",
       " 434: 'repay',\n",
       " 435: 'rest',\n",
       " 436: 'revive',\n",
       " 437: 'reward',\n",
       " 438: 'rhyme',\n",
       " 439: 'ri',\n",
       " 440: 'rich',\n",
       " 441: 'ride',\n",
       " 442: 'right',\n",
       " 443: 'ring',\n",
       " 444: 'ringer',\n",
       " 445: 'rise',\n",
       " 446: 'river',\n",
       " 447: 'robe',\n",
       " 448: 'rochambeau',\n",
       " 449: 'rome',\n",
       " 450: 'round',\n",
       " 451: 'row',\n",
       " 452: 'ruby',\n",
       " 453: 'run',\n",
       " 454: 's',\n",
       " 455: 'sacred',\n",
       " 456: 'saintly',\n",
       " 457: 'say',\n",
       " 458: 'science',\n",
       " 459: 'sea',\n",
       " 460: 'seal',\n",
       " 461: 'seat',\n",
       " 462: 'see',\n",
       " 463: 'seed',\n",
       " 464: 'seem',\n",
       " 465: 'serene',\n",
       " 466: 'seven',\n",
       " 467: 'sex',\n",
       " 468: 'shade',\n",
       " 469: 'shall',\n",
       " 470: 'sharpness',\n",
       " 471: 'she',\n",
       " 472: 'shin',\n",
       " 473: 'shine',\n",
       " 474: 'shore',\n",
       " 475: 'should',\n",
       " 476: 'sigurd',\n",
       " 477: 'silent',\n",
       " 478: 'simpler',\n",
       " 479: 'sincerest',\n",
       " 480: 'sing',\n",
       " 481: 'sleep',\n",
       " 482: 'slight',\n",
       " 483: 'slumber',\n",
       " 484: 'smile',\n",
       " 485: 'snow',\n",
       " 486: 'so',\n",
       " 487: 'soft',\n",
       " 488: 'some',\n",
       " 489: 'sometimes',\n",
       " 490: 'song',\n",
       " 491: 'soul',\n",
       " 492: 'source',\n",
       " 493: 'spark',\n",
       " 494: 'speech',\n",
       " 495: 'spirit',\n",
       " 496: 'spoil',\n",
       " 497: 'spring',\n",
       " 498: 'stand',\n",
       " 499: 'star',\n",
       " 500: 'stately',\n",
       " 501: 'stept',\n",
       " 502: 'still',\n",
       " 503: 'stir',\n",
       " 504: 'stormy',\n",
       " 505: 'straight',\n",
       " 506: 'stream',\n",
       " 507: 'strife',\n",
       " 508: 'strike',\n",
       " 509: 'string',\n",
       " 510: 'strive',\n",
       " 511: 'such',\n",
       " 512: 'suddenly',\n",
       " 513: \"summer's\",\n",
       " 514: 'sun',\n",
       " 515: \"sundown's\",\n",
       " 516: 'superior',\n",
       " 517: 'surround',\n",
       " 518: 'sweet',\n",
       " 519: 'sweeter',\n",
       " 520: 'sweetly',\n",
       " 521: 'sweetness',\n",
       " 522: 'swifter',\n",
       " 523: 'sword',\n",
       " 524: 'sympathy',\n",
       " 525: 'symphony',\n",
       " 526: 'take',\n",
       " 527: 'teach',\n",
       " 528: 'tender',\n",
       " 529: 'tenderest',\n",
       " 530: 'thames',\n",
       " 531: 'than',\n",
       " 532: 'that',\n",
       " 533: 'the',\n",
       " 534: 'thee',\n",
       " 535: 'their',\n",
       " 536: 'then',\n",
       " 537: 'there',\n",
       " 538: 'thereupon',\n",
       " 539: 'these',\n",
       " 540: 'they',\n",
       " 541: 'thing',\n",
       " 542: 'think',\n",
       " 543: 'this',\n",
       " 544: 'those',\n",
       " 545: 'thou',\n",
       " 546: 'though',\n",
       " 547: 'thro',\n",
       " 548: 'throne',\n",
       " 549: 'throug',\n",
       " 550: 'through',\n",
       " 551: 'throw',\n",
       " 552: 'thunder',\n",
       " 553: 'thus',\n",
       " 554: 'thy',\n",
       " 555: 'till',\n",
       " 556: 'time',\n",
       " 557: 'tinkle',\n",
       " 558: 'to',\n",
       " 559: 'tongue',\n",
       " 560: 'too',\n",
       " 561: 'torch',\n",
       " 562: 'touch',\n",
       " 563: 'towards',\n",
       " 564: 'trail',\n",
       " 565: 'train',\n",
       " 566: 'tranquil',\n",
       " 567: 'trouble',\n",
       " 568: 'trust',\n",
       " 569: 'truth',\n",
       " 570: 'try',\n",
       " 571: 'turn',\n",
       " 572: 'twin',\n",
       " 573: 'u',\n",
       " 574: 'unity',\n",
       " 575: 'unrest',\n",
       " 576: 'unresting',\n",
       " 577: 'upon',\n",
       " 578: 'uppe',\n",
       " 579: 'utter',\n",
       " 580: 'vast',\n",
       " 581: 'very',\n",
       " 582: 'victor',\n",
       " 583: \"virtue's\",\n",
       " 584: 'voice',\n",
       " 585: 'vow',\n",
       " 586: 'wa',\n",
       " 587: 'wan',\n",
       " 588: 'war',\n",
       " 589: 'warm',\n",
       " 590: 'water',\n",
       " 591: 'we',\n",
       " 592: 'weary',\n",
       " 593: 'welcome',\n",
       " 594: 'what',\n",
       " 595: 'when',\n",
       " 596: 'where',\n",
       " 597: 'which',\n",
       " 598: 'whimper',\n",
       " 599: 'whine',\n",
       " 600: 'who',\n",
       " 601: 'whose',\n",
       " 602: 'why',\n",
       " 603: 'wild',\n",
       " 604: 'will',\n",
       " 605: 'wilt',\n",
       " 606: \"wisdom's\",\n",
       " 607: 'wise',\n",
       " 608: 'wit',\n",
       " 609: 'with',\n",
       " 610: 'woman',\n",
       " 611: 'wonder',\n",
       " 612: 'world',\n",
       " 613: 'worship',\n",
       " 614: 'worst',\n",
       " 615: 'worth',\n",
       " 616: 'worthy',\n",
       " 617: 'would',\n",
       " 618: 'wouldst',\n",
       " 619: 'wovest',\n",
       " 620: 'wrath',\n",
       " 621: 'wrong',\n",
       " 622: 'year',\n",
       " 623: 'yet',\n",
       " 624: 'you',\n",
       " 625: 'your',\n",
       " 626: 'yours',\n",
       " 627: \"youth's\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create index-word mappings \n",
    "indices_words = dict((index, word) for index, word in enumerate(unique_words))\n",
    "indices_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e485f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'tis\": 0,\n",
       " 'a': 1,\n",
       " 'abide': 2,\n",
       " 'abloom': 3,\n",
       " 'about': 4,\n",
       " 'accordance': 5,\n",
       " 'adam': 6,\n",
       " 'adept': 7,\n",
       " 'admiration': 8,\n",
       " 'after': 9,\n",
       " 'again': 10,\n",
       " 'ah': 11,\n",
       " 'air': 12,\n",
       " 'all': 13,\n",
       " 'already': 14,\n",
       " 'amidst': 15,\n",
       " 'among': 16,\n",
       " 'an': 17,\n",
       " 'and': 18,\n",
       " 'angel': 19,\n",
       " 'angry': 20,\n",
       " 'arm': 21,\n",
       " 'around': 22,\n",
       " 'art': 23,\n",
       " 'ascend': 24,\n",
       " 'ash': 25,\n",
       " 'aspire': 26,\n",
       " 'assay': 27,\n",
       " 'augment': 28,\n",
       " 'away': 29,\n",
       " 'awe': 30,\n",
       " 'ay': 31,\n",
       " 'b': 32,\n",
       " 'bare': 33,\n",
       " 'be': 34,\n",
       " 'bear': 35,\n",
       " 'beauteous': 36,\n",
       " 'beautiful': 37,\n",
       " 'beauty': 38,\n",
       " \"beauty'\": 39,\n",
       " 'because': 40,\n",
       " 'before': 41,\n",
       " 'bell': 42,\n",
       " 'bend': 43,\n",
       " 'beneath': 44,\n",
       " 'berry': 45,\n",
       " 'best': 46,\n",
       " 'betray': 47,\n",
       " 'between': 48,\n",
       " 'blade': 49,\n",
       " 'bland': 50,\n",
       " 'blaze': 51,\n",
       " 'bless': 52,\n",
       " 'blind': 53,\n",
       " 'blue': 54,\n",
       " 'bolt': 55,\n",
       " 'borrow': 56,\n",
       " 'boston': 57,\n",
       " 'bourbon': 58,\n",
       " 'bow': 59,\n",
       " 'brave': 60,\n",
       " 'braver': 61,\n",
       " 'breast': 62,\n",
       " 'bright': 63,\n",
       " 'brightly': 64,\n",
       " 'brilliant': 65,\n",
       " 'brother': 66,\n",
       " \"brynhilda's\": 67,\n",
       " 'bugle': 68,\n",
       " 'burn': 69,\n",
       " 'burst': 70,\n",
       " 'but': 71,\n",
       " 'by': 72,\n",
       " 'cabinet': 73,\n",
       " 'call': 74,\n",
       " \"call'd\": 75,\n",
       " 'calm': 76,\n",
       " 'can': 77,\n",
       " \"ceas'd\": 78,\n",
       " 'chair': 79,\n",
       " 'change': 80,\n",
       " 'channing': 81,\n",
       " 'chariot': 82,\n",
       " 'charmingly': 83,\n",
       " 'cheer': 84,\n",
       " \"childhood's\": 85,\n",
       " 'chime': 86,\n",
       " 'china': 87,\n",
       " 'circle': 88,\n",
       " 'civil': 89,\n",
       " 'clear': 90,\n",
       " 'clearer': 91,\n",
       " 'climax': 92,\n",
       " 'close': 93,\n",
       " 'cloud': 94,\n",
       " 'cohort': 95,\n",
       " 'cometh': 96,\n",
       " 'command': 97,\n",
       " 'commend': 98,\n",
       " 'concentric': 99,\n",
       " 'conquer': 100,\n",
       " 'conviction': 101,\n",
       " 'corpse': 102,\n",
       " 'costume': 103,\n",
       " 'country': 104,\n",
       " 'course': 105,\n",
       " 'creed': 106,\n",
       " 'creep': 107,\n",
       " 'crown': 108,\n",
       " 'cunning': 109,\n",
       " 'cup': 110,\n",
       " 'curl': 111,\n",
       " 'cycle': 112,\n",
       " 'dark': 113,\n",
       " 'darling': 114,\n",
       " 'day': 115,\n",
       " 'de': 116,\n",
       " 'dead': 117,\n",
       " 'dear': 118,\n",
       " 'dearest': 119,\n",
       " 'death': 120,\n",
       " 'deathward': 121,\n",
       " 'declare': 122,\n",
       " 'delicious': 123,\n",
       " 'delight': 124,\n",
       " 'despise': 125,\n",
       " 'dim': 126,\n",
       " 'distil': 127,\n",
       " 'do': 128,\n",
       " 'doe': 129,\n",
       " 'doth': 130,\n",
       " 'down': 131,\n",
       " 'dream': 132,\n",
       " 'drop': 133,\n",
       " 'drum': 134,\n",
       " 'dumb': 135,\n",
       " 'dustless': 136,\n",
       " 'duty': 137,\n",
       " 'earl': 138,\n",
       " 'embrace': 139,\n",
       " 'enfold': 140,\n",
       " 'enstamped': 141,\n",
       " 'entrance': 142,\n",
       " 'ere': 143,\n",
       " 'estrange': 144,\n",
       " 'eve': 145,\n",
       " 'even': 146,\n",
       " 'ever': 147,\n",
       " 'every': 148,\n",
       " 'expressive': 149,\n",
       " 'eye': 150,\n",
       " 'face': 151,\n",
       " 'fade': 152,\n",
       " 'faint': 153,\n",
       " 'fair': 154,\n",
       " 'fairer': 155,\n",
       " 'fame': 156,\n",
       " 'far': 157,\n",
       " 'farr': 158,\n",
       " 'faster': 159,\n",
       " 'father': 160,\n",
       " 'fear': 161,\n",
       " 'fearless': 162,\n",
       " 'feel': 163,\n",
       " 'fell': 164,\n",
       " 'felt': 165,\n",
       " 'field': 166,\n",
       " 'fill': 167,\n",
       " 'first': 168,\n",
       " 'flame': 169,\n",
       " 'flow': 170,\n",
       " 'flower': 171,\n",
       " 'foe': 172,\n",
       " 'fold': 173,\n",
       " 'folk': 174,\n",
       " 'fond': 175,\n",
       " 'foot': 176,\n",
       " 'for': 177,\n",
       " 'force': 178,\n",
       " 'forget': 179,\n",
       " 'fragrance': 180,\n",
       " 'free': 181,\n",
       " \"freedom's\": 182,\n",
       " 'friend': 183,\n",
       " \"friendship'\": 184,\n",
       " \"friendship's\": 185,\n",
       " 'from': 186,\n",
       " 'full': 187,\n",
       " 'gay': 188,\n",
       " 'gem': 189,\n",
       " 'give': 190,\n",
       " 'glad': 191,\n",
       " 'glorious': 192,\n",
       " 'glory': 193,\n",
       " \"glory's\": 194,\n",
       " 'glow': 195,\n",
       " 'goal': 196,\n",
       " 'god': 197,\n",
       " 'gold': 198,\n",
       " 'golden': 199,\n",
       " 'good': 200,\n",
       " 'goodness': 201,\n",
       " 'gore': 202,\n",
       " 'goth': 203,\n",
       " 'grace': 204,\n",
       " 'gracious': 205,\n",
       " 'graciousness': 206,\n",
       " 'grander': 207,\n",
       " 'grave': 208,\n",
       " 'gre': 209,\n",
       " 'great': 210,\n",
       " 'grecian': 211,\n",
       " 'grimy': 212,\n",
       " 'grind': 213,\n",
       " 'gudrun': 214,\n",
       " 'ha': 215,\n",
       " 'hadst': 216,\n",
       " 'halcyon': 217,\n",
       " 'hale': 218,\n",
       " 'hand': 219,\n",
       " 'hang': 220,\n",
       " 'happy': 221,\n",
       " 'harmless': 222,\n",
       " 'harriet': 223,\n",
       " 'hast': 224,\n",
       " 'hate': 225,\n",
       " 'have': 226,\n",
       " 'he': 227,\n",
       " 'head': 228,\n",
       " 'heart': 229,\n",
       " \"heav'n\": 230,\n",
       " 'heaven': 231,\n",
       " \"heaven's\": 232,\n",
       " 'heavenly': 233,\n",
       " 'heed': 234,\n",
       " 'her': 235,\n",
       " 'here': 236,\n",
       " 'hero': 237,\n",
       " 'high': 238,\n",
       " 'hint': 239,\n",
       " 'his': 240,\n",
       " 'hoarse': 241,\n",
       " 'holy': 242,\n",
       " 'home': 243,\n",
       " 'honest': 244,\n",
       " 'honour': 245,\n",
       " 'hope': 246,\n",
       " 'horn': 247,\n",
       " 'hostile': 248,\n",
       " 'how': 249,\n",
       " 'hue': 250,\n",
       " 'human': 251,\n",
       " 'hundred': 252,\n",
       " \"hunter's\": 253,\n",
       " 'i': 254,\n",
       " 'if': 255,\n",
       " 'image': 256,\n",
       " 'in': 257,\n",
       " 'inward': 258,\n",
       " 'isadore': 259,\n",
       " 'it': 260,\n",
       " 'itself': 261,\n",
       " 'jet': 262,\n",
       " 'jewel': 263,\n",
       " 'john': 264,\n",
       " 'joy': 265,\n",
       " 'joyous': 266,\n",
       " 'just': 267,\n",
       " 'k': 268,\n",
       " 'keen': 269,\n",
       " 'keep': 270,\n",
       " 'kindle': 271,\n",
       " 'king': 272,\n",
       " \"king's\": 273,\n",
       " 'kiss': 274,\n",
       " 'kneel': 275,\n",
       " 'knife': 276,\n",
       " 'knightly': 277,\n",
       " 'know': 278,\n",
       " 'lamp': 279,\n",
       " 'land': 280,\n",
       " 'lap': 281,\n",
       " 'large': 282,\n",
       " 'last': 283,\n",
       " 'laugh': 284,\n",
       " 'leda': 285,\n",
       " 'leprous': 286,\n",
       " 'liberty': 287,\n",
       " 'life': 288,\n",
       " 'light': 289,\n",
       " 'like': 290,\n",
       " 'limpid': 291,\n",
       " 'lion': 292,\n",
       " 'lip': 293,\n",
       " 'little': 294,\n",
       " 'live': 295,\n",
       " 'long': 296,\n",
       " 'look': 297,\n",
       " 'love': 298,\n",
       " 'loveliest': 299,\n",
       " 'lovely': 300,\n",
       " 'low': 301,\n",
       " 'lulld': 302,\n",
       " 'luminous': 303,\n",
       " 'lure': 304,\n",
       " 'luting': 305,\n",
       " 'majestical': 306,\n",
       " 'make': 307,\n",
       " 'man': 308,\n",
       " 'manner': 309,\n",
       " 'martineau': 310,\n",
       " 'master': 311,\n",
       " 'matter': 312,\n",
       " 'may': 313,\n",
       " 'me': 314,\n",
       " 'mee': 315,\n",
       " 'melt': 316,\n",
       " 'memory': 317,\n",
       " 'men': 318,\n",
       " 'merciful': 319,\n",
       " 'merit': 320,\n",
       " 'might': 321,\n",
       " 'mightily': 322,\n",
       " 'mighty': 323,\n",
       " 'million': 324,\n",
       " 'mind': 325,\n",
       " 'mine': 326,\n",
       " 'mirror': 327,\n",
       " 'miscall': 328,\n",
       " 'miss': 329,\n",
       " 'monumental': 330,\n",
       " 'moonstruck': 331,\n",
       " 'more': 332,\n",
       " 'morn': 333,\n",
       " 'morning': 334,\n",
       " 'moses': 335,\n",
       " 'most': 336,\n",
       " 'mother': 337,\n",
       " 'mount': 338,\n",
       " 'mourn': 339,\n",
       " 'music': 340,\n",
       " 'my': 341,\n",
       " 'myriad': 342,\n",
       " 'nature': 343,\n",
       " \"nature's\": 344,\n",
       " 'never': 345,\n",
       " 'new': 346,\n",
       " 'night': 347,\n",
       " 'no': 348,\n",
       " 'noble': 349,\n",
       " 'nobler': 350,\n",
       " 'noon': 351,\n",
       " 'nor': 352,\n",
       " 'o': 353,\n",
       " \"o'er\": 354,\n",
       " 'obscurity': 355,\n",
       " 'ocean': 356,\n",
       " 'of': 357,\n",
       " 'off': 358,\n",
       " 'oh': 359,\n",
       " 'old': 360,\n",
       " 'on': 361,\n",
       " 'onaway': 362,\n",
       " 'once': 363,\n",
       " 'one': 364,\n",
       " 'only': 365,\n",
       " 'onward': 366,\n",
       " 'opera': 367,\n",
       " 'or': 368,\n",
       " 'orchestra': 369,\n",
       " 'ordain': 370,\n",
       " 'our': 371,\n",
       " 'out': 372,\n",
       " 'outshine': 373,\n",
       " 'outward': 374,\n",
       " 'overcome': 375,\n",
       " 'own': 376,\n",
       " 'paint': 377,\n",
       " 'pale': 378,\n",
       " 'palm': 379,\n",
       " 'pas': 380,\n",
       " 'pass': 381,\n",
       " 'passionate': 382,\n",
       " 'past': 383,\n",
       " 'paynim': 384,\n",
       " 'peace': 385,\n",
       " 'peaceful': 386,\n",
       " 'peak': 387,\n",
       " 'pen': 388,\n",
       " 'perfect': 389,\n",
       " 'period': 390,\n",
       " 'peruse': 391,\n",
       " 'pilot': 392,\n",
       " 'place': 393,\n",
       " 'play': 394,\n",
       " \"playmates'\": 395,\n",
       " \"pleas'd\": 396,\n",
       " 'please': 397,\n",
       " 'pluck': 398,\n",
       " 'plume': 399,\n",
       " 'poem': 400,\n",
       " 'poesy': 401,\n",
       " 'pomp': 402,\n",
       " 'port': 403,\n",
       " 'potent': 404,\n",
       " 'praise': 405,\n",
       " 'pray': 406,\n",
       " 'prayer': 407,\n",
       " 'presidency': 408,\n",
       " 'president': 409,\n",
       " 'pride': 410,\n",
       " 'privilege': 411,\n",
       " 'prize': 412,\n",
       " 'profound': 413,\n",
       " 'profounds': 414,\n",
       " 'prosper': 415,\n",
       " 'prosperous': 416,\n",
       " 'pull': 417,\n",
       " 'pulse': 418,\n",
       " 'pure': 419,\n",
       " 'quaver': 420,\n",
       " 'queen': 421,\n",
       " 'quoth': 422,\n",
       " 'radiant': 423,\n",
       " 'raise': 424,\n",
       " 'rapid': 425,\n",
       " 'rapture': 426,\n",
       " 'reason': 427,\n",
       " 'record': 428,\n",
       " 'red': 429,\n",
       " 'regal': 430,\n",
       " 'remain': 431,\n",
       " 'remembrance': 432,\n",
       " 'renew': 433,\n",
       " 'repay': 434,\n",
       " 'rest': 435,\n",
       " 'revive': 436,\n",
       " 'reward': 437,\n",
       " 'rhyme': 438,\n",
       " 'ri': 439,\n",
       " 'rich': 440,\n",
       " 'ride': 441,\n",
       " 'right': 442,\n",
       " 'ring': 443,\n",
       " 'ringer': 444,\n",
       " 'rise': 445,\n",
       " 'river': 446,\n",
       " 'robe': 447,\n",
       " 'rochambeau': 448,\n",
       " 'rome': 449,\n",
       " 'round': 450,\n",
       " 'row': 451,\n",
       " 'ruby': 452,\n",
       " 'run': 453,\n",
       " 's': 454,\n",
       " 'sacred': 455,\n",
       " 'saintly': 456,\n",
       " 'say': 457,\n",
       " 'science': 458,\n",
       " 'sea': 459,\n",
       " 'seal': 460,\n",
       " 'seat': 461,\n",
       " 'see': 462,\n",
       " 'seed': 463,\n",
       " 'seem': 464,\n",
       " 'serene': 465,\n",
       " 'seven': 466,\n",
       " 'sex': 467,\n",
       " 'shade': 468,\n",
       " 'shall': 469,\n",
       " 'sharpness': 470,\n",
       " 'she': 471,\n",
       " 'shin': 472,\n",
       " 'shine': 473,\n",
       " 'shore': 474,\n",
       " 'should': 475,\n",
       " 'sigurd': 476,\n",
       " 'silent': 477,\n",
       " 'simpler': 478,\n",
       " 'sincerest': 479,\n",
       " 'sing': 480,\n",
       " 'sleep': 481,\n",
       " 'slight': 482,\n",
       " 'slumber': 483,\n",
       " 'smile': 484,\n",
       " 'snow': 485,\n",
       " 'so': 486,\n",
       " 'soft': 487,\n",
       " 'some': 488,\n",
       " 'sometimes': 489,\n",
       " 'song': 490,\n",
       " 'soul': 491,\n",
       " 'source': 492,\n",
       " 'spark': 493,\n",
       " 'speech': 494,\n",
       " 'spirit': 495,\n",
       " 'spoil': 496,\n",
       " 'spring': 497,\n",
       " 'stand': 498,\n",
       " 'star': 499,\n",
       " 'stately': 500,\n",
       " 'stept': 501,\n",
       " 'still': 502,\n",
       " 'stir': 503,\n",
       " 'stormy': 504,\n",
       " 'straight': 505,\n",
       " 'stream': 506,\n",
       " 'strife': 507,\n",
       " 'strike': 508,\n",
       " 'string': 509,\n",
       " 'strive': 510,\n",
       " 'such': 511,\n",
       " 'suddenly': 512,\n",
       " \"summer's\": 513,\n",
       " 'sun': 514,\n",
       " \"sundown's\": 515,\n",
       " 'superior': 516,\n",
       " 'surround': 517,\n",
       " 'sweet': 518,\n",
       " 'sweeter': 519,\n",
       " 'sweetly': 520,\n",
       " 'sweetness': 521,\n",
       " 'swifter': 522,\n",
       " 'sword': 523,\n",
       " 'sympathy': 524,\n",
       " 'symphony': 525,\n",
       " 'take': 526,\n",
       " 'teach': 527,\n",
       " 'tender': 528,\n",
       " 'tenderest': 529,\n",
       " 'thames': 530,\n",
       " 'than': 531,\n",
       " 'that': 532,\n",
       " 'the': 533,\n",
       " 'thee': 534,\n",
       " 'their': 535,\n",
       " 'then': 536,\n",
       " 'there': 537,\n",
       " 'thereupon': 538,\n",
       " 'these': 539,\n",
       " 'they': 540,\n",
       " 'thing': 541,\n",
       " 'think': 542,\n",
       " 'this': 543,\n",
       " 'those': 544,\n",
       " 'thou': 545,\n",
       " 'though': 546,\n",
       " 'thro': 547,\n",
       " 'throne': 548,\n",
       " 'throug': 549,\n",
       " 'through': 550,\n",
       " 'throw': 551,\n",
       " 'thunder': 552,\n",
       " 'thus': 553,\n",
       " 'thy': 554,\n",
       " 'till': 555,\n",
       " 'time': 556,\n",
       " 'tinkle': 557,\n",
       " 'to': 558,\n",
       " 'tongue': 559,\n",
       " 'too': 560,\n",
       " 'torch': 561,\n",
       " 'touch': 562,\n",
       " 'towards': 563,\n",
       " 'trail': 564,\n",
       " 'train': 565,\n",
       " 'tranquil': 566,\n",
       " 'trouble': 567,\n",
       " 'trust': 568,\n",
       " 'truth': 569,\n",
       " 'try': 570,\n",
       " 'turn': 571,\n",
       " 'twin': 572,\n",
       " 'u': 573,\n",
       " 'unity': 574,\n",
       " 'unrest': 575,\n",
       " 'unresting': 576,\n",
       " 'upon': 577,\n",
       " 'uppe': 578,\n",
       " 'utter': 579,\n",
       " 'vast': 580,\n",
       " 'very': 581,\n",
       " 'victor': 582,\n",
       " \"virtue's\": 583,\n",
       " 'voice': 584,\n",
       " 'vow': 585,\n",
       " 'wa': 586,\n",
       " 'wan': 587,\n",
       " 'war': 588,\n",
       " 'warm': 589,\n",
       " 'water': 590,\n",
       " 'we': 591,\n",
       " 'weary': 592,\n",
       " 'welcome': 593,\n",
       " 'what': 594,\n",
       " 'when': 595,\n",
       " 'where': 596,\n",
       " 'which': 597,\n",
       " 'whimper': 598,\n",
       " 'whine': 599,\n",
       " 'who': 600,\n",
       " 'whose': 601,\n",
       " 'why': 602,\n",
       " 'wild': 603,\n",
       " 'will': 604,\n",
       " 'wilt': 605,\n",
       " \"wisdom's\": 606,\n",
       " 'wise': 607,\n",
       " 'wit': 608,\n",
       " 'with': 609,\n",
       " 'woman': 610,\n",
       " 'wonder': 611,\n",
       " 'world': 612,\n",
       " 'worship': 613,\n",
       " 'worst': 614,\n",
       " 'worth': 615,\n",
       " 'worthy': 616,\n",
       " 'would': 617,\n",
       " 'wouldst': 618,\n",
       " 'wovest': 619,\n",
       " 'wrath': 620,\n",
       " 'wrong': 621,\n",
       " 'year': 622,\n",
       " 'yet': 623,\n",
       " 'you': 624,\n",
       " 'your': 625,\n",
       " 'yours': 626,\n",
       " \"youth's\": 627}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create word-index mappings\n",
    "word_indices = dict((word, index) for index, word in enumerate(unique_words))\n",
    "word_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6413184",
   "metadata": {},
   "source": [
    "### Create Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5afa53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x (input): Split text into blocks, where each block has the same amount of words\n",
    "# Create y (targets): For each x input, the y is the word that comes next\n",
    "# The model should learn to predict y from the input x\n",
    "\n",
    "block_size = 2\n",
    "step = 1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(word_tokens) - block_size, step):\n",
    "    x.append(word_tokens[i: i+block_size])\n",
    "    y.append(word_tokens[i + block_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83d0d331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['with', 'pale'],\n",
       " ['pale', 'blue'],\n",
       " ['blue', 'berry'],\n",
       " ['berry', 'in'],\n",
       " ['in', 'these']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect x\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1dd331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1290"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of blocks\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c87bed",
   "metadata": {},
   "source": [
    "### Create One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ba28e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoding of x\n",
    "x_encoded = []\n",
    "\n",
    "for x_arr in x:\n",
    "    x_ints = [word_indices[item] for item in x_arr]\n",
    "    \n",
    "    x_row = []\n",
    "    for item in x_ints:\n",
    "        x_vector = np.zeros(len(unique_words))\n",
    "        x_vector[item] = 1\n",
    "        x_row.append(x_vector)\n",
    "        \n",
    "    x_encoded.append(x_row)\n",
    "    \n",
    "x_encoded = np.array(x_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e351e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue', 'berry', 'in', 'these', 'peaceful']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect y\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea6c112d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54, 45, 257, 539, 386]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each word in y into their corresponding indices\n",
    "y_ints = [word_indices[item] for item in y]\n",
    "y_ints[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fc0bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoding of y\n",
    "y_encoded = []\n",
    "\n",
    "for item in y_ints:\n",
    "    y_vector = np.zeros(len(unique_words))\n",
    "    y_vector[item] = 1\n",
    "    y_encoded.append(y_vector)\n",
    "\n",
    "y_encoded = np.array(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcda4bf",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caa21464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Linear(input_dim, 2000)\n",
    "        self.hidden = nn.Linear(2000, 1200)\n",
    "        self.output = nn.Linear(1200, output_dim)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.embeddings(x))\n",
    "        x = self.tanh(self.hidden(x))\n",
    "        x = self.softmax(self.output(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6ff9f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1256\n"
     ]
    }
   ],
   "source": [
    "# Get size of input for training the model\n",
    "input_size = x_encoded[0].ravel().shape[0]\n",
    "print(x_encoded[0].ravel().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df58af12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing torch operations on cuda device\n"
     ]
    }
   ],
   "source": [
    "# Allocate tensors to the device used for computation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Performing torch operations on {device} device\")\n",
    "\n",
    "# Create x and y PyTorch tensors\n",
    "x = torch.tensor(x_encoded).float().to(device)\n",
    "y = torch.tensor(y_encoded).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb023182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextGenerator(\n",
       "  (embeddings): Linear(in_features=1256, out_features=2000, bias=True)\n",
       "  (hidden): Linear(in_features=2000, out_features=1200, bias=True)\n",
       "  (output): Linear(in_features=1200, out_features=628, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = TextGenerator(input_size, len(unique_words), block_size).to(device)\n",
    "\n",
    "# Print model configuration\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "433009b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.000000001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ee59c",
   "metadata": {},
   "source": [
    "### Create Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9e8b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom Dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        self.n_samples = len(x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index].ravel(), self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9228e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset using custom Dataset class\n",
    "training_ds = CustomDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a488527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset into DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    training_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03558f02",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a82f8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to train model\n",
    "def train_fn(loader, model, optimizer, loss_fn, device):\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    ave_loss = 0\n",
    "    count = 0 \n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        # Forward\n",
    "        predictions = model.forward(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update tqdm loading bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        count += 1\n",
    "        ave_loss += loss.item()\n",
    "    \n",
    "    ave_loss = ave_loss / count\n",
    "\n",
    "    return ave_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7811b5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:02<00:00, 43.49it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442569477613582\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 124.23it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442569403685341\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 125.22it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.4425693186678625\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 125.11it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.4425692595252695\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 126.30it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442569215168325\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 123.67it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442569163418556\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 126.01it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442569115365198\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 125.43it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442569063615429\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 118.87it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442569037740545\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 112.57it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442569004472836\n",
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 111.77it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568978597952\n",
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 116.26it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568926848183\n",
      "Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 118.53it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.44256883443788\n",
      "Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 121.02it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568782688111\n",
      "Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 122.97it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568756813227\n",
      "Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 118.04it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.44256869027781\n",
      "Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 119.20it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.4425686570101\n",
      "Epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 119.39it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568575689035\n",
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 120.21it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568546117738\n",
      "Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 118.96it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568527635678\n",
      "Epoch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 112.64it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.4425684426182\n",
      "Epoch: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 113.68it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568390868431\n",
      "Epoch: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 114.51it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568364993546\n",
      "Epoch: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 120.70it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568298458129\n",
      "Epoch: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 120.45it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568261494008\n",
      "Epoch: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 112.25it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568198655003\n",
      "Epoch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 112.87it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568117333937\n",
      "Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 96.55it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442568080369816\n",
      "Epoch: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 101.18it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.4425680397092835\n",
      "Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 108.57it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567973173866\n",
      "Epoch: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 109.56it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567928816921\n",
      "Epoch: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 110.88it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567888156388\n",
      "Epoch: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 111.16it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567832710207\n",
      "Epoch: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 113.05it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567777264026\n",
      "Epoch: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 113.40it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567718121433\n",
      "Epoch: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 113.12it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.4425676294075425\n",
      "Epoch: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 111.45it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567544390065\n",
      "Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 113.32it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567525908005\n",
      "Epoch: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 112.36it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567444586938\n",
      "Epoch: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 113.25it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567403926406\n",
      "Epoch: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 110.80it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567355873049\n",
      "Epoch: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 113.35it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567300426868\n",
      "Epoch: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 113.39it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567226498626\n",
      "Epoch: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 110.73it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567141481148\n",
      "Epoch: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 73.22it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.4425671119098515\n",
      "Epoch: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:02<00:00, 57.21it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567078642143\n",
      "Epoch: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 97.62it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442567026892374\n",
      "Epoch: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 86.53it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566964053368\n",
      "Epoch: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 86.75it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566923392835\n",
      "Epoch: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 92.79it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.4425668753394785\n",
      "Epoch: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 88.53it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566849464594\n",
      "Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 111.69it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.44256675335788\n",
      "Epoch: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 78.10it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566709000935\n",
      "Epoch: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 106.51it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566649858342\n",
      "Epoch: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:02<00:00, 61.88it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566627679869\n",
      "Epoch: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 114.67it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566583322924\n",
      "Epoch: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:02<00:00, 63.23it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566546358804\n",
      "Epoch: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 111.02it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566494609034\n",
      "Epoch: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 112.66it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566450252089\n",
      "Epoch: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 117.49it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566365234612\n",
      "Epoch: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 97.88it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566295002782\n",
      "Epoch: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:02<00:00, 60.64it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566250645837\n",
      "Epoch: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 86.75it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566198896068\n",
      "Epoch: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 91.55it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566128664239\n",
      "Epoch: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 94.09it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566084307294\n",
      "Epoch: 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:02<00:00, 52.40it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442566051039585\n",
      "Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:02<00:00, 63.00it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565999289815\n",
      "Epoch: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 107.81it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565969718519\n",
      "Epoch: 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 102.33it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565910575926\n",
      "Epoch: 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 108.41it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565895790278\n",
      "Epoch: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 90.32it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565840344097\n",
      "Epoch: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:02<00:00, 51.57it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565784897915\n",
      "Epoch: 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:02<00:00, 63.12it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.4425657109696735\n",
      "Epoch: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 85.18it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.4425656592199045\n",
      "Epoch: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 86.31it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565611166548\n",
      "Epoch: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 91.15it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565563113191\n",
      "Epoch: 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 112.50it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565485488537\n",
      "Epoch: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 121.84it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.44256543743518\n",
      "Epoch: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 112.54it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565378292587\n",
      "Epoch: 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 92.69it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.44256529697152\n",
      "Epoch: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 89.86it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565237828927\n",
      "Epoch: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 91.38it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565230436103\n",
      "Epoch: 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 97.44it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565186079158\n",
      "Epoch: 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 94.19it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565163900686\n",
      "Epoch: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 91.88it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565097365269\n",
      "Epoch: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 94.42it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442565053008323\n",
      "Epoch: 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 91.46it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442564990169318\n",
      "Epoch: 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 84.37it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442564879276956\n",
      "Epoch: 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 87.25it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442564831223598\n",
      "Epoch: 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 87.45it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442564809045126\n",
      "Epoch: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 79.04it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442564764688181\n",
      "Epoch: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 83.63it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442564738813297\n",
      "Epoch: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 87.43it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442564705545588\n",
      "Epoch: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 88.30it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442564627920935\n",
      "Epoch: 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 87.55it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442564594653225\n",
      "Epoch: 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 84.06it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442564528117808\n",
      "Epoch: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 84.25it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.4425644394039185\n",
      "Epoch: 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 82.27it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.44256440613621\n",
      "Epoch: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 84.55it/s, loss=6.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442564306333083\n",
      "Epoch: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 89.87it/s, loss=6.44]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 6.442564284154611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 100\n",
    "average_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    ave_loss = train_fn(train_loader, model, optimizer, criterion, device)\n",
    "    \n",
    "    print(\"Ave Loss: {}\".format(ave_loss))\n",
    "    average_losses.append(ave_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e315a81",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92dd4d34",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     x_vector\u001b[38;5;241m.\u001b[39mappend(x_item)\n\u001b[0;32m     15\u001b[0m initial_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([np\u001b[38;5;241m.\u001b[39marray([x_vector])\u001b[38;5;241m.\u001b[39mravel()])\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_input\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     18\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(output)\n\u001b[0;32m     19\u001b[0m df\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[20], line 14\u001b[0m, in \u001b[0;36mTextGenerator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden(x))\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "# Inspect probability distribution of word tokens\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "random.seed(2)\n",
    "\n",
    "phrase = [indices_words[random.randint(0, len(unique_words))], indices_words[random.randint(0, len(unique_words))]]\n",
    "x_ints = [word_indices[item] for item in phrase]\n",
    "x_vector = []\n",
    "\n",
    "for item in x_ints:\n",
    "    x_item = np.zeros(len(unique_words))\n",
    "    x_item[item] = 1\n",
    "    x_vector.append(x_item)\n",
    "\n",
    "initial_input = torch.tensor([np.array([x_vector]).ravel()]).to(dtype=torch.float32).to(device)\n",
    "output = model(initial_input)[0].detach().cpu().numpy()\n",
    "\n",
    "df = pd.DataFrame(output)\n",
    "df.plot.bar()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140eb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text sample from model output\n",
    "word_count = 100\n",
    "text = []\n",
    "paragraph_count = 5\n",
    "\n",
    "# Length of phrase should be same as block_size\n",
    "word1, word2 = indices_words[random.randint(0, len(unique_words))], indices_words[random.randint(0, len(unique_words))]\n",
    "\n",
    "for p in range(paragraph_count):\n",
    "    text.append([])\n",
    "    \n",
    "    for i in range(word_count):\n",
    "        phrase = [word1, word2]\n",
    "        x_ints = [word_indices[item] for item in phrase]\n",
    "        x_vector = []\n",
    "\n",
    "        for item in x_ints:\n",
    "            x_item = np.zeros(len(unique_words))\n",
    "            x_item[item] = 1\n",
    "            x_vector.append(x_item)\n",
    "\n",
    "        initial_input = torch.tensor([np.array([x_vector]).ravel()]).float().to(device)\n",
    "\n",
    "        output = model(initial_input)[0].detach().cpu().numpy()\n",
    "\n",
    "        # Workaround to fix occasional sum(pvals[:-1]) > 1.0  bug from implicit casting in np.random.multinomial \n",
    "        output = output.astype(float)\n",
    "        output /= output.sum()\n",
    "\n",
    "        index = np.where(np.random.multinomial(1, output) == 1)[0][0]\n",
    "        word3 = indices_words[index]\n",
    "        text[p].append(word3)\n",
    "\n",
    "        # Use generated word from this run as seed for next run\n",
    "        word1, word2 = word2, word3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd566b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in range(paragraph_count):\n",
    "    print(f\"Generated Paragraph {p}:\")\n",
    "    print(' '.join(text[p]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d46cb7",
   "metadata": {},
   "source": [
    "### Create Feature Vectors from Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "# Tokenize by line\n",
    "for index, row in positive.iterrows():\n",
    "    tokenized_row = row['Text'].split(' ')\n",
    "      \n",
    "    # Preprocess using the same settings as preprocessing done before training model\n",
    "    tokenized_row = regexp_tokenize(' '.join(tokenized_row), pattern=r'[^\\S\\r\\n]+|[\\.,;!?()--_\"]', gaps=True)\n",
    "    tokenized_row = [lemmatizer.lemmatize(token) for token in tokenized_row] # Lemmatize nouns\n",
    "    tokenized_row = [lemmatizer.lemmatize(token, 'v') for token in tokenized_row] # Lemmatize verbs\n",
    "    \n",
    "    sentences.append(tokenized_row)\n",
    "    \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "w2v_model = Word2Vec(sentences, vector_size=vector_size, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d010c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = w2v_model.wv.index_to_key\n",
    "vocab_length = len(vocab)\n",
    "\n",
    "print(f'Vocabulary Size: {format(vocab_length)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2016e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('i', 'me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector_averaging(sentence, model):\n",
    "    embeddings = [model.wv[word] for word in sentence if word in vocab]\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258efc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    features.append(sentence_to_vector_averaging(sentence, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f'x{i}' for i in range(w2v_model.vector_size)]\n",
    "\n",
    "df_text = pd.DataFrame(features, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb6064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_text['y'] = [float(x) for x in positive['Sentiment']]\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacbab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature vectors as csv\n",
    "df_text.to_csv('positive.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ecc06",
   "metadata": {},
   "source": [
    "### Create Feature Vectors from Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3088f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "w2v_model = Word2Vec(text, vector_size=vector_size, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcffb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = w2v_model.wv.index_to_key\n",
    "vocab_length = len(vocab)\n",
    "\n",
    "print(f'Vocabulary Size: {format(vocab_length)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [w2v_model.wv[word] for word in vocab]\n",
    "len(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1304879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector_averaging(sentence, model):\n",
    "    embeddings = [model.wv[word] for word in sentence if word in vocab]\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a160fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for sentence in text:\n",
    "    features.append(sentence_to_vector_averaging(sentence, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f'x{i}' for i in range(w2v_model.vector_size)]\n",
    "\n",
    "df_text = pd.DataFrame(features, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcafa9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_text['y'] = [1] * len(df_text)\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature vectors as csv\n",
    "df_text.to_csv('positive_generated.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
